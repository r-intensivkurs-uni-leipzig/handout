[["index.html", "Handout R-Intensivkurs 1 Willkommen", " Handout R-Intensivkurs Christoph Mayer 2024-05-03 1 Willkommen Schön, dass ihr Interesse an der Programmiersprache R habt! Hier seid ihr auf der Seite des kursbegleitenden Handouts für den R Intensivkurs des Unirechenzentrums Leipzig in Kooperation mit dem Academic Lab. Teilnehmende des Kurses sollen hiermit die Möglichkeit bekommen, während bzw. nach dem Kurs Inhalte erneut einzusehen. Hinweis: Das Handout wird stetig überarbeitet bzw. erweitert. Über Anmerkungen und Verbesserungsvorschläge freuen wir uns :) Danksagung: Das Handout basiert in Teilen auf bereitgestellten Materialien von Ingo Seifert &amp; Isabel Gebhardt. Ehemalige Tutor:innen: Franz Andersch Vielen Dank! "],["einführung-in-r-und-r-studio.html", "2 Einführung in R und R Studio 2.1 Was ist R? 2.2 Sechs Gründe R zu lernen 2.3 Benutzeroberfläche von R und RStudio 2.4 Empfohlenes Vorgehen", " 2 Einführung in R und R Studio Dieses Kapitel bietet einen Einführung in was R und RStudio eigentlich sind und wie deren Benutzeroberfläche aufgebaut ist. 2.1 Was ist R? R ist eine freie Programmiersprache Sprache: Vokabeln -&gt; Begriffe für Objekte, Eigenschaften, Handlungen etc. Grammatik -&gt; Regeln, wie Vokabeln in Sinnzusammenhang gebracht werden Programmiersprache funktioniert analog: Vokabeln -&gt; Funktionen, Objekte Grammatik -&gt; Regeln für die Anwendung von Funktionen auf Objekte Wie können Sprachen möglichst nachhaltig erlernt werden? Starte bei den Basics Basic Vokabeln Basic Grammatik -&gt; Ziel des Kurses ist es, die wesentliche Grammatik von R und die grundlegenden Vokabeln zu vermitteln. 2.2 Sechs Gründe R zu lernen Open Source und plattformübergreifend Praktisch unbegrenzter Funktionsumfang Entwickelt sich zum Standard in akademischer Welt und in der Wirtschaft Publizierfähige Graphiken Integration von Aufbereitung, Analyse und Präsentation Internationale Community 2.3 Benutzeroberfläche von R und RStudio 2.3.1 R GUI Haben wir dem Computer R durch Installation beigebracht, wird ein Graphical User Interface (GUI) mitinstalliert. Hier ist der volle Funktionsumfang bereits enthalten. Die GUI ist leider wenig übersichtlich und wenig “komfortabel”. 2.3.2 RStudio RStudio bietet eine nutzer:innenfreundliche (und freie) Alternative für die Nutzung von R. Enthält Script-Editor Übersicht über Objekte in globaler Umgebung Einfache Dateienhandhabung durch Projekte, u.v.m. 2.3.2.1 RStudio - Konsole Dient zur direkten Übergabe von Ausdrücken bzw. Funktionen an R Geeignet für kleine Tasks bzw. Tests Ergebnis wird direkt angezeigt Ergebnisse aus den Scripten, die keine Grafiken sind, werden hier angezeigt 2.3.2.2 RStudio - Script Editor Dient zur Eingabe von komplexen Funktionssequenzen (Scripts) Erlaubt das Speichern von Funktionssequenzen Der Editor öffnet sich nicht automatisch. Er muss händisch geöffnet werden über File -&gt; New File -&gt; R Script. 2.3.2.3 RStudio - Environment und Files Zeigt alle importierten Daten an Bietet Zugriff auf die History GitHub Zugriff, wenn angelegt Anzeige für File Browser Zeigt alle Packages, Plots etc. 2.3.2.4 Projekte in RStudio Vorteile: - Keine definition des working directory nötig - Erleichtert Weitergabe unserer Projekte - Hilft unsere Arbeit zu organisieren - Ein neues Projekt erstellt ihr indem ihr in der oberen linken Ecke File -&gt; New Project auswählt. 2.4 Empfohlenes Vorgehen In unseren Projekten arbeiten wir mit Skripten, dabei ist ein strukturiertes Vorgehen wichtig. Das bedeutet, das wir für eine Datenanalyse jeweils ein Skript erstellen. Dieses nachvollziehbar benennen und unseren Code mit Kommentaren versehen. 2.4.1 Empfohlener Ablauf einer Analyse Projekt und Skript(e) erstellen -&gt; siehe Modul 1 Installieren bzw. laden aller nötigen Pakete -&gt; siehe Modul 2 Daten importieren -&gt; siehe Modul 3 Daten organisieren -&gt; siehe Modul 3 und 4 Daten analysieren -&gt; siehe Modul 5 und 6 Ergebnisse präsentieren -&gt; siehe Modul 7 2.4.2 Beispiel Skript # Packages installieren und laden install.packages(&quot;tidyverse&quot;) library(tidyverse) # Daten laden df_salary &lt;- read.table(file = &quot;Daten/Salary.txt&quot;) # Daten organisieren ## Kategoriale Variable &quot;salary_cat&quot; erstellen (3 Kategorien) df_salary_final &lt;- mutate(df_salary, salary_cat = cut_number(salary, n = 3, labels = c(&quot;niedrig&quot;,&quot;mittel&quot;,&quot;hoch&quot;))) # Daten analysieren ## Absolute Häufigkeiten Berufe in Gehaltskategorien(salary_cat) table(df_salary_final$salary_cat, df_salary_final$profession) # Daten präsentiern ## Mosaikplot für Berufe in Gehaltskategorien(salary_cat) mosaicplot(table(df_salary_final$salary_cat, df_salary_final$profession), main = &quot;Gehaltskategorie nach Beruf&quot;) ... "],["grundlagen.html", "3 Grundlagen 3.1 Fundamentale Datentypen in R 3.2 R als Taschenrechner 3.3 Variablen und Objekte 3.4 Funktionen 3.5 Datenstrukturen 3.6 Packages", " 3 Grundlagen 3.1 Fundamentale Datentypen in R In R existieren verschiedene atomare Datentypen aus welchen sich wiederum komplexere Datentypen konstruieren lassen. In R existieren verschiedene atomare Datentypen aus welchen sich wiederum komplexere Datentypen konstruieren lassen. Datentyp Beschreibung Beispiel integer ganze Zahlen -2L numeric reelle Zahlen 5.2456 logic logische Werte TRUE, FALSE character Zeichenfolge ‘a’, “Mexico” Textelemente müssen immer in Anführungszeichen stehen, also entweder “…” oder ‘…’ In R können manche atomare Datentypen besondere Ausprägungen annehmen. Datentyp Ausprägung Bedeutung numeric NaN “Not a Number” numeric Inf Unendlich logic NA Unbestimmt NULL NULL Leer 3.1.1 Umformung Gelegentlich kommt es vor, dass beim Import von Datensätzen die Daten im falschen Format übernommen werden, diese können wieder umgeformt werden character \\(\\rightarrow\\) numeric as.numeric(&quot;2&quot;) [1] 2 logical \\(\\rightarrow\\) numeric (FALSE = 0, TRUE = 1) as.numeric(TRUE) [1] 1 numeric \\(\\rightarrow\\) character as.character(2024) [1] &quot;2024&quot; numeric \\(\\rightarrow\\) logical (0 = FALSE, jede andere Zahl = TRUE) as.logical(5) [1] TRUE 3.2 R als Taschenrechner 3.2.1 Dezimalstellen und Leerzeichen Dezimalzahlen werden in R mit einem Punkt und nicht mit einem Komma angegeben 2,5 Error: &lt;text&gt;:1:2: unexpected &#39;,&#39; 1: 2, ^ 2.5 [1] 2.5 Die Anzahl von Leerzeichen zwischen Befehlsbestandteilen ist (mit Ausnahmen) egal 3.2.2 Mathematische Operatoren R- Befehl Bedeutung + Addition - Subtraktion * Multiplikation / Division ^ Potenz sqrt(x) Wurzel log(x, base) Logarithmus Wichtig: Klammersetzung und Punkt vor Strich 3.2.2.1 Beispiele 3+2*8 [1] 19 (2*10)/(2*2.5) [1] 4 sqrt(2)^2 [1] 2 log(100000, base=10) [1] 5 3.2.3 Logische Operatoren R Befehl Bedeutung == Gleich &gt; Größer &lt; Kleiner &gt;= Größer gleich &lt;= Kleiner gleich != Nicht-gleich &amp; logisches UND | logisches ODER 3.2.3.1 Beispiele 4&lt;5 [1] TRUE 23!=24 [1] TRUE 10&lt;=4 | 12==6 [1] FALSE !(&quot;a&quot;&lt;&quot;b&quot; &amp; TRUE==TRUE) [1] FALSE 3.3 Variablen und Objekte 3.3.1 Abspeichern von Variablen Werte können in Variablen (Objekten) gespeichert werden x &lt;- 5 + 3 Objekte sind nach der Zuweisung mit ihrem Namen abrufbar x [1] 8 Gespeicherte Objekte können weiterverwendet werden x &lt;- 5 x + 10 [1] 15 y &lt;- 2 * 3 y + x [1] 11 z &lt;- y + x z [1] 11 Es können alle möglichen Elemente in Objekten gespeichert werden Text: z &lt;- &quot;Textelement&quot; z [1] &quot;Textelement&quot; logische Werte: y &lt;- 5 &lt; 4 y [1] FALSE 3.3.2 Hinweise zur Benennung R unterscheidet Groß- und Kleinschreibung: X und x sind nicht das Gleiche Variablennamen dürfen nur Buchstaben, Zahlen, Unterstrich und Punkt enthalten Variablennamen sollten keine Leerzeichen enthalten Umlaute und ß wenn möglich vermeiden Variablennamen dürfen nicht mit einer Zahl oder einem Unterstrich beginnen 7b &lt;- 5+5 Error: &lt;text&gt;:1:2: unexpected symbol 1: 7b ^ 3.3.3 Enviroment Gespeicherte Objekte sind im Workspace (Environment) mit zusätzlichen Informationen aufgelistet Objekte können durch das Kästchen ausgewählt und durch das Besensymbol gelöscht werden 3.3.3.1 Befehle für Zugriff auf Environment: ls() listet in der R-Konsole den Workspace auf Mit rm(\"objektname1\",\"objektname2\") werden Objekte entfernt Mit rm(list = ls()) werden alle Objekte entfernt 3.4 Funktionen Woraus besteht eine Funktion? R beinhaltet eine große Menge an Grundfunktionen Allgemeine Struktur: Funktion(Argument1, Argument2, …) Die Argumente definieren und spezifizieren die Funktion Beispiele für einfache Funktionen (hier nur ein Argument): sqrt(x) berechnet Quadratwurzel mean(X) berechnet Mittelwert eines Vektors 3.4.1 Funktionen mit mehreren Argumenten c(Argument1, Argument2, Argument3) definiert einen Vektor mit drei Werten (z.B. Spaltennamen) c(1,2,3,4,5) [1] 1 2 3 4 5 c(&quot;Hund&quot;,&quot;Katze&quot;,&quot;Maus&quot;) [1] &quot;Hund&quot; &quot;Katze&quot; &quot;Maus&quot; round(x, digits = 2) rundet x auf 2 Nachkommastellen round(3.141593, digits = 2) [1] 3.14 3.4.2 Default-Werte in Funktionen Nicht alle Argumente von Funktionen müssen angegeben werden Diese optionalen Argumente nehmen dann default-Werte ein Beispielsweise bei round() ist der Default-Wert digits = 0 default-Werte können in R Documentation nachgesehen werden (s. Help-Funktion) round(3.141593) [1] 3 Die Argumente müssen nicht explizit benannt werden round(3.141593,2) [1] 3.14 round(2,3.141593) [1] 2 Dann ist aber die Reihenfolge wichtig, in der die Werte der Argumente eingegeben werden Benennen ist allerdings nachvollziehbarer (hilft, Fehler zu vermeiden) 3.4.3 Exkurs: Help-Funktion Durch die Funktion help() und ? lässt sich unter anderem herausfinden, welche Funktion welche Argumente benötigt help(round) ?round 3.4.4 Abspeichern von Output einer Funktionen Die Ergebnisse von Funktionen können ebenfalls unter Objekten abgespeichert (und weiterverwendet) werden x &lt;- c(1,2,3,4,5) x [1] 1 2 3 4 5 r &lt;- round(3.141593,2) r+5 [1] 8.14 3.5 Datenstrukturen Übersicht 3.5.1 Vektoren eindimensionale Datenstruktur, in dem mehreren Elemente enthalten sind können im Enviroment als Objekt abgespeichert werden Mathematische und logische Operatoren können auf den Vektor (auf alle Elemente) angewandt werden x &lt;- c(1,2,3,4,5) x+1 [1] 2 3 4 5 6 x&gt;3 [1] FALSE FALSE FALSE TRUE TRUE Durch eckige Klammern [] können durch Angabe ihrer Position einzelne Elemente eines Vektors ausgewählt (indiziert) werden. (Wichtig: in R beginnt der Index immer mit 1) y &lt;- c(110,120,130,140,150) y [1] 110 120 130 140 150 y[3] [1] 130 y[c(1,3,5)] [1] 110 130 150 Die Elemente können auch aufgrund von gewissen Bedingungen ausgewählt werden y &lt;- c(110,120,130,140,150) y [1] 110 120 130 140 150 y[y&gt;=130] [1] 130 140 150 y[y==120] [1] 120 Steht ein negatives Vorzeichen vor dem Index, so wird jedes Element außer dem indizierten aus dem Vektor ausgewählt x &lt;- c(1,2,3,4,5) x [1] 1 2 3 4 5 x[-3] [1] 1 2 4 5 x[c(-1,-3)] [1] 2 4 5 Soll eine gewisse Reihe von angrenzenden Elementen (von Element i bis Element j) vektorisiert werden, werden die Indizes mit : verbunden (also i:j). x &lt;- c(1:5) x [1] 1 2 3 4 5 x[1:3] [1] 1 2 3 3.5.2 Matrizen Zweidimensionale Datenstruktur Matrizen werden in R mit matrix(data = …, nrow = …, ncol = …, byrow = …) erstellt data ist ein Vektor, der die Elemente der Matrix enthält nrow definiert die Anzahl der Zeilen der Matrix ncol definiert die Anzahl der Spalten der Matrix Wird die Matrix zeilenweise (byrow = TRUE) oder spaltenweise (byrow = FALSE) mit den Elementen befüllt? d &lt;- c(1:9) m &lt;- matrix(data = d, nrow = 3, ncol = 3, byrow = TRUE) m [,1] [,2] [,3] [1,] 1 2 3 [2,] 4 5 6 [3,] 7 8 9 Die Auswahl von Matrixelementen ist analog zu der Auswahl von Elementen aus einem Vektor Allerdings wird bei einer Matrix ein Element durch zwei Positionen (Zeile und Spalte) definiert m [,1] [,2] [,3] [1,] 1 2 3 [2,] 4 5 6 [3,] 7 8 9 m[3,2] [1] 8 Sollen alle Spalten in einer bestimmten Zeile ausgewählt werden, bleibt der Spaltenindex leer Analog für die Auswahl aller Zeilen in einer bestimmten Spalte Negative Indizierung und Mehrfachauswahl auch möglich m [,1] [,2] [,3] [1,] 1 2 3 [2,] 4 5 6 [3,] 7 8 9 m[3,] [1] 7 8 9 m[,2] [1] 2 5 8 3.5.3 Dataframes Ein Dataframe ist eine Datenstruktur ähnlich zur Matrix jede Spalte beinhaltet Werte einer Variable und jede Reihe Werte von jeder Variable (Spalten müssen gleichlang sein) Data Frames können mit data.frame() aus mehreren (gleichlangen) Vektoren erstellt werden alter &lt;- c(16,17,15,22) geschlecht &lt;- c(&quot;w&quot;,&quot;w&quot;,&quot;m&quot;,&quot;m&quot;) dataframe &lt;- data.frame(alter, geschlecht) dataframe alter geschlecht 1 16 w 2 17 w 3 15 m 4 22 m Verschieden Möglichkeiten für Elementauswahl Zeilen- und Spaltennummer (analog zu Matrizen) dataframe[2,1] [1] 17 oder mithilfe des $} operators direkt auf Variablen zugreifen dataframe$alter [1] 16 17 15 22 dataframe$alter[2] [1] 17 Mit dem \\$ Operator können zudem einzelne Werte verändert werden dataframe$geschlecht[2] &lt;- &quot;m&quot; … oder ganze Variablen gelöscht werden dataframe$geschlecht &lt;- NULL ** Um festzustellen, ob die Veränderungen funktioniert haben, muss nicht immer das Objekt extra in der Konsole aufgerufen werden. Es reicht im Enviroment zu schauen ob sich das Objekt entsprechend verändert hat.** Desweiteren können auch neue Variablen hinzugefügt werden dataframe$aktuellesJahr &lt;- rep(2022, times = 4) …oder aus bestehenden Variablen gebildet werden dataframe$geburtsjahr &lt;- dataframe$aktuellesJahr - dataframe$alter dataframe alter aktuellesJahr geburtsjahr 1 16 2022 2006 2 17 2022 2005 3 15 2022 2007 4 22 2022 2000 3.6 Packages R besteht aus einer Reihe von vorinstallierten Grundfunktionen Zusätzliche Funktionen erhält man durch externe Pakete (packages) Diese müssen heruntergeladen und eingebunden werden Anzahl R Packages 3.6.1 Installieren von Packages library(): Auflistung aller bereits installierten Pakete install.packages(\"NameDesPakets\")1: Paket installieren (einmalig), Internetverbindung benötigt library(NameDesPakets): Laden eines Pakets (in jeder R-Sitzung nötig) !Hier unbedingt in Anführungszeichen↩︎ "],["datenmanagement-i.html", "4 Datenmanagement I 4.1 Daten importieren 4.2 Daten speichern 4.3 Datensätze zusammenfügen", " 4 Datenmanagement I 4.1 Daten importieren 4.1.1 Organisation von Dateien Empfehlung: Einen Ordner für alle Datendateien innerhalb des Projektordners (bspw. data) 4.1.2 Exkurs: Dateipfade Beispiel für den absoluten Pfad der Datei Daten.txt - lässt sich aus den Eigenschaften einer Datei entnehmen. D:\\\\Dokumente\\\\Kurse\\\\Rintensiv\\\\Projektordner\\\\data\\\\Daten.txt Wenn nur vom Projektordner aus navigiert werden muss, dann entfällt bei der Pfadangabe der ganze Teil vor data (relative Pfadangabe) Wichtig: R nutzt Backslash “\\” zu anderen Zwecken, daher muss ein normaler Slash “/” genutzt werden Die relative Pfadangabe für Daten.txt würde also so aussehen: data/Daten.txt 4.1.3 Daten aus .txt, .csv, .xlsx und weiteren Dateiformaten auslesen dataset &lt;- read.table(file = &quot;data/Daten.txt&quot;, header = TRUE) Dateipfad unter file = immer in \"\" angeben Navigieren innerhalb des Projektordners (Vorteil: keine vollständigen Dateipfade müssen angegeben werden) header (default: FALSE): nimmt die erste Zeile als Variablennamen sep (default: \"\"): legt fest wie die Spalten in der Ursprungsdatei getrennt sind dec (default: \".\"): legt fest welches Zeichen zur Dezimaltrennung genutzt wird Befehl für .csv Dateien dataset &lt;- read.csv2(&quot;data/Daten.csv&quot;) Befehl für .xlsx Dateien install.packages(&quot;xlsx&quot;) library(xlsx) dataset &lt;- read.xlsx(&quot;data/Daten.xlsx&quot;) Befehl für .sav Dateien (SPSS) install.packages(&quot;haven&quot;) library(haven) dataset &lt;- read_sav(&quot;data/Daten.sav&quot;) 4.1.4 Alternative per Klickpfad Im Environment auf Import Dataset klicken und gewünschtes Format auswählen: Anschließend gewünschtes Dataset suchen und anklicken. Vorschau mit allen Argumenten, die auch im read.table() enthalten sind: Eingabe in die Konsole: Daten_2 &lt;- read.csv(&quot;data/Daten_2.txt&quot;, sep=&quot;&quot;) View(Daten_2) Vorteil: automatisch richtiger Befehl Vorschau Nachteil Kompletter Dateipfad angegeben, nicht brauchbar für andere 4.1.5 Überblick verschaffen str() gibt die Struktur des Dataframes aus str(dataset) &#39;data.frame&#39;: 10 obs. of 12 variables: $ Person: int 1 2 3 4 5 6 7 8 9 10 $ item1 : int 3 4 3 2 2 4 2 3 3 4 $ item2 : int 2 1 1 2 2 1 3 1 1 1 $ item3 : int 3 4 3 2 2 3 3 4 3 3 $ item4 : int 2 3 3 2 2 3 2 4 3 2 $ item5 : int 2 1 1 2 1 1 2 1 1 1 $ item6 : int 1 1 1 2 1 1 3 1 1 1 $ item7 : int 4 4 3 2 3 4 2 4 3 4 $ item8 : int 1 1 1 2 2 1 4 1 1 2 $ item9 : int 1 1 1 2 2 1 4 1 1 1 $ item10: int 3 4 3 2 2 4 1 3 3 4 $ item11: int 4 2 4 4 2 5 4 2 4 4 dim() gibt die Dimensionen (Zeilen und Spalten) einer Matrix bzw. Dataframes an dim(dataset) #&gt; [1] 10 12 mit head() lassen sich die ersten n = c(Zeilen, Spalten) anzeigen head(x = dataset, n = c(2,11)) Person item1 item2 item3 item4 item5 item6 item7 item8 1 1 3 2 3 2 2 1 4 1 2 2 4 1 4 3 1 1 4 1 item9 item10 1 1 3 2 1 4 View(Dataset) öffnet eine Ansicht ähnlich zu Excel (View groß geschrieben!) names() gibt die Variablennamen des Datensatzes aus names(dataset) [1] &quot;Person&quot; &quot;item1&quot; &quot;item2&quot; &quot;item3&quot; &quot;item4&quot; &quot;item5&quot; [7] &quot;item6&quot; &quot;item7&quot; &quot;item8&quot; &quot;item9&quot; &quot;item10&quot; &quot;item11&quot; Damit lassen sich auch Namen von Variablen im Datenesatz verändern names(dataset)[names(dataset) == &quot;item11&quot;] &lt;- &quot;item12&quot; names(dataset) [1] &quot;Person&quot; &quot;item1&quot; &quot;item2&quot; &quot;item3&quot; &quot;item4&quot; &quot;item5&quot; [7] &quot;item6&quot; &quot;item7&quot; &quot;item8&quot; &quot;item9&quot; &quot;item10&quot; &quot;item12&quot; 4.2 Daten speichern 4.2.1 Datensatz exportieren als .txt, .csv, .xlsx oder .sav Befehl: write.table(x = data, file = \"data/Daten.txt\", row.names = FALSE) Argumente: x: Welches Objekt soll exportiert werden file: Pfad und Name der neuen Datei (.txt am ende nicht Vergessen!) sep, dec: Wie bei read.table row.names (default: TRUE): Zeilennamen/ -nummerierung als eigene Spalte col.names (default: TRUE): Variablennamen als erste Zeile speichern Genauso wie bei den read- Funktionen gibt es auch write.csv() für .csv Dateien write.xlsx() für .xlsx Dateien (Package “xlsx” benötigt) write_sav() für .sav Dateien (Package “haven” benötigt) Wenn beispielsweise: file = “Unterordner/exportierteDatei.txt”, dann liegt die Datei im Unterodner. Wenn kein Pfad angegeben wird, dann liegt sie im Projektordner. 4.3 Datensätze zusammenfügen 4.3.1 Fälle hinzufügen Situation: Es wurde eine weitere Erhebung mit anderen Personen durchgeführt, in der die gleichen Items abgefragt wurden dataset1 &lt;- read.table(&quot;data/Daten.txt&quot;, header = TRUE) head(dataset1,2) Person item1 item2 item3 item4 item5 item6 item7 item8 1 1 3 2 3 2 2 1 4 1 2 2 4 1 4 3 1 1 4 1 item9 item10 item11 1 1 3 4 2 1 4 2 dataset2 &lt;- read.table(&quot;data/Daten_2.txt&quot;, header = TRUE) head(dataset2,2) Person item1 item2 item3 item4 item5 item6 item7 item8 1 11 5 3 5 3 2 6 4 4 2 12 3 5 2 3 5 3 3 2 item9 item10 item11 1 3 3 4 2 6 4 6 mit rbind() lassen sich Daten (Vektoren, Matrizen, Dataframes) zeilenweise (reihenweise) zusammenfügen (rbind = rowbind) es können beliebig viele Datensätze in diesem Befehl zusammengefügt werden new_dataset &lt;- rbind(dataset1,dataset2) Überprüfung dim(new_dataset) [1] 20 12 Bedingungen Die Datasets haben die gleiche Anzahl an Spalten Die Datasets haben die gleichen Namen für die Variablen 4.3.2 Variablen hinzufügen Situation: Innerhalb der Erhebung gab es noch eine Abfrage von weiteren Items dataset1 &lt;- read.table(&quot;data/Daten.txt&quot;, header = TRUE) head(dataset1,2) Person item1 item2 item3 item4 item5 item6 item7 item8 1 1 3 2 3 2 2 1 4 1 2 2 4 1 4 3 1 1 4 1 item9 item10 item11 1 1 3 4 2 1 4 2 dataset3 &lt;- read.table(&quot;data/Daten_FB2.txt&quot;, header= TRUE) head(dataset3,2) Person item12 item13 item14 item15 item16 item17 item18 1 1 5 3 5 3 2 6 4 2 2 3 5 2 3 5 3 3 item19 item20 item21 item22 1 4 3 3 4 2 2 6 4 6 Mit cbind() lassen sich Daten (Vektoren, Matrizen, Dataframes) spaltenweise zusammenfügen new_dataset2 &lt;- cbind(dataset1, dataset3) Überprüfung dim(new_dataset2) [1] 10 24 Bedingung - Datensätze müssen gleiche Anzahl an Zeilen haben - Datensätze müssen gleich sortiert sein Situation: Innerhalb der Erhebung gab es noch eine Abfrage von weiteren Items, allerdings fehlt eine Person \\(\\rightarrow\\) cbind() führt nicht zum gewünschten Ergebnis dataset1 &lt;- read.table(&quot;data/Daten.txt&quot;, header = TRUE) head(dataset1,2) Person item1 item2 item3 item4 item5 item6 item7 item8 1 1 3 2 3 2 2 1 4 1 2 2 4 1 4 3 1 1 4 1 item9 item10 item11 1 1 3 4 2 1 4 2 dataset4 &lt;- read.table(&quot;data/Daten_FB2_NA.txt&quot;, header= TRUE) head(dataset4,2) Person item12 item13 item14 item15 item16 item17 item18 1 1 5 3 5 3 2 6 4 3 3 4 4 5 5 2 3 3 item19 item20 item21 item22 1 4 3 3 4 3 3 5 7 5 Mit merge(…, …, by=\"...“) lassen sich Daten (Vektoren, Matrizen, Dataframes) spaltenweise besser zusammenfügen Anhand einer Schlüsselvariablen (by) können zusammengehörige Fälle erkannt werden by: Welche Variable ist die Schlüsselvariable? by.x, by.y: wie by, aber separate Angabe pro Datensatz, z.B. wenn sie nicht gleich benannt ist all: Alle Fälle behalten (TRUE) oder nur die in beiden Datensätzen vorhandenen (FALSE) all.x, all.y: wie all, aber nur für einen der Datensätze sort: Neu nach Schlüsselvariable sortieren? (TRUE = ja) new_dataset3 &lt;- merge(dataset1, dataset4, by = &quot;Person&quot;, all = FALSE) head(new_dataset3,2) Person item1 item2 item3 item4 item5 item6 item7 item8 1 1 3 2 3 2 2 1 4 1 2 3 3 1 3 3 1 1 3 1 item9 item10 item11 item12 item13 item14 item15 item16 1 1 3 4 5 3 5 3 2 2 1 3 4 4 4 5 5 2 item17 item18 item19 item20 item21 item22 1 6 4 4 3 3 4 2 3 3 3 5 7 5 \\(\\rightarrow\\) Person 2 wird raus-gefiltert für alle restlichen Personen werden die neuen Variablen übernommen all = FALSE: Nur die Fälle werden behalten, die in beiden Datensätzen vorhanden sind new_dataset3 &lt;- merge(dataset1, dataset4, by = &quot;Person&quot;, all = TRUE) head(new_dataset3,2) Person item1 item2 item3 item4 item5 item6 item7 item8 1 1 3 2 3 2 2 1 4 1 2 2 4 1 4 3 1 1 4 1 item9 item10 item11 item12 item13 item14 item15 item16 1 1 3 4 5 3 5 3 2 2 1 4 2 NA NA NA NA NA item17 item18 item19 item20 item21 item22 1 6 4 4 3 3 4 2 NA NA NA NA NA NA \\(\\rightarrow\\) Person 2 bleibt erhalten. Die nicht vorhandenen VAriablen werden mit NA gefüllt "],["datenmanagement-ii.html", "5 Datenmanagement II 5.1 Datenbearbeitung 5.2 Long- und Wide-Format", " 5 Datenmanagement II 5.1 Datenbearbeitung Wir wissen nun wie Daten eingelesen und zusammengefügt werden Bevor wir mit unserem Datensatz Statistik betreiben können, müssen aber meist noch weitere Schritte der Datenbearbeitung durchgeführt werden Beispieldatensatz: personenDaten &lt;- read.table(&quot;data/PersonenDaten.txt&quot;, header = TRUE) head(x = personenDaten, n = c(5, 12)) Person item1 item2 item3 item4 item5 item6 item7 item8 1 1 3 2 3 2 2 1 4 1 2 2 4 1 4 3 1 1 4 1 3 3 3 1 3 3 1 1 3 1 4 4 2 2 2 2 2 2 2 2 5 5 2 2 2 2 1 1 3 2 item9 item10 item11 1 1 3 4 2 1 4 2 3 1 3 4 4 2 2 4 5 2 2 2 5.1.1 Umkodierung Situation: Im personDaten- Datensatz ist die Variable sex mit 1 und 2 kodiert. Die Kodierung soll aber 0 und 1 sein. Der $- Operator schafft hier abhilfe wenn… eine neue Variable aus einer alten erstellt werden soll personenDaten$sexUmkodiert &lt;- personenDaten$sex - 1 personenDaten$sexUmkodiert [1] 1 0 0 1 0 1 1 1 0 0 1 1 1 1 1 0 0 1 0 1 eine existierende Variable überschrieben werden soll personenDaten$sex &lt;- personenDaten$sex - 1 personenDaten$sex [1] 1 0 0 1 0 1 1 1 0 0 1 1 1 1 1 0 0 1 0 1 Situation: Einige Items im Datensatz sind umgepolt wurden. Für die weitere Verarbeitung sollen sie zurückgepolt werden. Anwendung der Formel: \\(Item _{Umkodiert} = max(Item)+1 -Item _{Original}\\) Beispiel: Umpolung von Item3 mit 6-stufiger Skala personenDaten$item3_umkodiert &lt;- max(personenDaten$item3) + 1 - personenDaten$item3 Original Umkodiert 1 3 4 2 4 3 3 3 4 4 2 5 5 2 5 5.1.2 Dichotomisierung Situation: Das Alter der Personen soll in eine Dummy Variable überführt werden, bei der alle maximal 30 jährigen den Wert 0 und alle älter als 30 den Wert 1 haben sollen. ifelse(test, yes, no) liefert die Möglichkeit, R-Befehle abhängig von einer Bedingung auszuführen condition: Bedingung, die überprüft werden soll TRUE: Welcher Wert soll zugewiesen werden, wenn die Bedingung erfüllt ist? FALSE: Welcher Wert soll zugewiesen werden, wenn die Bedingung nicht erfüllt ist? personenDaten$alterDichotom &lt;- ifelse(personenDaten$age &lt;= 30, 0, 1) Überprüfung Original Umkodiert 1 25 0 2 27 0 3 38 1 4 31 1 5 22 0 5.1.3 Kategorisierung Situation: Das Alter der Personen soll in eine weitere Variable Altersgruppen überführt werden, bei der: - alle unter 25 den Wert 1 - alle von 25 bis 30 den Wert 2 - alle von 30 bis 35 den Wert 3 - alle über 35 den Wert 4 erhalten Logisches Indizieren ist ein einfacher Weg, Variablenwerte abhängig von Bedingungen festzusetzen: dataset$neueVariable[test] &lt;- wert - test: Bedingung, die überprüft werden soll - wert: Welcher Wert soll zugewiesen werden, wenn die Bedingung erfüllt ist? personenDaten$altersgruppen[personenDaten$age &lt;= 25] &lt;- 1 personenDaten$altersgruppen[personenDaten$age &gt; 25 &amp; personenDaten$age &lt;= 30] &lt;- 2 personenDaten$altersgruppen[personenDaten$age &gt; 30 &amp; personenDaten$age &lt;= 35] &lt;- 3 personenDaten$altersgruppen[personenDaten$age &gt; 35] &lt;- 4 Überprüfung: original umkodiert 1 25 1 2 27 2 3 38 4 4 31 3 5 22 1 5.1.4 Skalenbildung Situation: aus den Werten der Items 1 bis 11 soll eine Summenscore/Mittelwertsscore gebildet werden. rowSums(x, na.rm = TRUE) bildet zeilenweise die Summe rowMeans(x, na.rm = TRUE) bildet zeilenweise den Mittelwert x: Legt den Bereich des Datensatzes fest für den die Funktion durchgeführt werden soll. Hier sollen alle Zeilen und die Spalten von 2 bis 12 einbezigen werden. na.rm: Bei TRUE werden fehlende Werte bei der Berechnung ausgelassen, bei FALSE wird die Berechnung nicht durchgeführt, sobald ein einzelner Wert fehlt. personenDaten$mittelwertsscore &lt;- rowMeans(personenDaten[,2:12], na.rm = T) personenDaten$summenscore &lt;- rowSums(personenDaten[ ,2:12], na.rm = T) 5.1.5 Arbeiten mit Daten (Datum) class(personenDaten$Date) [1] &quot;character&quot; personenDaten$Date &lt;- ymd(personenDaten$Date) class(personenDaten$Date) [1] &quot;Date&quot; Damit können jetzt einzelne Jahre (Monate, Tage) von den Daten extrahiert werden: year(personenDaten$Date[1]) [1] 2022 … oder auch Daten verglichen werden (früher oder später) personenDaten$Date[1] &lt; personenDaten$Date[2] [1] TRUE 5.1.6 Subsets erstellen Situation: Es sollen alle Personen heraus gefiltert werden, deren score von Item 1 über 4 oder von Item 2 über 3 liegt. Der neue Datensatz soll nur die Variablen Person, Item 1 und Item 2 enthalten. Die subset(x, subset, select) -Funktion bietet die Möglichkeit einen Datensatz nach bestimmten Voraussetzungen zu Filtern. - x: Datensatz - subset: logische Aussage, wenn TRUE wird der Eintrag in das Subset übernommen, wenn FALSE aussortiert. - select: Variablen die im Subset beibehalten werden sollen. personenDatenSubset &lt;- subset(personenDaten, personenDaten$item1 &gt; 4 | personenDaten$item2 &gt; 3, select = c(Person,item1,item2)) Überprüfung Person item1 item2 11 11 5 3 12 12 3 5 13 13 4 4 15 15 3 5 16 16 5 1 19 19 5 3 20 20 5 1 5.2 Long- und Wide-Format Wide Format Eigenschaften: Eine Zeile pro Person Messwiederholungen (Items) als mehrere Variablen (Spalten) dargestellt Häufige Variante für Messwiederholungsdaten bei SPSS Praktisch bei der Dateneingabe dataset &lt;- read.table(&quot;data/Daten.txt&quot;, header = TRUE) head(dataset, 5) Person item1 item2 item3 item4 item5 item6 item7 item8 1 1 3 2 3 2 2 1 4 1 2 2 4 1 4 3 1 1 4 1 3 3 3 1 3 3 1 1 3 1 4 4 2 2 2 2 2 2 2 2 5 5 2 2 2 2 1 1 3 2 item9 item10 item11 1 1 3 4 2 1 4 2 3 1 3 4 4 2 2 4 5 2 2 2 Long-Format Eigenschaften: - Eine Zeile pro Bedingung (Item) - Mehrere Zeilen pro Person - Messwiederholung wird in mehreren Zeilen dargestellt datasetLong &lt;- read.table(&quot;data/Daten_Lang.txt&quot;, header = TRUE) head(datasetLong, 14) Person Item Score 1 1 item1 3 2 2 item1 4 3 3 item1 3 4 4 item1 2 5 5 item1 2 6 6 item1 4 7 7 item1 2 8 8 item1 3 9 9 item1 3 10 10 item1 4 11 1 item2 2 12 2 item2 1 13 3 item2 1 14 4 item2 2 Wozu? Viele Modelle in R benötigen das Long Format 5.2.1 Umwandlung Long- und Wide-Format Wide \\(\\rightarrow\\) Long Ebenfalls aus dem Paket reshape2 ermöglicht der Befehl dcast(data, formula, value.var = \"...\") das Transformieren vom Long ins Wide-Format - data: Datensatz, dessen Format geändert werden soll - formula: Welche Variablen sollen nach welcher Bedingung getrennt werden? Format: A ~ B - A: ID-Variablen (vgl. id.vars) - B: Variable(n), die die Messungen (Zeitpunkte, Items, …) enthält \\(\\rightarrow\\) Bildet neue Spalten (vgl. variable.name) - Mit + werden mehrere Variablen in formula kombiniert - value.var: Welche Variable im Long-Format enthält die Werte, die aufgesplittet werden sollen? (vgl. value.name) head(datasetLong,3) Person Item Score 1 1 item1 3 2 2 item1 4 3 3 item1 3 datasetWide &lt;- dcast(datasetLong, Person ~ Item, value.var = &quot;Score&quot;) head(datasetWide,5) Person item1 item10 item11 item2 item3 item4 item5 item6 1 1 3 3 4 2 3 2 2 1 2 2 4 4 2 1 4 3 1 1 3 3 3 3 4 1 3 3 1 1 4 4 2 2 4 2 2 2 2 2 5 5 2 2 2 2 2 2 1 1 item7 item8 item9 1 4 1 1 2 4 1 1 3 3 1 1 4 2 2 2 5 3 2 2 "],["deskriptive-statistik.html", "6 Deskriptive Statistik 6.1 Absolute und Relative Häufigkeiten 6.2 Grundbefehle 6.3 Gruppengetrennte Analyse 6.4 Ergänzungen", " 6 Deskriptive Statistik 6.1 Absolute und Relative Häufigkeiten Absolute und Relative Häufigkeit Die absolute Häufigkeit ist die Anzahl (= ganze Zahl) wie oft ein Merkmal in einer Stichprobe vorkommt. Die relative Häufigkeit hingegen ist der Anteil den eine Merkmalsauspräung in einer Stichprobe ausmacht. 6.1.1 Absolute Häufigkeiten Absolute Häufigkeiten können durch table() in einer Tabelle ausgegeben werden: df_yoga &lt;- read.table(&quot;data/YogaPilates.txt&quot;, header = TRUE) tab &lt;- table(df_yoga$gruppe) tab kontroll pilates yoga 45 46 29 Die Gesamtzahl der Beobachtungen kann mit sum() ausgegeben werden: sum(tab) [1] 120 6.1.2 Relative Häufigkeiten 2 Möglichkeiten: 1. Berechnung der relativen Häufigkeit durch Divsion der absoluten Häufigkeiten mit der Gesamtzahl der Beobachtungen. 2. Verwenden der Funktion prop.table(). tab_rel &lt;- tab/sum(tab) tab_rel &lt;- prop.table(tab) kontroll pilates yoga 0.3750000 0.3833333 0.2416667 6.1.3 Kreuztabelle für mehrere Variablen Wie ist die (absolute und relative) Häufigkeit der Frauen und Männer (geschl) in den einzelnen Gruppen (gruppe)? Bei Angabe von mehreren Vektoren werden Kreuztabellen erzeugt, dessen Werte mit round() gerundet werden können: tab2 &lt;- table(df_yoga$geschl, df_yoga$gruppe) round(prop.table(tab2), 2) kontroll pilates yoga m 0.12 0.10 0.05 w 0.26 0.28 0.19 6.2 Grundbefehle R-Befehl Bedeutung sum() Summe mean() Mittelwert var() Varianz sd() Standardabweichung min() Minimum max() Maximum quantile() Quartile range() Spannweite median() Median Beispiele range(df_yoga$alter) [1] 21 40 mean(df_yoga$alter) [1] 30.73333 var(df_yoga$zufri) [1] NA Fehlende Werte Enthalten Daten fehlende Werte (NA), dann ergeben die deskriptiven Berechnungen auch NA. Durch das Argument na.rm = TRUE werden NA-Werte ignoriert: mean(df_yoga$zufri) [1] NA mean(df_yoga$zufri, na.rm = TRUE) [1] 3.543103 Funktion summary() Mit summary() werden verschiedene deskriptive Statistiken ausgegeben: summary(df_yoga$alter) Min. 1st Qu. Median Mean 3rd Qu. Max. 21.00 26.00 30.50 30.73 35.25 40.00 summary() gibt zudem die Anzahl fehlender Werte an: summary(df_yoga$zufri) Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s 1.000 3.000 4.000 3.543 4.000 5.000 4 Paket psych Mit der Funktion describe() aus dem Paket psych lassen sich eine Vielzahl von Verteilungsparametern gleichzeitig ausgeben: library(psych) describe(df_yoga$alter, skew = FALSE) vars n mean sd median min max range se X1 1 120 30.73 5.76 30.5 21 40 19 0.53 Optionale Argumente: - skew = FALSE, um Schiefe und Kurtosis nicht auszugeben - ranges = FALSE, um u.a. Range, Minimum und Maximum nicht auszugeben - IQR = TRUE, um Interquartilbereich auszugeben 6.3 Gruppengetrennte Analyse Wie können wir für die Variablen alter, zufri und angst deskriptive Statistien berechnen, je nachdem in welcher gruppe die Person ists? head(df_yoga, 5) vp geschl alter gruppe zufri angst 1 AA21 w 37 yoga 5 1 2 AW14 m 31 pilates 4 4 3 BA55 w 38 yoga 4 2 4 BA76 m 35 yoga 5 2 5 BP45 w 23 pilates 4 1 6.3.1 Logisches Indizieren Jeweils Teile der Daten durch logisches Indizieren auswählen: „Berechne den Mittelwert der Spalte alter, aber wähle hierfür nur Werte der Personen aus… …gruppe == yoga.“ mean(df_yoga$alter[df_yoga$gruppe == &quot;yoga&quot;]) [1] 31.13793 …gruppe == pilates.” mean(df_yoga$alter[df_yoga$gruppe == &quot;pilates&quot;]) [1] 30.86957 6.3.2 Funktion aggregate() Mit aggregate() können Funktionen für verschiedene Faktorenstufen (und deren Kombination) getrennt berechnet werden aggregate(AV ~ UV, FUN = …, data = …) AV: Variable, deren Werte analysiert werden sollen UV: Faktor(en), mehrere Faktoren werden mit + verbunden FUN: Welche Funktion (length, sum, mean, ...) soll berechnet werden? data: Datensatz Beispiel Durchschnittliche Zufriedenheit, Gruppen = geschl, gruppe aggregate(zufri ~ geschl + gruppe, FUN = mean, data = df_yoga) geschl gruppe zufri 1 m kontroll 3.785714 2 w kontroll 3.258065 3 m pilates 3.181818 4 w pilates 3.454545 5 m yoga 4.166667 6 w yoga 3.952381 6.3.3 Funktion describeBy() aus dem Paket psych Anwendung der describe-Funktion getrennt nach Faktor(en) describeBy(x = …, group = list(…), …) x: Variable, deren Werte analysiert werden sollen (wie AV bei aggregate()) group = list(): Faktor(en) Beispiel describeBy() Deskriptive Statistiken zu Zufriedenheit nach Gruppe (= geschl) describeBy(x = df_yoga$zufri, group = list(df_yoga$geschl), skew = FALSE) Descriptive statistics by group : m vars n mean sd median min max range se X1 1 31 3.65 0.95 4 2 5 3 0.17 --------------------------------------------- : w vars n mean sd median min max range se X1 1 85 3.51 1.06 4 1 5 4 0.12 6.4 Ergänzungen 6.4.1 Bedingte Wahrscheinlichkeiten Mit prop.table(…, margin = …) werden bedingte Wahrscheinlichkeiten für ein Merkmal ausgegeben. (margin 1 = zeilenweise, margin 2 = spaltenweise) Bedingte Wahrscheinlichkeit, zeilenweise, gerundet auf 2 Stellen round(prop.table(tab2, margin = 1), 2) kontroll pilates yoga m 0.44 0.38 0.19 w 0.35 0.39 0.26 Bedingte Wahrscheinlichkeit, spaltenweise, gerundet auf 2 Stellen round(prop.table(tab2, margin = 2), 2) kontroll pilates yoga m 0.31 0.26 0.21 w 0.69 0.74 0.79 6.4.2 Skalieren Mit dem Befehl scale(…, center = …, scale = …) können Variablen zentriert und z-standardisiert werden. center: Soll von jedem Wert in der Variable der Variablenmittelwert abgezogen werden? (TRUE = ja) scale: Soll jeder Wert in der Variable durch die Variablenstandardabweichung dividiert werden? (TRUE = ja) Zentrierung, wenn nur center = TRUE, z-Standardisierung, wenn beide Argumente mit TRUE definiert wurden. 6.4.3 Extremwerte Mit dem Argument trim = … im Befehl mean() kann der Anteil ausgeschlossener Extremwerte definiert werden Das Paket moments gibt mit dem Befehl all.moments() alle statistischen Momente aus, mit skewness() die Schiefe und mit kurtosis() den Exzess 6.4.4 Gruppengetrennte Analyse - Funktion tapply() tapply(X = …, INDEX = list(…), FUN = …) X: Variable, deren Werte analysiert werden sollen (wie AV bei aggregate()) INDEX = list(): Faktor(en), mehrere Faktoren werden mit “,” verbunden (wie UV bei aggregate()) Bei einem Faktor muss list() nicht angegeben werden FUN: Welche Funktion (deskriptive Statistik) soll berechnet werden? (wie bei aggregate()) "],["tests-zusammenhangsmaße-und-regression.html", "7 Tests, Zusammenhangsmaße und Regression 7.1 Statistisches Testen 7.2 Zusammenhangsmaße 7.3 Lineare Regressionsmodelle 7.4 Ergänzungen", " 7 Tests, Zusammenhangsmaße und Regression 7.1 Statistisches Testen Ziel ist mit Stichprobendaten allgemein gültige Aussagen über die Grungesamtheit treffen zu können. Dazu werden Hypothesentests durchgeführt häufig in Form von Signifikanztests. Allgemeines Vorgehen bei Signifikanztests: Null- und Alternativhypothese formulieren (\\(H_0\\ und\\ H_a\\)) Wahl eines passenden Tests (z.B. t-Test, …) Bestimmung des Signifikanzniveaus \\(\\alpha\\) Berechnung des Wertes der Beobachtung (z.B. t-Wert, …) Treffen der Testentscheidung 7.1.1 \\(\\chi^2\\)-Test Gleichverteilungstest - sind Kategorien gleich häufig? \\(H_0\\): Kategorien sind gleichhäufig \\(H_a\\): Kategorien sind nicht gleichhäufig Unanghängigkeitstest - sind zwei nominale Merkmale unabhängig voneinander? \\(H_0\\): Merkmale sind unabhängig \\(H_a\\): Merkmale sind abhängig Die Funktion chisq.test() berechnet je nach Eingabe einen entsprechenden Test. Eingabeparameter sind dabei im Format von table(): Gleichverteilungstest: table(x) Unanghängigkeitstest: table(x, y) Beispiel df_yoga &lt;- read.table(&quot;data/YogaPilates.txt&quot;, header = TRUE) Gleichverteilungstest: tab &lt;- table(df_yoga$gruppe) chisq.test(tab) Chi-squared test for given probabilities data: tab X-squared = 4.55, df = 2, p-value = 0.1028 Unanghängigkeitstest: tab2 &lt;- table(df_yoga$gruppe, df_yoga$geschl) chisq.test(tab2) Pearson&#39;s Chi-squared test data: tab2 X-squared = 0.99223, df = 2, p-value = 0.6089 7.1.2 t-Test Varianten des t-Tests: Einstichproben t-Test: Vergleich eines Mittelwerts mit einer Konstante \\(H_0\\): Mittelwert weicht nicht von Konstante ab \\(H_a\\): Mittelwert weicht von Konstante ab Zweistichproben t-Test: Vergleich von zwei Mittelwerten aus ab- oder unabhängigen Stichproben \\(H_0\\): Mittelwerte weichen nicht voneinander ab \\(H_a\\): Mittelwert weichen voneinander ab Alle Varianten des t-Tests werden mit t.test() berechnet. Zwei Möglichkeiten der Dateneingabe t.test(Werte1, Werte2) t.test(Werte ~ Faktor) Optionale Argumente: alternative: Richtung der Testung (\"two.sided\", \"greater\", \"less\") mu: Konstante, gegen die getestet werden soll paired: unabhängige (FALSE) oder abhängige (TRUE) Stichproben? var.equal: Annahme von Varianzgleichheit gegeben (ja = TRUE)? conf.level: Konfidenzniveau Beispiel t-Test - Einstichprobentest t.test(df_ex$Revise, mu = 5, alternative = &quot;greater&quot;) One Sample t-test data: df_ex$Revise t = 8.4094, df = 101, p-value = 1.395e-13 alternative hypothesis: true mean is greater than 5 95 percent confidence interval: 16.41727 Inf sample estimates: mean of x 19.22549 t-Test - Zweistichprobentest - abhängige Stichproben t.test(df_ex$Note, df_ex$Note_2, paired = TRUE) Paired t-test data: df_ex$Note and df_ex$Note_2 t = 3.3339, df = 101, p-value = 0.001198 alternative hypothesis: true mean difference is not equal to 0 95 percent confidence interval: 0.2382293 0.9382413 sample estimates: mean difference 0.5882353 t-Test - Zweistichprobentest - unhängige Stichproben t.test(df_ex$Revise ~ df_ex$bestanden) Welch Two Sample t-test data: df_ex$Revise by df_ex$bestanden t = 4.4799, df = 96.44, p-value = 2.053e-05 alternative hypothesis: true difference in means between group bestanden and group durchgefallen is not equal to 0 95 percent confidence interval: 6.707692 17.380155 sample estimates: mean in group bestanden mean in group durchgefallen 23.35821 11.31429 7.1.3 ANOVA Vergleich von Mittelwerten von mehr als zwei Gruppen: Abhängige (within ANVOA) oder unabhängige (between ANOVA) Stichproben? Post-hoc-Tests für genauere Gruppenvergleiche Mehrfaktorielle ANOVA für mehrere unabhängige Variablen und deren Interaktion Paket ez Varianzanalysen lassen sich in R auf viele (auch umständliche Arten) berechnen. Das Paket ez bietet mit der Funktion ezANOVA() eine benutzerfreundliche Möglichkeit zum Durchführen von ANOVAs Wichtige Argumente: data: Dataframe dv: Abhängige Variable wid: Variable, die die Fälle im Dataframe eindeutig unterscheidet (ID-Variable) between: Zwischensubjektfaktoren, anzugeben als Liste mit .(Faktor 1, Faktor 2, …) within: Innersubjektfaktoren, anzugeben als Liste mit .(Faktor 1, Faktor 2, …) type: Art der Quadratsummenzerlegung (Typ 2 ist Default, Typ 3 ist der Standard von SPSS) Beispiel df_yoga_clean &lt;- read.table(&quot;data/YogaPilates.txt&quot;, header = TRUE) |&gt; na.omit() library(ez) Einfaktorielle ANOVA - Between-Faktor ezANOVA(data = df_yoga_clean, dv = zufri, wid = vp, between = gruppe, type = 2) Coefficient covariances computed by hccm() $ANOVA Effect DFn DFd F p p&lt;.05 ges 1 gruppe 2 113 3.610455 0.0302038 * 0.06006367 $`Levene&#39;s Test for Homogeneity of Variance` DFn DFd SSn SSd F p p&lt;.05 1 2 113 0.7027865 71.73687 0.553515 0.5764761 Post-hoc-Tests Post-hoc-Tests für paarweise Gruppenvergleiche können mit pairwise.t.test(x, g, p.adjust.method = \"…\", paired = …, …) ausgegeben werden. x: Vektor der abhängigen Variable g: Vektor der unabhängigen Variable (Gruppe) p.adjust.method: Korrekturmethode für multiples Testen?p.adjust paired: Innersubjekt(within)- (TRUE) oder Zwischensubjekt(between)faktoren (FALSE)? pairwise.t.test(df_yoga_clean$zufri, df_yoga_clean$gruppe, p.adjust.method = &quot;bonferroni&quot;, paired = FALSE) Pairwise comparisons using t tests with pooled SD data: df_yoga_clean$zufri and df_yoga_clean$gruppe kontroll pilates pilates 1.000 - yoga 0.062 0.043 P value adjustment method: bonferroni Einfaktorielle ANOVA: Within-Faktoren z.B. „Unterscheiden sich die Zufriedenheitswerte der Teilnehmenden vor und nach dem Training?“ „Problem“: in diesem Datensatz haben wir keine within-Faktoren -&gt; angst sei die Zufriedenheitsmessung vor der jeweiligen Intervention (Gruppe) -&gt; Dadurch kann der Faktor Zeit mit aufgenommen werden df_yoga &lt;- read.table(&quot;data/YogaPilates.txt&quot;, header = T) head(df_yoga, n=5) vp geschl alter gruppe zufri angst 1 AA21 w 37 yoga 5 1 2 AW14 m 31 pilates 4 4 3 BA55 w 38 yoga 4 2 4 BA76 m 35 yoga 5 2 5 BP45 w 23 pilates 4 1 # Zufriedenheit t_1 df_yoga$zufri_t1 &lt;- df_yoga$angst # Zufriedenheit t_2 df_yoga$zufri_t2 &lt;- df_yoga$zufri head(df_yoga, n=5) vp geschl alter gruppe zufri angst zufri_t1 zufri_t2 1 AA21 w 37 yoga 5 1 1 5 2 AW14 m 31 pilates 4 4 4 4 3 BA55 w 38 yoga 4 2 2 4 4 BA76 m 35 yoga 5 2 2 5 5 BP45 w 23 pilates 4 1 1 4 Einfaktorielle ANOVA: Within-Faktoren Achtung!: Für ANOVA müssen auch within-Faktoren ins Long-Format gebracht werden library(reshape2) df_yogaL &lt;- melt(df_yoga, id.vars = c(&quot;vp&quot;, &quot;geschl&quot;, &quot;alter&quot;, &quot;gruppe&quot;, &quot;angst&quot;,&quot;zufri&quot;), variable.name = &quot;Zeitpunkt&quot;, value.name = &quot;Zufriedenheit&quot;) head(df_yogaL, n=5) vp geschl alter gruppe angst zufri Zeitpunkt 1 AA21 w 37 yoga 1 5 zufri_t1 2 AW14 m 31 pilates 4 4 zufri_t1 3 BA55 w 38 yoga 2 4 zufri_t1 4 BA76 m 35 yoga 2 5 zufri_t1 5 BP45 w 23 pilates 1 4 zufri_t1 Zufriedenheit 1 1 2 4 3 2 4 2 5 1 Einfaktorielle ANOVA: Within-Faktoren Deskriptive Analysen library(psych) describeBy(df_yogaL$Zufriedenheit, group = df_yogaL$Zeitpunkt) Descriptive statistics by group group: zufri_t1 vars n mean sd median trimmed mad min max range X1 1 120 2.49 1.33 2 2.36 1.48 1 5 4 skew kurtosis se X1 0.49 -0.95 0.12 --------------------------------------------- group: zufri_t2 vars n mean sd median trimmed mad min max range X1 1 116 3.54 1.03 4 3.59 1.48 1 5 4 skew kurtosis se X1 -0.42 -0.56 0.1 Vergleiche Output der between-ANOVA für die Interpretation der Werte Bei mehr als zwei Stufen des Innersubjektfaktors wird der Mauchly-Test (Test auf Sphärizität) mit ausgegeben library(ez) ezANOVA(na.omit(df_yogaL), dv = Zufriedenheit, wid = vp, within = .(Zeitpunkt)) 7.2 Zusammenhangsmaße Kovarianz: nichtstandardisiertes Zusammenhangsmaß monotoner Zusammenhang Korrelationskoeffizient (nach Bravais-Pearson): standardisiertes Zusammenhangsmaß linearer Zusammenhang Variablen mindestes intervallskaliert Beispieldatasatz Wie hängen die Itemantworten zusammen? df_exam &lt;- read.table(&quot;data/Daten.txt&quot;, header = TRUE) head(df_exam, c(5,8)) Person item1 item2 item3 item4 item5 item6 item7 1 1 3 2 3 2 2 1 4 2 2 4 1 4 3 1 1 4 3 3 3 1 3 3 1 1 3 4 4 2 2 2 2 2 2 2 5 5 2 2 2 2 1 1 3 Kovarianz und Korrelation - zwei Variablen Für die Berechnung der Korrelation wird cor() verwendet, für die Berechnung der Kovarianz cov() Argumente bei cor(x, y, method = \"…\", use = \"…\") x und y: Datenvektoren method: Welche Korrelation soll berechnet werden? Produkt-Moment-Korrelation (\"pearson\"), Rangkorrelation \\(\\rho\\) (\"spearman\") oder Rangkorrelation \\(\\tau\\) (\"kendall\") use: Umgang mit fehlenden Werten (siehe nächste Folie) use = \"…\" regelt den Umgang der Korrelationsfunktionen mit fehlenden Werten \"everything\": Kein Umgang mit fehlenden Werten \\(\\rightarrow\\) bei einzelnen fehlenden Werten wird keine (einzige) Korrelation berechnet (Ergebnis: NA) \"pairwise\": Korrelationen werden jeweils mit den vollständigen Fällen pro Variablenpaar berechnet (paarweiser Ausschluss) \"complete\": Korrelationen werden nur aus in allen Variablen vollständigen Fällen berechnet (fallweiser Ausschluss) everything pairwise complete Funktion cor.test() cor.test(x, y, method = \"…\", use = \"…\", alternative = \"…\", conf.level = …, …) für Inferenzstatistik bei Korrelationen x, y, method, use: Siehe cor() alternative: Ist die Testrichtung… Ungerichtet (default): \"two.sided\" einseitig: Positive Korrelationshypothese \"greater\" einseitig: Negative Korrelationshypothese \"less\" conf.level: Konfidenzniveau (1 - \\(\\alpha\\)) default: 0.95 Beispiel Korrelation item1, item2 cor.test(df_exam$item1, df_exam$item2, method = &quot;pearson&quot;, alternative = &quot;less&quot;, conf.level = .99) Pearson&#39;s product-moment correlation data: df_exam$item1 and df_exam$item2 t = -3.4112, df = 8, p-value = 0.004603 alternative hypothesis: true correlation is less than 0 99 percent confidence interval: -1.0000000 -0.1396423 sample estimates: cor -0.7698004 Produkt-Moment-Korrelation, Hypothese: negative Korrelation, alpha = 1% cor.test(df_exam$item1, df_exam$item2, method = &quot;pearson&quot;, alternative = &quot;less&quot;, conf.level = .99) Pearson&#39;s product-moment correlation data: df_exam$item1 and df_exam$item2 t = -3.4112, df = 8, p-value = 0.004603 alternative hypothesis: true correlation is less than 0 99 percent confidence interval: -1.0000000 -0.1396423 sample estimates: cor -0.7698004 7.2.1 Matrizen Werden dataframes (oder Teile dessen) in cov() oder cor() eingefügt, werden Kovarianz- und Korrelationsmatrizen ausgegeben: cov(df_exam[, 2:4]) item1 item2 item3 item1 0.6666667 -0.4444444 0.3333333 item2 -0.4444444 0.5000000 -0.2222222 item3 0.3333333 -0.2222222 0.4444444 cor(df_exam[, c(&quot;item1&quot;, &quot;item2&quot;, &quot;item3&quot;)], method = &quot;pearson&quot;) item1 item2 item3 item1 1.0000000 -0.7698004 0.6123724 item2 -0.7698004 1.0000000 -0.4714045 item3 0.6123724 -0.4714045 1.0000000 Funktion corr.test() corr.test() (Paket psych) liefert inferenzstatistische Berechnungen auch für Korrelationsmatrizen. corr.test(df, method = \"…\", adjust = \"…\", alpha = \"…\") df, method: Siehe cor() adjust: Methode zur \\(\\alpha\\)-Fehler-Adjustierung \"none\", \"bonferroni\", … Für weitere siehe ?p.adjust und ?corr.test alpha: Signifikanzniveau \\(\\alpha\\) library(psych) corr.test(df_exam[ ,2:4], method = &quot;pearson&quot;, adjust = &quot;bonferroni&quot;) Call:corr.test(x = df_exam[, 2:4], method = &quot;pearson&quot;, adjust = &quot;bonferroni&quot;) Correlation matrix item1 item2 item3 item1 1.00 -0.77 0.61 item2 -0.77 1.00 -0.47 item3 0.61 -0.47 1.00 Sample Size [1] 10 Probability values (Entries above the diagonal are adjusted for multiple tests.) item1 item2 item3 item1 0.00 0.03 0.18 item2 0.01 0.00 0.51 item3 0.06 0.17 0.00 To see confidence intervals of the correlations, print with the short=FALSE option Extraktion von Werten Mit $ können Werte aus der Berechnung extrahiert werden Dafür Korrelationsmatrix als Objekt speichern $se für Extraktion der Standardfehler $ci für Extraktion der Konfidenzintervalle $p für Extraktion der p-Werte $t für Extraktion der t-Werte Beispiel Definition eines Objekts mit Inferenzstatistik für df_exam KorMat &lt;- corr.test(df_exam[ ,2:4], method = &quot;pearson&quot;, adjust = &quot;bonferroni&quot;) Ausgabe Konfidenzintervalle KorMat$ci lower r upper p item1-item2 -0.9425738 -0.7698004 -0.2720171 0.00920665 item1-item3 -0.0280809 0.6123724 0.8963838 0.05983788 item2-item3 -0.8490310 -0.4714045 0.2250053 0.16902020 7.3 Lineare Regressionsmodelle Ziel ist ein Kriterium (abhängige Variable) durch einen Prädiktor (unabhängige Variable) oder einer Kombination aus Prädikatoren für neue Prädikatorenwerte vorherzusagen bzw. den Zusammenhang dieser zu untersuchen. Allgemeines Vorgehen: Abhängige und unabhängige Variablen aus Hypothesen ableiten Skalenniveaus der Variablen bestimmen Erwarteten Zusammenhang bestimmen Passendes Regressionsverfahren wählen Vorraussetzungen prüfen Regressionsmodell berechnen Interpretation der Regressionsanalyse Lineare Regressionsmodelle Für lineare Regressionen sind folgende Voraussetzungen zu überprüfen: Es existiert ein linearer Zusammenhang Erwartungswert der Residuen ist Null Varianzhomogenität der Residuen (Homoskedastizität) Die Residuen sind nicht miteinander Korreliert (Autoregression) Die Residuen sind nicht mit den unabhängigen Variablen korreliert und die unabhängige Variable ist deterministisch(z.B. Omitted-Variable-Bias) Normalverteilung der Residuen 7.3.1 Modelldefinition Um eine Regression in R zu rechnen, muss man anhand der vorliegenden Variablen ein Regressionsmodell spezifizieren Regressionsmodelle werden in sogenannten Formel-Objekten definiert Struktur: Abhängige Variable(n) ~ Unabhängige Variable + Unabhängige Variable + … Mehrere Variablen werden mit + verbunden Variablenbezeichnungen entsprechend anpassen item1 soll durch item2 und item3 vorhergesagt werden Beispiel Modelldefinition model1 &lt;- item1 ~ item2 + item3 7.3.2 Modellschätzung Anhand des definierten Regressionsmodells und der vorhandenen Daten kann R nun die Modellparameter (inkl. Modellpassung) schätzen Lineare Regressionsmodelle werden mit lm(formula = …, data = …) geschätzt: formula: Definiert das Modell data: dataframe, der die Variablen enthält lm() sollte für die weiteren Berechnungen als Objekt gesichert werden Beispiel Modellschätzung Modelldefinition model1 &lt;- item1 ~ item2 + item3 Modellschätzung und Sicherung in fit1 fit1 &lt;- lm(formula = model1, data = df_exam) Modell anzeigen fit1 Call: lm(formula = model1, data = df_exam) Coefficients: (Intercept) item2 item3 2.8929 -0.7143 0.3929 Mit summary() lassen sich detaillierte Informationen zum Regressionsmodell ausgeben: summary(fit1) Call: lm(formula = model1, data = df_exam) Residuals: Min 1Q Median 3Q Max -0.75000 -0.33036 -0.08929 0.33036 0.64286 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 2.8929 1.1752 2.462 0.0434 * item2 -0.7143 0.2832 -2.523 0.0397 * item3 0.3929 0.3003 1.308 0.2322 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.5297 on 7 degrees of freedom Multiple R-squared: 0.6726, Adjusted R-squared: 0.5791 F-statistic: 7.191 on 2 and 7 DF, p-value: 0.02008 7.3.3 Modellvergleich Zwei genestete Modelle können mit anova(fit1, fit2) miteinander verglichen werden fit1: Ausgangsmodell (restringiertes Modell) fit2: Erweitertes Modell (mit zusätzlichen Prädiktoren) Beispiel Modellvergleich Modelldefinition und Schätzung model1 &lt;- item1 ~ item2 + item3 model2 &lt;- item1 ~ item2 + item3 + item4 fit1 &lt;- lm(model1, df_exam) fit2 &lt;- lm(model2, df_exam) Modellvergleich, fit1 + fit2 anova(fit1, fit2) Analysis of Variance Table Model 1: item1 ~ item2 + item3 Model 2: item1 ~ item2 + item3 + item4 Res.Df RSS Df Sum of Sq F Pr(&gt;F) 1 7 1.96429 2 6 0.80569 1 1.1586 8.6282 0.02604 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 7.4 Ergänzungen 7.4.1 Matrizen Kovarianzmatrizen werden mit cov2cor() in Korrelationsmatrizen umgewandelt: # Zuweisung Kova &lt;- cov(df_exam[ , 2:4]) # Berechnung cov2cor(Kova) item1 item2 item3 item1 1.0000000 -0.7698004 0.6123724 item2 -0.7698004 1.0000000 -0.4714045 item3 0.6123724 -0.4714045 1.0000000 7.4.2 Weitere Korrelationen Mit dem Paket psych können zudem Partialkorrelationen (partial.r()), tetrachorische (tetrachoric()), polychorische (polychoric()), biserale (biserial()) und polyseriale (polyserial()) Korrelationen berechnet werden Mit dem Paket ppcor können Semipartialkorrelationen berechnet werden 7.4.3 Standartisierte und unstandartisierte Regressionkoeffizienten Mit coefficients() oder \\$coefficients lassen sich die unstandardisierten Regressionskoeffizienten ausgeben Mit lm.beta() aus dem Paket QuantPsyc lassen sich standardisierte Regressionskoeffizienten ausgeben # Unstandartisierte Regressionskoeffizienten coefficients(fit1) (Intercept) item2 item3 2.8928571 -0.7142857 0.3928571 # Ausgabe fit1$coefficients (Intercept) item2 item3 2.8928571 -0.7142857 0.3928571 # Standartisierte Regressionskoeffizienten QuantPsyc::lm.beta(fit1) item2 item3 -0.6185896 0.3207665 7.4.4 Vorhergesagte Werte predict(), fitted.values() oder \\$fitted.values gibt die vorhergesagten Werte für jeden Datenpunkt als Vektor aus: # Vorhersage Werte, fit1 predict(fit1) 1 2 3 4 5 6 2.642857 3.750000 3.357143 2.250000 2.250000 3.357143 7 8 9 10 1.928571 3.750000 3.357143 3.357143 # Alternative 1 fitted.values(fit1) 1 2 3 4 5 6 2.642857 3.750000 3.357143 2.250000 2.250000 3.357143 7 8 9 10 1.928571 3.750000 3.357143 3.357143 # Alternative 2 fit1$fitted.values 1 2 3 4 5 6 2.642857 3.750000 3.357143 2.250000 2.250000 3.357143 7 8 9 10 1.928571 3.750000 3.357143 3.357143 7.4.5 Residuen resid(), residuals() oder \\$residuals gibt die unstandardisierten Residuen pro Datenpunkt aus rstandard() gibt die standardisierten Residuen aus # Residuen, fit1 fit1$residuals 1 2 3 4 5 0.35714286 0.25000000 -0.35714286 -0.25000000 -0.25000000 6 7 8 9 10 0.64285714 0.07142857 -0.75000000 -0.35714286 0.64285714 7.4.6 Modelldefinition Spezielle Modelle: Modell ohne Prädiktoren (Nullmodell): AV ~ 1 Modell ohne Intercept (Konstante): AV ~ 0 + UV1 + … Interaktionsterme (Moderation): AV ~ UV1 + UV2 + UV1*UV2 Statt \\(*\\) kann auch \\(:\\) verwendet werden 7.4.7 Hinweise zu weiteren Regressionsmodellen in R Literatur: Fox, J., &amp; Weisberg, S. (2011). An R companion to applied regression. London: SAGE. James, G., Witten, D., Hastie, T., &amp; Tibshirani, R. (2013). An introduction to statistical learning: With applications in R. Springer texts in statistics. New York: Springer. In R sind viele weitere Verfahren zur Modellselektion, -schätzung und -bewertung umgesetzt wie etwa: Bootstrapping Kreuzvalidierung Shrinkage (Lasse/Ridge) Regression Robust Regression Power Analyse 7.4.8 General Linear Model (GLM) glm(forumla, family, data) Generalisierte lineare Modelle als Verallgemeinerung des linearen Modells \\(\\rightarrow\\) Breite Einsatzmöglichkeiten formula: Wie bei lm() data: Wie bei lm() family: Definiert die Zufallsverteilung von Y und die Linkfunktion Beispiele: GLM mit Normalverteilung und ohne besondere Linkfunktion (familiy = gaussian(link = \"identity\")) \\(\\rightarrow\\) Entspricht lm() GLM mit Binomialverteilung und logistischer Linkfunktion (family = binomial(link = \"logit\")) \\(\\rightarrow\\) Binär-logistische Regression 7.4.9 Mediation und SEM Paket lavaan Ständig weiterentwickeltes Paket zur Schätzung von Pfad- und Strukturgleichungsmodellen in R Ermöglicht auch Bootstrapping-Konfidenzintervalle für indirekte Effekte Ausführliche Dokumentation mit Beispielcodes: http://lavaan.ugent.be/tutorial/est.html 7.4.10 Linear Mixed Model (LMM) Paket lme4 Liefert eine breite und gut erprobte Palette an Werkzeugen für linear mixed models (Hierarchische lineare Modelle; Mehrebenenmodelle) Siehe: Bates, D., Mächler, M., Bolker, B., &amp; Walker, S. (2015). Fitting Linear Mixed-Effects Models Using lme4. Journal of Statistical Software, 67(1). Paket lmerTest Reicht Signifikanztests nach, die in lme4 standardmäßig nicht implementiert sind https://cran.r-project.org/web/packages/lmerTest/index.html "],["grafiken.html", "8 Grafiken 8.1 Allgemeines 8.2 Grafiken für univariate Verteilungen 8.3 Grafiken für bivariate Verteilungen 8.4 ggplot 8.5 Ergänzungen", " 8 Grafiken 8.1 Allgemeines Grafiksysteme Mit R lassen sich Graphiken auf ganz unterschiedliche Weise erzeugen: Mit dem vorinstallierten Graphiksystem (base R) lassen sich schnell und unkompliziert vollständige (High-Level) Graphiken erzeugen. Mit dem vorinstallierten Graphiksystem (base R) lassen sich jedoch auch (Low-Level) Graphiken von Grund auf aufbauen und können sehr individuell gestaltet werden. Mit verschiedenen Packages lassen sich mit vertretbarem Aufwand ansprechende und beliebig komplexe (Medium-Level) Graphiken erstellen. Hierunter zählt v.a. ggplot2 aus dem tidyverse. Grundfunktionen in base R Grafiktyp Base R Funktion (Datentyp abhängig) plot() Balkendiagramm barplot() Histogram hist() Boxplot boxplot() Kreisdiagramm pie() Argumente Grundfunktion in base R Fast alle Grafiken können mit folgenden Argumenten näher definiert werden: main = \"…\": Überschrift col = \"…\": Farbe; Mehrere Farben mit c(\"…\", \"…\", …) eingeben xlab = \"…\": Titel der x-Achse ylab = \"…\": Titel der y-Achse xlim = c(…, …): Achsenlimits x-Achse ylim = c(…, …): Achsenlimits y-Achse … Farben Farben können als: Farbname (“red”), siehe die Datei Farben in R (im moodle-Kurs) HEX-Code (“#FF8000FF”), oder RGB-Code (“229, 245, 249”) eingetragen werden. 8.2 Grafiken für univariate Verteilungen 8.2.1 Kreisdiagramme Kreisdiagramme mit pie() erstellen Datenformat: Variable in Häufigkeitstabelle (table()) 8.2.2 Balkendiagramme Balkendiagramme mit barplot() erstellen Datenformat: Variable in Häufigkeitstabelle (table()) tab &lt;- table(data$gruppe) barplot(tab) Mit einigen zusätzlichen Optionen: barplot(tab, main = &quot;Absolute Häufigkeiten der Gruppen&quot;, col = c(&quot;coral2&quot;, &quot;darkcyan&quot;, &quot;goldenrod3&quot;), names.arg = c(&quot;Kontrollgruppe&quot;, &quot;Pilates&quot;, &quot;Yoga&quot;), ylab = &quot;Absolute Häufigkeit&quot;, ylim = c(0, 50)) Beschriften der Balken Es gibt auch die Möglichkeit die jeweiligen Häufigkeiten über den einzelnen Balken abzutragen mit dem Befehl text(x,y,labels) können diese Labels eingefügt werden x: die x Koordinaten der Labels, hier den bereits erstellten Barplot y: die y Koordinaten der Labels, da diese dem table entnommen werden kann hier das erstellte table genommen werden. Es muss allerdings noch ein gewisser Wert aufgrechnet werden, damit die Zahlen über den Balken stehen labels: die eigentlichen Inhalte der Labels. Hier wird auch das table angegeben bar&lt;- barplot(tab, main = &quot;Absolute Häufigkeiten der Gruppen&quot;, col = c(&quot;coral2&quot;, &quot;darkcyan&quot;, &quot;goldenrod3&quot;), names.arg = c(&quot;Kontrollgruppe&quot;, &quot;Pilates&quot;, &quot;Yoga&quot;), ylab = &quot;Absolute Häufigkeit&quot;, ylim = c(0, 50)) text(x = bar, y = tab+2, labels = tab) 8.2.3 Histogramme hist(x, breaks, freq, …) zeigt die Verteilung einer Variablen in einem Histogramm. x: Variable, für die das Histrogramm erzeugt werden soll (wird mit dem $-Operator ausgewählt) breaks: Wie viele Balken sollen gebildet werden? Keine Angabe: Automatische Auswahl Einzelne Zahl n: Daten werden in n Balken geteilt Vektor: Balken werden zwischen den angegebenen Stellen gebildet z.B. breaks = c(0,1,2,3) \\(\\rightarrow\\) 3 Balken: 0 bis 1, 1 bis 2 und 2 bis 3 z.B. breaks = seq(0,100,20) \\(\\rightarrow\\) erstellt Werte im Schritt von 20 im Bereich zwischen 0 und 100 (5 Balken) freq: Soll statt der absoluten die relative Häufigkeit geplottet werden? (TRUE = ja) hist(data$alter) Mit einigen zusätzlichen Optionen: hist(data$alter, main = &quot;Histogramm von Alter&quot;, col = &quot;darkolivegreen3&quot;, xlab = &quot;Alter&quot;, ylab = &quot;Häufigkeit&quot;, ylim = c(0, 20)) 8.2.4 Boxplot boxplot(x, range =, …) erstellt einen bzw. mehrere Boxplots x: kann Vektor, Dataframe oder Formel sein Bei Eingabe eines Dataframes werden Boxplots für alle enthaltenen Variablen erzeugt Formeln ermöglichen Trennung nach Gruppen (s.u.) range: Definiert die maximale Länge des Whiskers Werte außerhalb werden als Ausreißer dargestellt z.B.: range = 2 (2 * Interquatilabstand bzw. Boxlänge) default: range = 1.5 ...: Weitere Graphikparameter boxplot(data$zufri, col = &quot;darksalmon&quot;, main = &quot;Boxplot der Zufriedenheitswerte&quot;, ylab = &quot;Zufriedenheitswert&quot;, ylim = c(0, 6)) 8.3 Grafiken für bivariate Verteilungen 8.3.1 Verteilung einer metrischen auf eine kategoriale Variabel Mithilfe von Boxplots lässt sich auch die Verteilung einer kategorialen Variable auf eine metrische Variable boxplot(A ~ B + C + …) erzeugt Boxplots für alle möglichen Kombinationen von Faktorstufen getrennt A: metrische abhängige Variable B + C: Kategoriale Variablen nach deren Ausprägungen aufgeschlüsselt wird boxplot(data$angst ~ data$gruppe, col = &quot;deeppink&quot;, main = &quot;Boxplots der Angstwerte nach Gruppen getrennt&quot;, ylab = &quot;Angstwerte&quot;, xlab = &quot;&quot;, ylim = c(0, 6), names = c(&quot;Kontrollgruppe&quot;, &quot;Pilates&quot;, &quot;Yoga&quot;)) 8.3.2 Verteilung zweier metrischer Variablen Für zwei metrische Variablen wird meistens die Darstellungsform des Streudiagramms gewählt, was mit dem Befehl plot(x,y,...) erstellt werden kann. x: Erste Variable () y: Zweite Variable ...: Weitere Graphikparameter (z.B. pch: Punkttyp \\(\\rightarrow\\) ?pch) plot(dataset$item1, dataset$item2, ylab = &quot;item1&quot;, xlab = &quot;item2&quot;, main = &quot;Streudiagramm von Item 1 und Item 2&quot;, xlim = c(0, 6), ylim = c(0, 6), pch = 18, col = &quot;firebrick4&quot;) Der Befehl abline() wird (separat) im Anschluss von plot() ausgeführt Mit abline(h = ..., v = ... , lwd = ..., lty = ...) können horizontale (h) bzw. vertikale (v) Linien an beliebiger Position eingefügt werden h/v: Wert durch welchen die Linie verlaufen soll lwd: legt die Stärke (Dicke) der Geraden fest (default = 1) lty: legt den Linientyp fest (s. ?par) plot(dataset$item1, dataset$item2, ylab = &quot;Item 2&quot;, xlab = &quot;Item 1&quot;, main = &quot;Streudiagramm von Item 1 und Item 2&quot;, xlim = c(0, 6), ylim = c(0, 6), pch = 18, col = &quot;firebrick4&quot;) abline(h = mean(dataset$item2), v = mean(dataset$item1), col = &quot;black&quot;, lty = 2) Mit abline(fit) wird die Regressionsgerade in das bestehende Streudiagramm eingezeichnet - fit entspricht dem definierten Modell, z.B. fit = lm(item2 ~ item1) model4 &lt;- item2 ~ item1 fit4 &lt;- lm(model4, dataset) plot(dataset$item1, dataset$item2, ylab = &quot;Item 2&quot;, xlab = &quot;Item 1&quot;, main = &quot;Streudiagramm von Item 1 und Item 2&quot;, sub = &quot;Item 2 = 3.5000 - 0.6667 * Item 1&quot;, xlim = c(0, 6), ylim = c(0, 6), pch = 18, col = &quot;firebrick4&quot;) abline(fit4, lwd = 2, col = &quot;darkslategrey&quot;, lty = 2) Achsen anpassen Es gibt die Möglichkeit die Achsen noch zusätzlich anzupassen. Dafür müssen aber zuerst die ursprünglichen Achsen gelöscht werden. Das funktioniert über das zusätzliche Argument axes = FALSE in der plot()-Funktion Mit dem Befehl axis(side, at, col) können im Nachhinein die Achsen angepasst werden. side: Kann entweder 1 für unten (x-Achse), 2 für links (y-Achse), 3 für oben, 4 für rechts at: ein Vektor mit dem die ticks auf den Achsen festgelegt werden können col: Die Farbe der Achsen plot(dataset$item1, dataset$item2, ylab = &quot;Item 2&quot;, xlab = &quot;Item 1&quot;, main = &quot;Streudiagramm von Item 1 und Item 2&quot;, xlim = c(0, 6), ylim = c(0, 6), pch = 18, col = &quot;firebrick4&quot;, axes = FALSE) axis(side = 1, at = c(0,2,4,6), col = &quot;blue&quot;) axis(side = 2, at = c(0,2,4,6), col = &quot;blue&quot;) 8.4 ggplot 8.4.1 Aufbau Die Logik des Aufbaus von Graphiken in ggplot lehnt sich an die Ideen von Leland Wilkinson (The Grammar of Graphics) an. Figure 8.1: Quelle: http://r.qcbs.ca/workshop03/book-en/grammar-of-graphics-gg-basics.html 8.4.2 Daten Zunächst übergeben wir den ersten “Layer” der Graphik: die Daten. Es passiert augenscheinlich noch nichts. Tatsächlich werden aber die Daten bereits hinterlegt. ggplot(data = df_regio) 8.4.3 Aesthetics Im nächsten Schritt werden über das sogenannte Mapping die Aesthetics festgelegt. Hierbei wird festgelegt, welches graphische Merkmal die Daten representieren soll (z.B. Achsen, Farbe, Größe, etc.) ggplot(data = df_regio, mapping = aes(x = Europawahl_Beteiligung_percent, y = Europawahl_AFD_percent)) 8.4.4 Geome Wie die Daten, in Form der Aesthetics, nun konkret dargestellt (besser: repräsentiert) werden (Linien, Striche, Punkte, etc.), wird durch sogenannte Geome festgelegt. Hierbei ist der Begriff der Layer wörtlich zunehmen. Geome können nämlich (beinahe) beliebig übereinandergelegt werden. Die Geome werden mit + an die Basislayer gebunden. Damit wird deutlich, dass es sich um zusätzliche Layer handelt. Außerdem wird klar, dass die Reihenfolge der Geome relevant ist, da später hinzugefügte Geome die vorher hinzugefügten überdecken können. ggplot(data = df_regio, mapping = aes(x = Europawahl_Beteiligung_percent, y = Europawahl_AFD_percent)) + geom_point() + geom_rug() 8.4.5 Geome - mapping Ein kleiner Clou: manchmal sollen die Aesthetics (z.B. die Farbe) nur für ein bestimmtes Geom angepasst werden. Wir können die Eigenschaften der Daten auch auf ein spezifisches Geom mappen, ohne das global zu tun. Beispiel: Die Farbe der Punkte soll sich nach alten und neuen Bundesländern unterscheiden, aber nur für die Punkte, nicht für die Randverteilung. ggplot(data = df_regio, mapping = aes(x = Europawahl_Beteiligung_percent, y = Europawahl_AFD_percent)) + geom_point(mapping = aes(color = region)) + geom_rug() 8.4.6 weitere Argumente Gleichzeitig können auch statische ästetische Eigenschaften festgelegt werden, die nicht an Daten geknüpft sind. Die Randverteilung im Plot soll grün (color) dargestellt werden und die Punkte sollen durchsichtiger (alpha) und größer (size) werden. ggplot(data = df_regio, mapping = aes(x = Europawahl_Beteiligung_percent, y = Europawahl_AFD_percent)) + geom_point(mapping = aes(color = region), alpha = 0.6, size = 3) + geom_rug(color = &quot;darkgreen&quot;) 8.4.7 Geome - Grundfunktionen Es existieren eine Vielzahl von Geomen. Hier eine kleine Übersicht über die wichtigsten: Geom Geometrische Objekt geom_points() Datenpunkte (Scatter-Plot) geom_jitter() Datenpunkte (Jitter-Plot) geom_bar() Balken geom_histogram() Histogramm geom_boxplot() Boxplot geom_smooth() Fit eines spezifizierten Zusammenhangs geom_rug() Marginale Verteilungen Für weitere Geome siehe . 8.5 Ergänzungen 8.5.1 ggplot 8.5.1.1 Globale Einstellungen Es lassen sich mit der Logik der Geome auch sämtliche globalen Aesthetics bearbeiten. Der einfachste Weg führt über das Geom labs(). Hier können Titel (title = \"\"), Untertitel (subtitle = \"\"), die Legendenüberschrift (in diesem Fall: color = \"\") oder die Achsenbeschriftungen (x = \"\", y = \"\") veränderbar. Das allgemeine Erscheinungsbild lässt sich mit sogenannten Themes verändern. Das Package ggtheme stellt dabei noch deutlich mehr Themes zur Verfügung. Hinweis: Weitere Eigenschaften lassen sich über theme() beherrschen. Diese Vorgehensweise ist aber deutlich komplizierter und erfordert ein wenig Erfahrung im Umgang mit diversen Online-Suchmaschinen ;-). library(ggthemes) ggplot(data = df_regio, mapping = aes(x = Europawahl_Beteiligung_percent, y = Europawahl_AFD_percent)) + geom_point(mapping = aes(color = Region)) + geom_rug() + labs(title = &quot;Wahlerfolg der AfD in Abhängigkeit der Wahlbeteiligung&quot;, subtitle = &quot;Europawahl 2019, Alle Angaben in %&quot;, x = &quot;Wahlbeteiligung&quot;, y = &quot;Wählerstimmen der AfD&quot;, color = &quot;Bundesländer&quot;) + theme_economist() 8.5.1.2 Plots als Objekte Plots sind in R nichts anderes als umfangreiche Listen vom Typ “ggplot”. ggplot(data = df_regio) %&gt;% class() #&gt; [1] &quot;gg&quot; &quot;ggplot&quot; Daher können wir auch Plots an Namen binden und im Nachhinein entsprechend aufrufen: base_plot &lt;- ggplot(data = df_regio, mapping = aes(x = Europawahl_Beteiligung_percent, y = Europawahl_AFD_percent)) Sollen viele Plots nacheinander “durchprobiert” werden bietet sich diese Vorgehensweise entsprechend an: base_plot &lt;- ggplot(data = df_regio, mapping = aes(x = Europawahl_Beteiligung_percent, y = Europawahl_AFD_percent ) ) base_plot + geom_point() base_plot + geom_smooth(method = &quot;lm&quot;) 8.5.1.3 Facets Mithilfe von Facets lassen sich mehrere Plots in einer Graphic erstellen. Hierfür nutzen wir das Geom facet_grid(). Die Daten werden hierbei nach den hier übergebenen Variablen gruppiert und jeweils ein Plot in der Graphik erstellt. Dabei können die Facets in Spalten (cols) oder Zeilen (rows) angeordnet werden. Deutlicher wird es an einem Beispiel. Hinweis: Die an facet_grid() übergebenen Variablen müssen erst als Variable expliziert werden. Dies erfolgt mit vars(). ggplot(data = df_regio, mapping = aes(x = Europawahl_Beteiligung_percent, y = Europawahl_AFD_percent)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + facet_grid(cols = vars(Region)) 8.5.1.4 Speichern von Graphiken mit GGSave Das Speichern ist mit GGPlot komfortabel. Hierzu wurde die Funktion ggsave() entwickelt. Vor dem Speichern muss die Graphik als Objekt an einen Namen gebunden werden. Im Anschluss können wir die Graphik an die Funktion übergeben. Die Endung des Files gibt dabei das Datenformat vor, indem diese gespeichert werden soll. Außerdem können wir in der Funktion weitere Parameter, wie die gewünschte Größe des Bildes, festlegen. plot_save &lt;- ggplot(data = df_regio, mapping = aes(x = Europawahl_Beteiligung_percent, y = Europawahl_AFD_percent)) + geom_point() ggsave(plot = plot_save, filename = &quot;plots/plot.pdf&quot;, height = 7, width = 8, units = &quot;in&quot;) 8.5.1.5 Erweiterungen GGPlot wurde über die Zeit noch deutlich erweitert. Eine kleine Übersicht findet ihr . Wir wollen uns noch kurz mit der Erweiterung GGForce und Viridis beschäftigen. findet ihr ausführliche Informationen zu GGForce. Vorbereitung: library(gapminder) library(ggforce) library(viridis) df_countries &lt;- gapminder 8.5.1.5.1 GGForce gapminder %&gt;% filter(country %in% c(&quot;Burundi&quot;,&quot;Germany&quot;)) %&gt;% ggplot(aes(x = lifeExp, y = gdpPercap, color = country)) + geom_point(alpha = 0.8, shape = 16, size = 1.7) Problem: Werte für Burundi “kleben” am Boden der Graphik und sind so praktisch nicht interpretierbar. gapminder %&gt;% filter(country %in% c(&quot;Burundi&quot;,&quot;Germany&quot;)) %&gt;% ggplot(aes(x = lifeExp, y = gdpPercap, color = country)) + geom_point(alpha = 0.8, shape = 16, size = 1.7) + facet_zoom(xy = country == &quot;Burundi&quot;) 8.5.1.5.2 Viridis Für viele nicht sichtbar: die Standardfarben von GGPlot sind nicht geeignet für Personen mit Farbblidheit. Hier hilft das Package viridis. gapminder %&gt;% filter(year == 2007) %&gt;% ggplot(aes(x = gdpPercap, y = lifeExp, size = pop,color = continent)) + geom_point(alpha = 0.8) + scale_color_viridis_d() Letzter Hinweis Das Erstellen und Anpassen von Graphiken kann durchaus noch viel komplexer werden. In dieser Sitzung gab es daher nur einen kleinen Einblick in die allgemeine Funktionsweise von GGPlot. Im Zweifel sollte einfach solange gegoogelt und rumprobiert werden, bis das gewünschte Ergebnis erzielt wurde. Inspiration für unterschiedliche Typen von Graphiken liefert die . 8.5.2 Tabellen 8.5.2.1 Mehrfelder Tabelle kontroll pilates yoga m 14 12 6 w 31 34 23 8.5.2.2 Korrelations Matrizen** item1 item2 item3 item1 1.0000000 -0.7698004 0.6123724 item2 -0.7698004 1.0000000 -0.4714045 item3 0.6123724 -0.4714045 1.0000000 8.5.2.3 Formatierung Das Package stargazer bietet den Befehl stargazer(...,type = \"html\", title = \"title\", out = \"...\",) ...: einen Vektor, Matrix, Datensatz oder Modell type: Welches Dateiformat soll der Befehl ausgeben. Beispielsweise \"html\", \"latex\" oder \"text\" title: Überschrift der Tabelle out: Name der Datei. Abhängig von type entweder mit .html, .tex oder .txt am Ende. Wenn davor kein Dateipfad angegeben wird, wird die Datei im Projektordner gespeichert. …noch viele weitere Möglichkeiten für Anpassungen. Siehe ?stargazer() 8.5.2.4 Anwendung an Beispielen Korrelationstabelle library(psych) dataset &lt;- read.table(&quot;data/Daten.txt&quot;, header = TRUE) Korrelationstabelle&lt;- cor(dataset[,2:4], method = &quot;pearson&quot;) Korrelationstabelle item1 item2 item3 item1 1.0000000 -0.7698004 0.6123724 item2 -0.7698004 1.0000000 -0.4714045 item3 0.6123724 -0.4714045 1.0000000 library(stargazer) stargazer(Korrelationstabelle, type = &quot;html&quot;, title = &quot;Korellationstabelle&quot;, out = &quot;Korellationstabelle.html&quot;) Aussehen in der html Datei: Für Übertragung in Word einfach in der html Dateien markieren, kopieren und in Word einfügen Regressionsmodelle fit1 &lt;- lm(data = dataset, formula = item1 ~ item2 + item3) summary(fit1) Call: lm(formula = item1 ~ item2 + item3, data = dataset) Residuals: Min 1Q Median 3Q Max -0.75000 -0.33036 -0.08929 0.33036 0.64286 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 2.8929 1.1752 2.462 0.0434 * item2 -0.7143 0.2832 -2.523 0.0397 * item3 0.3929 0.3003 1.308 0.2322 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.5297 on 7 degrees of freedom Multiple R-squared: 0.6726, Adjusted R-squared: 0.5791 F-statistic: 7.191 on 2 and 7 DF, p-value: 0.02008 library(stargazer) stargazer(fit1, type = &quot;html&quot;, title = &quot;Beispiel Regression&quot;, out = &quot;Regression.html&quot;) library(stargazer) stargazer(fit1, type = &quot;html&quot;, title = &quot;Beispiel Regression&quot;, out = &quot;Regression.html&quot;, ci = TRUE, covariate.labels = c(&quot;sozioökonomischer status&quot;, &quot;Alter&quot;,&quot;Konstante&quot;), dep.var.labels = &quot;Einkommen&quot;, dep.var.caption = &quot;Abhängige Variable&quot;) ci: wenn TRUE ersetzt die angegebenen Standardfehler mit dem Konfifenzintervall für das Konfidenzniveau von 95 Prozent covariate.labels: ein Character-Vektor, der die Namen der unabhängigen Variablen ersetzt dep.var.labels: Name der abhängigen Variable dep.var.caption: Überschrift über die Abhängige Variable Resultat "],["weiterführende-inhalte.html", "9 Weiterführende Inhalte 9.1 R Markdown", " 9 Weiterführende Inhalte 9.1 R Markdown In Markdown können wir alles was wir bisher gelernt haben und noch viel mehr (z.B. auch andere Programmiersprachen) in einem Textdokument, einer Präsentation oder Sonstigem ausgeben lassen. Insbesondere wenn wir Code, Tabellen und Grafiken in unserer Arbeit darstellen möchten, bietet es sich an R Markdown zu verwenden, da hier die Formatierung um ein Vielfaches einfacher ist und besser aussieht, als in herkömmlichen Texteditoren, wie z.B. Microsoft-Word oder LibreOffice. Hier kommt ihr zur RStudio-Seite von Markdown: (link)https://rmarkdown.rstudio.com/ Cheatsheet-PDF: (link)https://github.com/rstudio/cheatsheets/raw/main/rmarkdown-2.0.pdf 9.1.1 Getting Started Um R Markdown in eurer RStudio Umgebung freizuschalten müsst ihr es, falls noch nicht geschehen, zunächst installieren und laden: ## R Markdown installieren install.packages(&quot;rmarkdown&quot;, dependencies = TRUE) ## R Markdown laden library(rmarkdown) "],["statistik-grundlagen.html", "10 Statistik Grundlagen 10.1 Skalenniveaus 10.2 Häufigkeiten und Verteilungen 10.3 Deskriptive Statistik 10.4 Induktive Statistik 8 10.5 Hypothesentests 10.6 lineare Regression", " 10 Statistik Grundlagen Unser R-Intensivkurs behandelt einige Themenbereiche der Statistik. Damit ihr sie euch nochmal ins Gedächtnis rufen könnt, gibt es hier eine Übersicht für euch. 10.1 Skalenniveaus Es gibt verschiedene Skalenniveaus, die uns Auskunft darüber geben welchen Informationsgehalt unsere Variablen haben. Anhand der Skalenniveaus können wir entscheiden welche statistischen Operationen mit einer Variable möglich sind.2 Skala Subskala Eigenschaften Beispiele Nominal - Klassifikation, Kategorie Geschlecht, Automarke, Stadt Ordinal - Rangordnung Schulnote, Bildungsstand Metrisch Intervall Rangordnung und Abstände Temperatur in Celsisus, IQ Metrisch Ratio Rangordnung, Abstände, und natürlicher Nullpunkt Gehalt, Gewicht Metrisch Absolut Rangordnung, Abstände, natürlicher Nullpunkt, und natürliche Einheit Anzahl Fachsemester 10.2 Häufigkeiten und Verteilungen Absolute und Relative Häufigkeiten Die absolute Häufigkeit ist die Anzahl (= ganze Zahl) wie oft ein Merkmal in einer Stichprobe vorkommt. Die relative Häufigkeit hingegen ist der Anteil der Fälle, in denen ein Merkmal in einer Stichprobe auftritt. Verteilungen Verteilungen betrachten wir in der deskriptiven Statistik in Form von Häufigkeitsverteilungen. Die Schiefe und die Wölbung geben uns Auskunft darüber inwiefern die Verteilung von Variablen von der Normalverteilung abweichen. 3 Schiefe Die Schiefe (skewness) trifft Aussagen zur Verteilungskurve einer Variable. Interpretation der Schiefewerte: Wert Bedeutung Interpretation \\(&lt;\\) 0 linksschief Die Verteilung ist nach links gestreckt, und der Großteil der Ausprägungen konzentriert sich auf der rechten Seite \\(&gt;\\) 0 rechtsschief Die Verteilung ist nach rechts gestreckt, und der Großteil der Ausprägungen konzentriert sich auf der linken Seite 0 symmetrisch Die Daten sind symmetrisch und gleichmäßig um den Mittelwert verteilt Wölbung Die Wölbung (kurtosis) misst wie spitz bzw. flach die Verteilung einer Variable ist. Interpretation der Kurtosis-Werte: Wert Bedeutung Interpretation \\(&lt;\\) 3 flachgifplig Die Verteilung ist gestaucht und wirken deshalb flacher \\(&gt;\\) 3 spitzgipflig Die Verteilung hat eine ausgeprägerte Spitze im Vergleich zu einer Normalverteilung 3 normalgipflig Die Verteilung folgt einer Normalverteilung 10.3 Deskriptive Statistik Die deskriptive Statistik ist ein grundlegendes Werkzeug für die empirische Forschung. Wenn wir mit Datensätzen arbeiten, kann es schwierig sein, Muster, Trends oder wichtige Merkmale nur anhand der Rohdaten zu erkennen. Deskriptive Statistiken helfen uns die Daten auf leicht verständliche Weise zu bündeln und zu verstehen.4 Lageparameter Lageparameter geben die zentrale Tendenz einer Variable an. Beispiele sind arithmetische Mittel, Median oder Modus.5 Lageparameter Beschreibung Skalenniveau Arithmetisches Mittel Summe der Werte einer Variable, geteilt durch die Anzahl an Werten Metrisch Median Wert der genau in der Mitte liegt, wenn die Werte einer Variable aufsteigend sortiert sind Metrisch, Ordinal Quartile 1.Quartile = Wert unter den 25% der Daten Fallen; 2. Quartile = Median; 3.Quartile = Wert unter den 75% der Daten fallen Metrisch, Ordinal Modus Wert(e) einer Variable mit häufigstem Auftreten Metrisch, Ordinal, Nominal Streuungsmaße Streuungsmaße geben an wie sehr unsere Daten streuen. Die Streuung kann dabei auf ein Lageparameter bezogen sein, z.B. bei der Varianz oder Standardabweichung, oder auch auf die Breite der Streuung aller Daten, z.B Spannweite oder Interquartilabstand.6 Streungsmaße Beschreibung Lageparameter Maximum Maximum ist der größte Wert einer Variable - Minimum Minimum ist der kleinste Werte einer Variable - Spannweit Die Spannweite ist die Differenz zwischen dem Maximum und dem Minimum - Varianz Quadrierte Abweichung vom arithmetischen Mittel (schwer interpretierbar) arithmetisches Mittel Standardabweichung Wurzel aus der Varianz (leichter interpretierbar) arithmetisches Mittel Interquantilabstand Differenz aus 3. und 1. Quartile Median Zusammenhangsmaße Mit Zusammenhangsmaßen können wir untersuchen wie stark zwei Variablen sich gegenseitig beieinflussen (zusammenhängen). Je nach Skalenniveau der betrachteten Variablen wählen wir unterschiedliche Verfahren.7 Skalennivau (Dichotom) Nominal Ordinal Metrisch (Dichotom) Phi-Koeffizient Cramers V, Kontingenzkoeffizienten Biserialer Rangkorrelationskoeffizient Punktbiseriale Korrel. tetrachorischr o. biseriale Korrelation Nominal Cramers V, Kontingenzkoeffizienten Cramers V, Kontingenzkoeffizienten – Eta Ordinal Kontingenzkoeffizient, Cramers V Kontingenzkoeffizient, Cramers V Spearman-Rangkorrelation, Kendalls Tau, Gamma – Metrisch Kontingenzkoeffizient, Cramers V Kontingenzkoeffizient, Cramers V Spearman-Rangkorrelation, Kendalls Tau, Gamma Korrelationskoeffizient n. Pearson, Produkt-Moment-Korrelation 10.4 Induktive Statistik 8 10.4.1 Grundlagen der induktiven Statistik Induktive Statistik, wofür? Das Problem in der Statistik ist oft, dass Aussagen über Grundgesamtheiten, z.B (Gesamte Bevölkerung eines Landes, Alle Mitglieder einer Firma) getroffen werden sollen. Allerdings ist das aus Kosten- und Aufwandgründen nicht realisierbar. Daher werden Stichproben aus der Gesamtheit gezogen. Die Frage ist jetzt, ob sich die Merkmale die in der Stichprobe erhoben auch auf die Grundgesamtheit übertragen lassen. Daher auch der Name “induktiv”, da man hier vom Speziellen auf das Allgemeine schließt anders als bei der Deduktion bei der vom Allgemeinen auf das Spezielle geschlossen wird. Das Problem ist, dass Aussagen, welche aus induktiven Schlüsse herrühren, nicht immer zu 100 Prozent mit der Wirklichkeit übereinstimmen. Es gibt immer eine Restunsicherheit. Und um diese zu Beschreiben gibt es die Induktive Statistik Wichtige Begriffe Grundgesamtheit: Die zu untersuchende Zielgruppe, z.B. alle Studenten der Uni Leipzig Stichprobe: Eine meist zufällig gezogene Teilmenge aus der Grundgesamtheit z.B. alle Erstsemestler statistischer Kennwert: Wert der ein Merkmal in irgendeiner Form zusammenfasst Beispiele: Mittelwert Varianz Standardabweichung (Siehe dazu Teil deskriptive Statistik) 10.5 Hypothesentests Um mit unseren Daten allgemein gültige Aussagen über die Grungesamtheit treffen zu können brauchen wir Hypothesentests. Solche Hypothesentests können verschiedene Forschungsfragen zugrunde liegen, z.B. Unterscheiden sich Populationsmittel eines Merkmals zwischen zwei Populatione? Sind die Ausprägungen eines Merkmals auf eine bestimmte Weise verteilt? Sind zwei Merkmale voneinander unabhängig? Die allgemein am meisten genutze Form des statistischen Hypothesentests ist der Signifikanztest. Diese sind immer gleich aufgebaut. Zuerst gibt es die Alternativhypothese \\(H_a\\). Das ist diejenige, von der wir hoffen, sie mit den Daten nachweisen zu können. Ihr gegenüber steht die Null-Hypothese \\(H_0\\), von der wir hoffen, dass sie sich anhand der Daten als statistisch unplausibel herausstellt und wir sie ablehnen können. Die zu untersuchende Hypothese bestimmt dabei die Auswahl des statistischen Testverfahrens. Nachfolgend werden anhand von Beispielen verschiedene Testverfahren erläutert. 10.5.1 t-Tests T-Tests sind die am häufigsten verwendeten statistischen Tests, wenn es darum geht mit Stichproben zu arbeiten. Sie bieten sich gut an um metrische Variablen zu vergleichen. 10.5.1.1 Einstichproben t-Test Beispiel: In einer Umfrage wurden 200 zufällig ausgewählte Studenten aus der UNI Leipzig zu ihrer persönlichen Zufriedenheit befragt und aus den gegebenen Antworten wurden pro Person (zeilenweise) ein Summenscore gebildet. Der Mittelwert des Summenscores der Stichprobe beträgt \\(\\bar x = 208\\) mit einer Standardabweichung von \\(s = 30\\). Es ist bekannt, dass der durchschnittliche Wert für die Gesammtpopulation aller Studenten bei \\(\\mu = 150\\) liegt, die Frage ist jetzt, weicht unserer Stichprobenmittel signifikant vom Populationsmittel ab? Dazu formulieren wir folgende Hypothese: Studenten der Uni Leipzig weichen signifikant von Studenten anderer Universitäten in ihren Zufriedenheitswerten ab. Daraus resultieren zwei statistische Hypothesen, welche die zwei Ausgänge eines Hypothesentests sind. Zum einen die Nullhypothese: \\(H_0: \\mu = \\mu_0\\) Übersetzt bedeutet das, wir vermuten nach dieser Hypothese, dass unser Stichprobenmittelwert \\(\\mu (= \\bar x)\\) mit dem Mittelwert der Population \\(\\mu_0\\) übereinstimmt. Das klingt jetzt erstmal verwirrend, da das ja unserer Forschungsfrage entgegengesetzt ist. Aber genau das ist, was die Nullhypothese machen soll, da sie ja am Ende verworfen werden soll. Die Alternativhypothese dazu ist entsprechend: \\(H_a: \\mu \\neq \\mu_0\\) Für jedes ausgewählte Testverfahren müssen bestimmte Voraussetzungen geprüft werden. Bevor ein Einstichproben t-Test durchgeführt werden kann, muss die abhängige Variable also die Zufriedenheitswerte auf Normalverteilung geprüft werden. Dies kann durch einen Shapiro-Test geklärt werden. Dafür wird der Vektor mit den Zufriedenheitswerten der Studenten verwendet. Die statistischen Hypothesen für diesen Test lauten: \\(H_0:\\) Daten sind normalverteilt \\(H_a:\\) Daten sind nicht normalverteilt Hier ist also das Ziel ein nicht signifikantes Testergebnis zu erhalten set.seed(6) stichprobe &lt;- rnorm(200, 208, 30) #erstellung einer zufälligen Verteilung von 200 Werten mit dem Mittelwert 208 und einer standardabweichung von 30 shapiro.test(stichprobe) Shapiro-Wilk normality test data: stichprobe W = 0.99478, p-value = 0.7174 Hier gilt es auf den berechneten p-Wert zu achten. Ist dieser über 0.05 können wir annehmen, dass die zugrunde liegenden zufriedenheitswerte normalverteilt sind. Heißt also der t-Test kann angewendet werden. Dafür gibt es in R den Befehl \\(\\texttt{t.test()}\\) t.test(stichprobe, mu = 150) #mit dem mu-Argument wird der wahre Wert des Mittelwerts festgelegt One Sample t-test data: stichprobe t = 27.333, df = 199, p-value &lt; 2.2e-16 alternative hypothesis: true mean is not equal to 150 95 percent confidence interval: 202.4357 210.5902 sample estimates: mean of x 206.513 auch hier ist wieder der p-Wert wichtig. Liegt dieser unter 0.05 können wir zu einem Signifikanzniveau von 5 % feststellen, dass der Mittelwert der Stichprobe tatsächlich vom wahren Mittelwert 150 abweicht. Da der p-Wert hier auch noch deutlich geringer ausfällt (\\(2.2e-16 = 2.2 * 10^-16\\)), können wir dies sogar für ein noch niedrigeres Signifikanzniveau feststellen. Für die p Werte gilt im Allgemeinen Symbol p-Wert Bedeutung * &lt; 0.05 signifikant ** &lt; 0.01 sehr signifikant *** &lt; 0.001 hoch signifikant 10.5.1.2 Zweistichproben t-Test Beispiel Zusätzlich zu der Stichprobe an der Uni Leipzig, wurde noch eine weitere Erhebung mit 200 Personen an der Uni Halle durchgeführt. Der Durchschnitt lag hier bei 180 und die Standardabweichung bei 55. Es soll jetzt festgestellt werden, ob die´zwei Stichproben sich signifikant voneinander unterscheiden. Dazu wird ein Zweistichproben t-Test verwendet. Dazu müssen allerdings auch die Voraussetzungen erfüllt sein, dass beide Stichproben normalverteilt sind… shapiro.test(stichprobe2) Shapiro-Wilk normality test data: stichprobe2 W = 0.99666, p-value = 0.9456 …sowie dass beide Stichproben die gleiche Varianz haben. Dies kann mithilfe des \\(\\texttt{var.test()}\\)-Befehls überprüft werden. var.test(stichprobe,stichprobe2) F test to compare two variances data: stichprobe and stichprobe2 F = 0.29927, num df = 199, denom df = 199, p-value &lt; 2.2e-16 alternative hypothesis: true ratio of variances is not equal to 1 95 percent confidence interval: 0.2264854 0.3954516 sample estimates: ratio of variances 0.2992725 Bei diesem Test ist die Nullhypothese, dass die Varianzen gleich sind. Also \\(H_0: \\sigma^2_1 = \\sigma^2_2\\) \\(H_a: \\sigma^2_1 \\neq \\sigma^2_2\\) Auch hier ist ein nicht signifikantes Ergebnis erwünscht. Da also die Bedingung nicht erfüllt ist, muss bei der Berechnung des t-Tests noch eine Einstellung geändert werden. Der Befehl bleibt aber gleich, nur, dass jetzt beide Stichproben in den Befehl eingefügt werden. t.test(stichprobe,stichprobe2, var.equal = FALSE)#var.equal auf FALSE zu setzen, sorgt dafür, dass eine abgewandelte Form des t-Tests angewandt wird, der die unterschiedlichen Varianzen einkalkuliert Welch Two Sample t-test data: stichprobe and stichprobe2 t = 7.3541, df = 308.32, p-value = 1.746e-12 alternative hypothesis: true difference in means is not equal to 0 95 percent confidence interval: 23.20512 40.15904 sample estimates: mean of x mean of y 206.5130 174.8309 Aufgrund des sehr niedrigen p-Werts zeigt sich hier, dass die zwei Stichproben sich hochsignifikant voneinander unterscheiden. Zusätzlich gibt es auch noch die Möglichkeit einen t-Test mit gepaarten Stichproben durchzuführen. Das wäre z.B. der Fall, wenn die Umfrage an der Uni Leipzig nochmal mit dem gleichen Personenkreis zu einem späteren Zeitpunkt durchgeführt würde. Dazu muss in der \\(\\texttt{t.test()}\\)-Funktion das Argument \\(\\texttt{paired}\\) auf \\(\\texttt{TRUE}\\) gesetzt werden. 10.5.2 Chi-Quadrat Test Wenn es darum geht ob eine kategoriale Variable einen Einfluss auf eine andere kategoriale Variable hat, bietet es sich an, das ganze mit einem Chi-Quadrat Test zu überprüfen. Dieser Test kann für folgende Fragestellungen verwendet werden: (1) folgen die Häufigkeiten einer bestimmten Variablen einer bekannten Verteilung? (2) beeinflussen sich zwei kategoriale Variablen gegenseitig? Beispiel: In einer Umfrage wurden Personen befragt,ob sie sich in ihrem leben glücklich fühlen und zum zweiten, ob sie eine feste Freundesgruppe haben. Es wird vermutet, dass es einen Zusammenhang zwischen diesen beiden Variablen gibt. Die Ergebnisse sehen so aus:   glücklich nicht glücklich   keine feste Freundesgruppe 13 % 4.5 % 17 % feste Freundesgruppe 61.5 % 21 % 82.5 %   74.5 % 25.5 % 100 % mit diesen Datens kann dann die Hypothese mithilfe des \\(\\texttt{chisq.test()}\\) überprüft werden chisq.test(stichprobe3$glck,stichprobe3$frnd) #&gt; #&gt; Pearson&#39;s Chi-squared test with Yates&#39; continuity #&gt; correction #&gt; #&gt; data: stichprobe3$glck and stichprobe3$frnd #&gt; X-squared = 0.070178, df = 1, p-value = 0.7911 Die Nullhypothese ist bei diesem Test, dass die zwei Variablen unabhängig sind bzw. dass sie sich nicht gegenseitig beeinflussen. Die Alternavhypothese besagt das Gegenteil, also dass sie sich gegenseitig beeinflussen. Da der p-Wert hier deutlich über 0,05 liegt können wir die Nullhypothese nicht ablehnen und müssen davon ausgehen, dass in diesem Fall das Vorhandensein von Freunden keinen Einfluss auf die Glücklichkeit hat. 10.6 lineare Regression Hinweis: Für lineare Regressionen sind eine vielzahl an Voraussetzungen zu überprüfen: (1)Es existiert ein linearer Zusammenhang (2)Erwartungswert der Residuen ist Null (3)Varianzhomogenität der Residuen (Homoskedastizität) (4)Die Residuen sind nicht miteinander Korreliert (Autoregression) (5)Die Residuen sind nicht mit den unabhängigen Variablen korreliert und die unabhängige Variable ist deterministisch(z.B. Omitted-Variable-Bias) (6)Normalverteilung der Residuen 10.6.1 einfache lineare Regression Mithilfe der linearen Regression ist es möglich, einen gerichteten Einfluss einer Variablen X auf eine Variable Y nachzuweisen. Es wird dabei ein linearer Zusammenhang angenommen. Die Grundlegende Form dieses Zusammenhanges ist wie folgt: \\(Y=\\alpha+\\beta \\cdot X +\\varepsilon\\) Die Koeffizienten beschreiben dabei: \\(\\alpha\\) ist der y-Achsenabschnitt, der uns sagt welchen Wert Y hätte, wenn X null wäre \\(\\beta\\) ist der Anstieg der linearen Regression. Wenn also der X-Wert um eins ansteigt, steigt die Y-Variable um den Wert von \\(\\beta\\) \\(\\varepsilon\\) ist der Fehlerterm. Da die wenigsten Zusammenhänge im echten Leben perfekt linear sind, zeigen sich in dem Fehlerterm zufällige Einflüsse. Am besten lässt sich ein solcher Zusammenhang vorstellen wenn man das ganze visuell betrachtet Was der Algorithmus hinter der lineare Regression macht, ist eine lineare Funktion für eine Gerade zu finden, die einen möglichst geringen Abstand zu den einzelnen Punkten hat. Etwa so: Eine lineare Regression wird in R mit dem Befehl \\(\\texttt{lm()}\\) erstellt wobei das erste Argument eine \\(\\texttt{formula}\\) mit der Struktur \\(\\texttt{AbhängigeVariable} \\sim \\texttt{UnabhängigeVariable}\\) das erste Argument ist. Hier wird jetzt für den Allbus2018 (allgemeine Bevölkerungsumfrage der Sozialwissenschaften) untersucht, ob das Alter (in Jahre) einen Einfluss auf die allgemeine Zufriedenheit (10 stufige Skala) hat. Allbus &lt;- readRDS(&quot;data/Allbus2018.rds&quot;) output &lt;- lm(Zufriedenheit~Alter,data = Allbus) summary(output) #&gt; #&gt; Call: #&gt; lm(formula = Zufriedenheit ~ Alter, data = Allbus) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -7.8923 -0.8804 0.1164 1.1124 2.1403 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 7.9218361 0.1471104 53.850 &lt;2e-16 *** #&gt; Alter -0.0007972 0.0031956 -0.249 0.803 #&gt; --- #&gt; Signif. codes: #&gt; 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 1.532 on 1574 degrees of freedom #&gt; Multiple R-squared: 3.954e-05, Adjusted R-squared: -0.0005958 #&gt; F-statistic: 0.06224 on 1 and 1574 DF, p-value: 0.803 Dem Output können drei wichtige Ergebnisse entnommen werden: (1) Der F-Test besagt, ob das gesammte Modell signifikant ist (wenn p-Wert &gt;0.05, dann nicht signifikannt) (2) Bestimmtheitsmaß \\(R^2\\) beschreibt den Anteil der durch das Gesamtmodell aufgeklärten Varianz (liegt zwischen 0 und 1, wobei 0 bedeutet, dass das Modell keine Erklärungskraft hat und 1, dass das Modell die abhängige Variable perfekt erklärt); wird für die Interpretation in Prozent angegeben. (3) Koeffizienten (\\(\\texttt{Estimates}\\)) beschreiben den Einfluss der unabhängigen Variable (Alter) auf die abhängige Variable (Zufriedenheitswert). Interpretation Der p-Wert der F-Statistik liegt hier mit 0.803 deutlich über dem Signifikanzniveau von 0.05, was bedeutet, dass das gesammte Modell nicht Statistisch signifikant ist. Dementsprechend klein ist auch das \\(R^2\\), welches mit $ 3.954^{-5}$ sehr gering ausfällt. Für die Interpretion der Koeffizienten gilt es zuerst auf die Werte unter \\(\\texttt{Coefficients}\\) in der Spalte \\(\\texttt{Pr(&gt;|t|)}\\) zu schauen. Die Werte hier sind die p-Werte für die jeweiligen Koeffizienten. Um an diese Signifikanzniveaus zu kommen wird automatisch für jeden Koeffizienten ein t-test gerechnet, welcher prüft, ob die Koeffizienten signifikant von Null verschieden sind. Also, ob ausgeschlossen werden kann, dass der berechnente Koeffizient nicht nur zufällig einen Einfluss auf die abhängige Variable zeigt. Generell gilt, je niedriger diese p-Werte, desto besser. Um von einem Signifikanten Koeffizienten zu sprechen, muss der p-Wert zumindest einen Wert von 0.05 unterschreiten. In der Tabelle wird das zusätzlich durch Sterne hinter den p-Werten angezeigt. Die bedeutung ist hier: Erst wenn einer dieser Fälle eintritt, können die Koeffizienten sinvoll interpretiert werden. Für diesen Fall können wir sagen, dass der \\(\\alpha\\)-Koeffizient, der hier mit \\(\\texttt{(Intercept)}\\) beschrieben wird, hochsignifikant ist. Interpretiert wird der Wert, der in der \\(Estimate\\)-Spalte steht (7.9218361). Übersetzt heißt dieser Wert, das wir für eine Person mit dem Alter null Jahre im Durchschnitt einen Zufriedenswert von \\(\\approx\\) 7.9 haben. Die Interpretation des Intercepts ist aber meistens nicht sinvoll, da Personen, die damit beschrieben werden irrelevant sind. Interessanter sind die \\(\\beta\\)-Koeffizienten, die neben den jeweiligen Variablen stehen. Für den Koffizient für Alter haben wir hier zwar keine Signifikanz, was bedeutet, dass das Alter keinen Einfluss auf die Zufriedenheit hat. Zur Veranschaulichung würde der Wert hier so interpretiert werden, dass wenn das Alter um ein Jahr steigt (immer um eine Einheit) der Zufriedenheitswert im Durchschnitt um 0.0007972 Punkte sinkt (da negativ). 10.6.2 multiple lineare Regression Meistens ist es das Ziel nicht nur den Einfluss von einer unabhängigen Variablen auf eine abhängige Variable zu prüfen, sondern festzustellen, ob mehrere Variablen zusammen einen Einfluss auf die abhängige Variable haben. Dementsprechend sieht die neue Formel dann so aus: \\(Y=\\alpha+\\beta_1 \\cdot X_1 + \\beta_2 \\cdot X_2 +...+ \\beta_k \\cdot X_k +\\varepsilon\\) mit \\(k\\) ist hier gemeint, dass unendlich viele unabhänige Variablen hinzugezogen werden können. Die Vorgehensweise ist ähnlich zur einfachen linearen Regression, nur das hier jetzt zusätlich zum Alter auch noch Einkommen (in 100 EUR) und Geschlecht als erklärende Variablen einbezogen werden. Diese werden in der Formel mit einem \\(+\\) hinzugefügt. output &lt;- lm(Zufriedenheit~Alter+Einkommen+Geschlecht,data = Allbus) summary(output) #&gt; #&gt; Call: #&gt; lm(formula = Zufriedenheit ~ Alter + Einkommen + Geschlecht, #&gt; data = Allbus) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -7.7490 -0.7313 0.1577 1.0315 2.7668 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 7.522119 0.154954 48.544 &lt; 2e-16 #&gt; Alter -0.006036 0.003219 -1.875 0.0609 #&gt; Einkommen 0.022417 0.003086 7.264 5.90e-13 #&gt; Geschlechtweiblich 0.354772 0.081627 4.346 1.47e-05 #&gt; #&gt; (Intercept) *** #&gt; Alter . #&gt; Einkommen *** #&gt; Geschlechtweiblich *** #&gt; --- #&gt; Signif. codes: #&gt; 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 1.506 on 1572 degrees of freedom #&gt; Multiple R-squared: 0.03466, Adjusted R-squared: 0.03282 #&gt; F-statistic: 18.81 on 3 and 1572 DF, p-value: 5.463e-12 Zusätlich gilt es bei multiplen linearen Regressionen auch auf den Wert des \\(\\texttt{Adjusted R-squared}\\) zu achten, welches den \\(\\texttt{R-Squared}\\)-Wert korrigiert, um den Effekt von zusätzlichen unabhängigen Variablen auszugleichen. Man kann es zum Vergleich verschiedener Modelle nutzen, aber nur das \\(\\texttt{R-Squared}\\) kann interpretiert werden. Interpretation Der p-Wert der F-Statistik liegt für dieses Modell deutlich unter 0.05 und damit ist das gesammte Modell hochsignifikant. Der Wert von \\(\\texttt{R-Squared}\\) liegt bei 0.03466, was bedeutet, dass die unabhängigen Variablen insgesammt 3.5% Varianz in der abhängigen Variable erklären. Ob diese Varianzaufklärung einen hohen Wert darstellt muss im Vergleich mit der umliegenden Literatur geklärt werden. Für das Alter zeigt sich auch in diesem Modell keine Signifikanz. Das Einkommen hat dafür einen hochsignifikanten Einfluss auf die Zufriedenheit. Der Wert kann so interpretiert werden, dass wenn das Einkommen um 100 EUR steigt, der Zufriedenheitswert im Durchschnitt und unter Konstanthaltung aller anderer Variablen um \\(\\approx\\) 0.022 Punkte steigt. Bei multiplen Regressionen ist es wichtig, immer dazu zu sagen, dass die restlichen Variablen Konstant gehalten werden (auch genannt ceteris paribus), da dies Vorraussetzung für die Interpretation ist. Das Geschlecht ist ebenfalls hochsignifikant. Diese Variable ist aber ein Sonderfall, da es sich hier um eine kategoriale Variable handelt. Diese können nicht um einen gewissen Betrag steigen. Daher werden für die Interpretaion eine Ausprägung der Variable (hier: männlich) als Referenzkategorie genommen, auf die sich die restlichen Ausprägungen beziehen. Das erklärt auch warum in der Tabelle \\(\\texttt{Geschlechtweiblich}\\) steht. Es geht hier um den Vergleich von weiblich zur Referenzkategorie männlich. Konkret bedeutet das hier, dass eine weibliche Person im Durchschnitt und unter Konstanthaltung aller anderer Variablen einen 0.35 Punkte höheren Zufriedenheitswert hat als eine männliche Person. Benning, V. (2022, 02. Dezember). Zusammenhangsmaße verstehen und anwenden + Beispiele. Scribbr. Abgerufen am 27. Juni 2023, von https://www.scribbr.de/statistik/zusammenhangsmasse/↩︎ https://de.statista.com/statistik/lexikon/definition/139/verteilung/↩︎ Benning, V. (2023, 09. Januar). Deskriptive Statistik verstehen und anwenden. Scribbr. Abgerufen am 16. Juni 2023, von https://www.scribbr.de/statistik/deskriptive-statistik/↩︎ Benning, V. (2023, 23. Mai). Die Lageparameter erklärt mit Beispielen. Scribbr. Abgerufen am 14. Juni 2023, von https://www.scribbr.de/statistik/lageparameter/↩︎ Benning, V. (2022, 01. Juni). Die Streuungsmaße einfach erklärt mit Beispielen. Scribbr. Abgerufen am 14. Juni 2023, von https://www.scribbr.de/statistik/streuungsmasse/↩︎ Benning, V. (2022, 02. Dezember). Zusammenhangsmaße verstehen und anwenden + Beispiele. Scribbr. Abgerufen am 27. Juni 2023, von https://www.scribbr.de/statistik/zusammenhangsmasse/, https://wwwhomes.uni-bielefeld.de/fvan_veen/StatistikII%20SS06/Sonstiges/Interpretationshilfe.pdf↩︎ Teile des Kapitels sind den Vorlesungsmaterialien zu “Einführung in die Statistik” von Dr. Stephan Poppe entnommen↩︎ "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]

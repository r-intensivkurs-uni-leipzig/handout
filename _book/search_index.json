[["index.html", "Handout R-Intensivkurs 1 Willkommen", " Handout R-Intensivkurs Christoph Mayer 2024-03-19 1 Willkommen Schön, dass ihr Interesse an der Programmiersprache R habt! Hier seid ihr auf der Seite des kursbegleitenden Handouts für den R Intensivkurs. Teilnehmende des Kurses sollen hiermit die Möglichkeit bekommen, während bzw. nach dem Kurs Inhalte erneut einzusehen. Hinweis: Das Handout wird stetig überarbeitet bzw. erweitert. Über Anmerkungen und Verbesserungsvorschläge freuen wir uns :) Kontakt: Christoph Mayer: christoph.mayer@uni-leipzig.de Franz Andersch: franz.andersch@uni-leipzig.de Danksagung: Das Handout basiert in Teilen auf bereitgestellten Materialien von Ingo Seifert &amp; Isabel Gebhardt. Vielen Dank! "],["einführung-in-r-und-r-studio.html", "2 Einführung in R und R Studio 2.1 Was ist R? 2.2 Sechs Gründe R zu lernen 2.3 Benutzeroberfläche von R und RStudio 2.4 Empfohlenes Vorgehen", " 2 Einführung in R und R Studio Dieses Kapitel bietet einen Einführung in was R und RStudio eigentlich sind und wie deren Benutzeroberfläche aufgebaut ist. 2.1 Was ist R? R ist eine freie Programmiersprache Sprache: Vokabeln -&gt; Begriffe für Objekte, Eigenschaften, Handlungen etc. Grammatik -&gt; Regeln, wie Vokabeln in Sinnzusammenhang gebracht werden Programmiersprache funktioniert analog: Vokabeln -&gt; Funktionen, Objekte Grammatik -&gt; Regeln für die Anwendung von Funktionen auf Objekte Wie können Sprachen möglichst nachhaltig erlernt werden? Starte bei den Basics Basic Vokabeln Basic Grammatik -&gt; Ziel des Kurses ist es, die wesentliche Grammatik von R und die grundlegenden Vokabeln zu vermitteln. 2.2 Sechs Gründe R zu lernen Open Source und plattformübergreifend Praktisch unbegrenzter Funktionsumfang Entwickelt sich zum Standard in akademischer Welt und in der Wirtschaft Publizierfähige Graphiken Integration von Aufbereitung, Analyse und Präsentation Internationale Community 2.3 Benutzeroberfläche von R und RStudio 2.3.1 R GUI Haben wir dem Computer R durch Installation beigebracht, wird ein Graphical User Interface (GUI) mitinstalliert. Hier ist der volle Funktionsumfang bereits enthalten. Die GUI ist leider wenig übersichtlich und wenig “komfortabel”. 2.3.2 RStudio RStudio bietet eine nutzer:innenfreundliche (und freie) Alternative für die Nutzung von R. Enthält Script-Editor Übersicht über Objekte in globaler Umgebung Einfache Dateienhandhabung durch Projekte, u.v.m. 2.3.2.1 RStudio - Konsole Dient zur direkten Übergabe von Ausdrücken bzw. Funktionen an R Geeignet für kleine Tasks bzw. Tests Ergebnis wird direkt angezeigt Ergebnisse aus den Scripten, die keine Grafiken sind, werden hier angezeigt 2.3.2.2 RStudio - Script Editor Dient zur Eingabe von komplexen Funktionssequenzen (Scripts) Erlaubt das Speichern von Funktionssequenzen Der Editor öffnet sich nicht automatisch. Er muss händisch geöffnet werden über File -&gt; New File -&gt; R Script. 2.3.2.3 RStudio - Environment und Files Zeigt alle importierten Daten an Bietet Zugriff auf die History GitHub Zugriff, wenn angelegt Anzeige für File Browser Zeigt alle Packages, Plots etc. 2.3.2.4 Projekte in RStudio Vorteile: - Keine definition des working directory nötig - Erleichtert Weitergabe unserer Projekte - Hilft unsere Arbeit zu organisieren - Ein neues Projekt erstellt ihr indem ihr in der oberen linken Ecke File -&gt; New Project auswählt. 2.4 Empfohlenes Vorgehen In unseren Projekten arbeiten wir mit Skripten, dabei ist ein strukturiertes Vorgehen wichtig. Das bedeutet, das wir für eine Datenanalyse jeweils ein Skript erstellen. Dieses nachvollziehbar benennen und unseren Code mit Kommentaren versehen. 2.4.1 Empfohlener Ablauf einer Analyse Projekt und Skript(e) erstellen -&gt; siehe Modul 1 Installieren bzw. laden aller nötigen Pakete -&gt; siehe Modul 2 Daten importieren -&gt; siehe Modul 3 Daten organisieren -&gt; siehe Modul 3 und 4 Daten analysieren -&gt; siehe Modul 5 und 6 Ergebnisse präsentieren -&gt; siehe Modul 7 2.4.2 Beispiel Skript # Packages installieren und laden install.packages(&quot;tidyverse&quot;) library(tidyverse) # Daten laden df_salary &lt;- read.table(file = &quot;Daten/Salary.txt&quot;) # Daten organisieren ## Kategoriale Variable &quot;salary_cat&quot; erstellen (3 Kategorien) df_salary_final &lt;- mutate(df_salary, salary_cat = cut_number(salary, n = 3, labels = c(&quot;niedrig&quot;,&quot;mittel&quot;,&quot;hoch&quot;))) # Daten analysieren ## Absolute Häufigkeiten Berufe in Gehaltskategorien(salary_cat) table(df_salary_final$salary_cat, df_salary_final$profession) # Daten präsentiern ## Mosaikplot für Berufe in Gehaltskategorien(salary_cat) mosaicplot(table(df_salary_final$salary_cat, df_salary_final$profession), main = &quot;Gehaltskategorie nach Beruf&quot;) ... "],["grundlagen.html", "3 Grundlagen 3.1 Fundamentale Datentypen in R 3.2 R als Taschenrechner 3.3 Variablen und Objekte 3.4 Funktionen 3.5 Datenstrukturen 3.6 Packages", " 3 Grundlagen 3.1 Fundamentale Datentypen in R In R existieren verschiedene atomare Datentypen aus welchen sich wiederum komplexere Datentypen konstruieren lassen. Datentyp Beschreibung Beispiel integer ganze Zahlen -2 numeric reelle Zahlen 5.2456 logic logische Werte TRUE character Zeichenfolge “Mexico” NA,NULL,NaN Unbestimmt, Leer, Undefiniert Inf Unendlich Textelemente müssen immer in Anführungszeichen stehen, also entweder “…” oder ‘…’ 3.2 R als Taschenrechner 3.2.1 Dezimalstellen und Leerzeichen Dezimalzahlen werden in R mit einem Punkt und nicht mit einem Komma angegeben 2,5 Error: &lt;text&gt;:1:2: unexpected &#39;,&#39; 1: 2, ^ 2.5 [1] 2.5 Die Anzahl von Leerzeichen zwischen Befehlsbestandteilen ist (mit Ausnahmen) egal 3.2.2 Mathematische Operatoren R- Befehl Bedeutung + Addition - Subtraktion * Multiplikation / Division ^ Potenz sqrt(x) Wurzel log(x, base) Logarithmus Wichtig: Klammersetzung und Punkt vor Strich 3.2.2.1 Beispiele 3+2*8 [1] 19 (2*10)/(2*2.5) [1] 4 sqrt(2)^2 [1] 2 log(100000, base=10) [1] 5 3.2.3 Logische Operatoren R Befehl Bedeutung == Gleich &gt; Größer &lt; Kleiner &gt;= Größer gleich &lt;= Kleiner gleich != Nicht-gleich &amp; logisches UND | logisches ODER 3.2.3.1 Beispiele 4&lt;5 [1] TRUE 23!=24 [1] TRUE 10&lt;=4 | 12==6 [1] FALSE !(&quot;a&quot;&lt;&quot;b&quot; &amp; TRUE==TRUE) [1] FALSE 3.2.4 Exkurs: Dokumentation Alles in einer Zeile nach # gilt als Kommentar und wird nicht in der Konsole ausgeführt Dabei gilt # = Überschrift, ## Unter-überschrift, ### Unter-unter-Überschrift usw. Überschrift \\(----\\) ermöglicht eine Dropdownfunktion 3.3 Variablen und Objekte 3.3.1 Abspeichern von Variablen Werte können in Variablen (Objekten) gespeichert werden x &lt;- 5+3 Objekte sind nach der Zuweisung mit ihrem Namen abrufbar x &lt;- 5+3 x [1] 8 3.3.2 Abspeichern von Variablen Gespeicherte Objekte können weiterverwendet werden x &lt;- 5 x + 10 [1] 15 y &lt;- 2*3 y+x [1] 11 z &lt;- y+x Es können alle möglichen Elemente in Objekten gespeichert werden Text: z &lt;- &quot;Textelement&quot; z [1] &quot;Textelement&quot; Vergleichsergebnisse y &lt;- 5&lt;4 y [1] FALSE 3.3.3 Hinweise zur Benennung** R unterscheidet Groß- und Kleinschreibung: \\(\\texttt{X}\\) und \\(\\texttt{x&quot;&quot;}\\) sind nicht das Gleiche Variablennamen dürfen nur Buchstaben, Zahlen, Unterstrich und Punkt enthalten Variablennamen sollten keine Leerzeichen enthalten Umlaute und ß wenn möglich vermeiden Variablennamen dürfen nicht mit einer Zahl oder einem Unterstrich beginnen 7b &lt;- 5+5 Error: &lt;text&gt;:1:2: unexpected symbol 1: 7b ^ 3.3.4 Enviroment Gespeicherte Objekte sind im Workspace (Environment) mit zusätzlichen Informationen aufgelistet Objekte können durch das Kästchen ausgewählt und durch das Besensymbol gelöscht werden 3.3.5 Befehle für Zugriff auf Environment: ls() listet in der R-Konsole den Workspace auf Mit rm(“objektname1”,“objektname2”) werden Objekte entfernt Mit rm(list = ls() werden alle Objekte entfernt 3.4 Funktionen 3.4.1 Woraus besteht eine Funktion? R beinhaltet eine große Menge an Grundfunktionen Allgemeine Struktur: Funktion(Argument1, Argument2, …) Die Argumente definieren und spezifizieren die Funktion Beispiele für einfache Funktionen (hier nur ein Argument): sqrt(x) berechnet Quadratwurzel mean(X) berechnet Mittelwert eines Vektors 3.4.1.1 Funktionen mit mehreren Argumenten c(Argument1, Argument2, Argument3)} definiert einen Vektor mit drei Werten (z.B. Spaltennamen) c(1,2,3,4,5) [1] 1 2 3 4 5 c(&quot;Hund&quot;,&quot;Katze&quot;,&quot;Maus&quot;) [1] &quot;Hund&quot; &quot;Katze&quot; &quot;Maus&quot; round(x, digits = 2)} rundet x auf 2 Nachkommastellen round(3.141593, digits = 2) [1] 3.14 3.4.2 Default-Werte in Funktionen Nicht alle Argumente von Funktionen müssen angegeben werden Diese optionalen Argumente nehmen dann default-Werte ein Beispielsweise bei \\(\\texttt{round()}\\) ist der Default-Wert \\(\\texttt{digits = 0}\\) round(3.141593) [1] 3 default-Werte können in R Documentation nachgesehen werden (s. Help-Funktion) Die Argumente müssen nicht explizit benannt werden round(3.141593,2) [1] 3.14 round(2,3.141593) [1] 2 Dann ist aber die Reihenfolge wichtig, in der die Werte der Argumente eingegeben werden Benennen ist allerdings nachvollziehbarer (hilft, Fehler zu vermeiden) 3.4.3 Exkurs: Help-Funktion Durch die Funktion \\(\\texttt{help()}\\) und \\(\\texttt{?}\\) lässt sich unter anderem herausfinden, welche Funktion welche Argumente benötigt help(round) ?round 3.4.4 Abspeichern von Output einer Funktionen** Die Ergebnisse von Funktionen können ebenfalls unter Objekten abgespeichert (und weiterverwendet) werden x &lt;- c(1,2,3,4,5) x [1] 1 2 3 4 5 r &lt;- round(3.141593,2) r+5 [1] 8.14 3.5 Datenstrukturen 3.5.1 Übersicht 3.5.2 Vektoren eindimensionale Datenstruktur, in dem mehreren Elemente enthalten sind können im Enviroment als Objekt abgespeichert werden Mathematische und logische Operatoren können auf den Vektor (auf alle Elemente) angewandt werden x &lt;- c(1,2,3,4,5) x+1 [1] 2 3 4 5 6 x&gt;3 [1] FALSE FALSE FALSE TRUE TRUE Durch eckige Klammern \\(\\texttt{[]}\\) können durch Angabe ihrer Position einzelne Elemente eines Vektors ausgewählt (indiziert) werden. (Wichtig: in R beginnt der Index immer mit 1) y &lt;- c(110,120,130,140,150) y [1] 110 120 130 140 150 y[3] [1] 130 y[c(1,3,5)] [1] 110 130 150 Die Elemente können auch aufgrund von gewissen Bedingungen ausgewählt werden y &lt;- c(110,120,130,140,150) y [1] 110 120 130 140 150 y[y&gt;=130] [1] 130 140 150 y[y==120] [1] 120 Steht ein negatives Vorzeichen vor dem Index, so wird jedes Element außer dem indizierten aus dem Vektor ausgewählt x &lt;- c(1,2,3,4,5) x [1] 1 2 3 4 5 x[-3] [1] 1 2 4 5 x[c(-1,-3)] [1] 2 4 5 Soll eine gewisse Reihe von angrenzenden Elementen (von Element i bis Element j) vektorisiert werden, werden die Indizes mit \\(\\texttt{:}\\) verbunden (also \\(\\texttt{i:j}\\)) x &lt;- c(1:5) x [1] 1 2 3 4 5 x[1:3] [1] 1 2 3 3.5.3 Matrizen Zweidimensionale Datenstruktur Matrizen werden in R mit \\(\\texttt{matrix(data = …, nrow = …, ncol = …, byrow = …)}\\) erstellt data ist ein Vektor, der die Elemente der Matrix enthält nrow definiert die Anzahl der Zeilen der Matrix ncol definiert die Anzahl der Spalten der Matrix Wird die Matrix zeilenweise (byrow = TRUE) oder spaltenweise (byrow = FALSE) mit den Elementen befüllt? d &lt;- c(1:9) M &lt;- matrix(data = d,nrow = 3,ncol = 3,byrow = TRUE) M [,1] [,2] [,3] [1,] 1 2 3 [2,] 4 5 6 [3,] 7 8 9 Die Auswahl von Matrixelementen ist analog zu der Auswahl von Elementen aus einem Vektor Allerdings wird bei einer Matrix ein Element durch zwei Positionen (Zeile und Spalte) definiert M [,1] [,2] [,3] [1,] 1 2 3 [2,] 4 5 6 [3,] 7 8 9 M[3,2] [1] 8 Sollen alle Spalten in einer bestimmten Zeile ausgewählt werden, bleibt der Spaltenindex leer Analog für die Auswahl aller Zeilen in einer bestimmten Spalte Negative Indizierung und Mehrfachauswahl auch möglich M [,1] [,2] [,3] [1,] 1 2 3 [2,] 4 5 6 [3,] 7 8 9 M[3,] [1] 7 8 9 M[,2] [1] 2 5 8 3.5.4 Dataframes Ein Dataframe ist eine Datenstruktur ähnlich zur Matrix jede Spalte beinhaltet Werte einer Variable und jede Reihe Werte von jeder Variable (Spalten müssen gleichlang sein) Data Frames können mit data.frame()} aus mehreren (gleichlangen) Vektoren erstellt werden Alter &lt;- c(16,17,15,22) Geschlecht &lt;- c(&quot;w&quot;,&quot;w&quot;,&quot;m&quot;,&quot;m&quot;) Dataframe &lt;- data.frame(Alter,Geschlecht) Dataframe Alter Geschlecht 1 16 w 2 17 w 3 15 m 4 22 m Verschieden Möglichkeiten für Elementauswahl Zeilen- und Spaltennummer (analog zu Matrizen) Dataframe[2,1] [1] 17 oder mithilfe des $} operators direkt auf Variablen zugreifen Dataframe$Alter [1] 16 17 15 22 Dataframe$Alter[2] [1] 17 Mit dem $ Operator können zudem einzelne Werte verändert werden Dataframe$Geschlecht[2] &lt;- &quot;m&quot; … oder ganze Variablen gelöscht werden Dataframe$Geschlecht &lt;- NULL ** Um festzustellen, ob die Veränderungen funktioniert haben, muss nicht immer das Objekt extra in der Konsole aufgerufen werden. Es reicht im Enviroment zu schauen ob sich das Objekt entsprechend verändert hat.** Desweiteren können auch neue Variablen hinzugefügt werden Dataframe$Aktuelles_Jahr &lt;- rep(2022,times=4) …oder aus bestehenden Variablen gebildet werden Dataframe$Geburtsjahr &lt;- Dataframe$Aktuelles_Jahr - Dataframe$Alter Dataframe Alter Aktuelles_Jahr Geburtsjahr 1 16 2022 2006 2 17 2022 2005 3 15 2022 2007 4 22 2022 2000 3.6 Packages R besteht aus einer Reihe von vorinstallierten Grundfunktionen Zusätzliche Funktionen erhält man durch externe Pakete (packages) Diese müssen heruntergeladen und eingebunden werden Anzahl R Packages 3.6.1 Installieren von Packages library()}: Auflistung aller bereits installierten Pakete install.packages(“NamedesPakets”)} (!Hier unbedingt in Anführungszeichen): Paket installieren (einmalig), Internetverbindung benötigt library(NamedesPakets)}: Laden eines Pakets (in jeder R-Sitzung nötig) "],["datenmanagement-i.html", "4 Datenmanagement I 4.1 Daten importieren 4.2 Daten importieren 4.3 Daten speichern 4.4 Datensätze zusammenfügen", " 4 Datenmanagement I 4.1 Daten importieren 4.1.1 Organisation der Dateien Empfehlung: Einen Ordner für alle Datendateien innerhalb des Projektordners (bspw. Data) 4.1.2 Exkurs: Dateipfade Beispiel für den Dateipfad einer beliebigen Datendatei Daten.txt (lässt sich aus den Eigenschaften einer Datei entnehmen): D:\\Dokumente\\Kurse\\Rintensiv\\Projektordner\\data Wenn nur vom Projektordner aus navigiert werden muss, dann entfällt bei der Pfadangabe der ganze Teil vor data (relative Pfadangabe) Wichtig: R nutzt Backslash “\\” zu anderen Zwecken, daher muss ein normaler Slash “/” genutzt werden Die Pfadangabe für Daten.txt würde also so aussehen: data/Daten.txt 4.1.3 Daten aus .txt, .csv, .xlsx und weiteren Dateiformaten auslesen Dataset &lt;- read.table(&quot;data/Daten.txt&quot;, header = TRUE, sep = &quot;&quot;, dec = &quot;,&quot;) Dateipfad immer in ““ angeben Navigieren innerhalb des Projektordners (Vorteil: keine vollständigen Dateipfade müssen angegeben werden) header (default: TRUE) nimmt die erste Zeile als Variablennamen sep (default: ““) legt fest wie die Spalten in der Ursprungsdatei getrennt sind dec legt fest welches Zeichen zur Dezimaltrennung genutzt wird Befehl für .csv Dateien Dataset &lt;- read.csv(&quot;data/Daten.csv&quot;) Befehl für .xlsx Dateien ## Benötigtest Package installieren und aktivieren install.packages(&quot;xlsx&quot;) library(xlsx) ## Datensatz einlesen Dataset &lt;- read.xlsx(&quot;data/Daten.xlsx&quot;) Befehl für .sav Dateien (SPSS) ## Benötigtest Package installieren und aktivieren isntall.packages(&quot;haven&quot;) library(haven) ## Datensatz einlesen Dataset &lt;- read_sav(&quot;data/Daten.sav&quot;) 4.1.4 Alternative per Klickpfad Im Environment auf Import Dataset klicken und gewünschtes Format auswählen: Anschließend gewünschtes Dataset suchen und anklicken. 4.2 Daten importieren Vorschau mit allen Argumenten, die auch im read.table() enthalten sind: Eingabe in die Konsole: Daten_2 &lt;- read.csv(&quot;data/Daten_2.txt&quot;, sep=&quot;&quot;) View(Daten_2) Vorteil: automatisch richtiger Befehl Vorschau Nachteil Kompletter Dateipfad angegeben, nicht brauchbar für andere 4.2.1 Überblick verschaffen str() gibt die Struktur des Dataframes aus str(Dataset) &#39;data.frame&#39;: 10 obs. of 12 variables: $ Person: int 1 2 3 4 5 6 7 8 9 10 $ item1 : int 3 4 3 2 2 4 2 3 3 4 $ item2 : int 2 1 1 2 2 1 3 1 1 1 $ item3 : int 3 4 3 2 2 3 3 4 3 3 $ item4 : int 2 3 3 2 2 3 2 4 3 2 $ item5 : int 2 1 1 2 1 1 2 1 1 1 $ item6 : int 1 1 1 2 1 1 3 1 1 1 $ item7 : int 4 4 3 2 3 4 2 4 3 4 $ item8 : int 1 1 1 2 2 1 4 1 1 2 $ item9 : int 1 1 1 2 2 1 4 1 1 1 $ item10: int 3 4 3 2 2 4 1 3 3 4 $ item11: int 4 2 4 4 2 5 4 2 4 4 dim() gibt die Dimensionen (Zeilen und Spalten) einer Matrix an dim(Dataset) #&gt; [1] 10 12 head() zeigt die ersten n Zeilen anzeigen head(Dataset, 2) Person item1 item2 item3 item4 item5 item6 item7 item8 1 1 3 2 3 2 2 1 4 1 2 2 4 1 4 3 1 1 4 1 item9 item10 item11 1 1 3 4 2 1 4 2 View(Dataset) öffnet eine Ansicht ähnlich zu Excel (View groß geschrieben!) names() gibt die Variablennamen des Datensatzes aus names(Dataset) [1] &quot;Person&quot; &quot;item1&quot; &quot;item2&quot; &quot;item3&quot; &quot;item4&quot; &quot;item5&quot; [7] &quot;item6&quot; &quot;item7&quot; &quot;item8&quot; &quot;item9&quot; &quot;item10&quot; &quot;item11&quot; Damit lassen sich auch Namen von Variablen im Datenesatz verändern names(Dataset)[names(Dataset) == &quot;item11&quot;] &lt;- &quot;item12&quot; names(Dataset) [1] &quot;Person&quot; &quot;item1&quot; &quot;item2&quot; &quot;item3&quot; &quot;item4&quot; &quot;item5&quot; [7] &quot;item6&quot; &quot;item7&quot; &quot;item8&quot; &quot;item9&quot; &quot;item10&quot; &quot;item12&quot; 4.3 Daten speichern 4.3.1 Datensatz exportieren als .txt, .csv, .xlsx oder .sav Befehl: write.table(data, file = “Daten.txt”, sep = “…”, row.names = TRUE, col.names = TRUE) Argumente data: Welcher Dataframe soll exportiert werden file: Name der neuen Datei (.txt am ende nicht Vergessen!) sep, dec: Wie bei read.table row.names: Zeilennummerierung als eigene Spalte col.names: Variablennamen als erste Zeile speichern Genauso wie bei den read()- Funktionen gibt es auch write.csv2() für .csv Dateien write.xlsx() für .xlsx Dateien (Package “xlsx” benötigt) write_sav() für .sav Dateien (Package “haven” benötigt) Wenn beispielsweise: file = “Unterordner/exportierteDatei.txt”**, dann im Unterodner. Wenn kein Pfad angegeben wird, dann liegt die Datei im Projektordner. 4.4 Datensätze zusammenfügen 4.4.1 Fälle hinzufügen Situation: Es wurde eine weitere Erhebung mit anderen Personen durchgeführt, in der die gleichen Items abgefragt wurden Dataset1 &lt;- read.table(&quot;data/Daten.txt&quot;, header = TRUE) head(Dataset1,2) Person item1 item2 item3 item4 item5 item6 item7 item8 1 1 3 2 3 2 2 1 4 1 2 2 4 1 4 3 1 1 4 1 item9 item10 item11 1 1 3 4 2 1 4 2 Dataset2 &lt;- read.table(&quot;data/Daten_2.txt&quot;, header = TRUE) head(Dataset2,2) Person item1 item2 item3 item4 item5 item6 item7 item8 1 11 5 3 5 3 2 6 4 4 2 12 3 5 2 3 5 3 3 2 item9 item10 item11 1 3 3 4 2 6 4 6 mit rbind() lassen sich Daten (Vektoren, Matrizen, Dataframes) zeilenweise (reihenweise) zusammenfügen (rbind = rowbind) es können unendlich viele Datensätze in diesem Befehl zusammengefügt werden New_Dataset &lt;- rbind(Dataset1,Dataset2) Überprüfung dim(New_Dataset) [1] 20 12 Bedingungen Die Datasets haben die gleiche Anzahl an Spalten Die Datasets haben die gleichen Namen für die Variablen 4.4.2 Variablen hinzufügen** Situation: Innerhalb der Erhebung gab es noch eine Abfrage von weiteren Items Dataset1 &lt;- read.table(&quot;data/Daten.txt&quot;, header = TRUE) head(Dataset1,2) Person item1 item2 item3 item4 item5 item6 item7 item8 1 1 3 2 3 2 2 1 4 1 2 2 4 1 4 3 1 1 4 1 item9 item10 item11 1 1 3 4 2 1 4 2 Dataset3 &lt;- Dataset3 &lt;- read.table(&quot;data/Daten_FB2.txt&quot;, header= TRUE) head(Dataset3,2) Person item12 item13 item14 item15 item16 item17 item18 1 1 5 3 5 3 2 6 4 2 2 3 5 2 3 5 3 3 item19 item20 item21 item22 1 4 3 3 4 2 2 6 4 6 Mit cbind() lassen sich Daten (Vektoren, Matrizen, Dataframes) spaltenweise zusammenfügen (cbind = columbind) New_Dataset2 &lt;- cbind(Dataset1, Dataset3) Überprüfung dim(New_Dataset2) [1] 10 24 Datensätze müssen gleiche Anzahl an Zeilen haben Datensätze müssen gleich sortiert sein Situation: Innerhalb der Erhebung gab es noch eine Abfrage von weiteren Items, allerdings fehlt eine Person \\(\\rightarrow\\) cbind()} führt nicht zum gewünschten Ergebnis Dataset1 &lt;- read.table(&quot;data/Daten.txt&quot;, header = TRUE) head(Dataset1,2) Person item1 item2 item3 item4 item5 item6 item7 item8 1 1 3 2 3 2 2 1 4 1 2 2 4 1 4 3 1 1 4 1 item9 item10 item11 1 1 3 4 2 1 4 2 Dataset4 &lt;- read.table(&quot;data/Daten_FB2_NA.txt&quot;, header= TRUE) head(Dataset4,2) Person item12 item13 item14 item15 item16 item17 item18 1 1 5 3 5 3 2 6 4 3 3 4 4 5 5 2 3 3 item19 item20 item21 item22 1 4 3 3 4 3 3 5 7 5 Mit merge(…, …, by=“…“) lassen sich Daten (Vektoren, Matrizen, Dataframes) spaltenweise besser zusammenfügen Anhand einer Schlüsselvariablen (by) können zusammengehörige Fälle erkannt werden by: Welche Variable ist die Schlüsselvariable? by.x, by.y: wie by, aber separate Angabe pro Datensatz all: Alle Fälle behalten (TRUE) oder nur die in beiden Datensätzen vorhandenen (FALSE) all.x, all.y: wie all, aber nur für einen der Datensätze sort: Neu nach Schlüsselvariable sortieren? (TRUE = ja) all = FALSE: Nur die Fälle werden behalten, die in beiden Datensätzen vorhanden sind New_Dataset3 &lt;- merge(Dataset1, Dataset4, by = &quot;Person&quot;, all = FALSE) head(New_Dataset3,2) Person item1 item2 item3 item4 item5 item6 item7 item8 1 1 3 2 3 2 2 1 4 1 2 3 3 1 3 3 1 1 3 1 item9 item10 item11 item12 item13 item14 item15 item16 1 1 3 4 5 3 5 3 2 2 1 3 4 4 4 5 5 2 item17 item18 item19 item20 item21 item22 1 6 4 4 3 3 4 2 3 3 3 5 7 5 –&gt; Person 2 wird raus-gefiltert für alle restlichen Personen werden die neuen Variablen übernommen all = TRUE: Alle Fälle werden beibehalten, fehlende Werte werden durch NA ergänzt New_Dataset3 &lt;- merge(Dataset1, Dataset4, by = &quot;Person&quot;, all = TRUE) head(New_Dataset3,2) Person item1 item2 item3 item4 item5 item6 item7 item8 1 1 3 2 3 2 2 1 4 1 2 2 4 1 4 3 1 1 4 1 item9 item10 item11 item12 item13 item14 item15 item16 1 1 3 4 5 3 5 3 2 2 1 4 2 NA NA NA NA NA item17 item18 item19 item20 item21 item22 1 6 4 4 3 3 4 2 NA NA NA NA NA NA –&gt; Person 2 bleibt erhalten. Die nicht vorhandenen VAriablen werden mit NA gefüllt "],["datenmanagement-ii.html", "5 Datenmanagement II 5.1 Datenbearbeitung 5.2 Long- und Wide-Format", " 5 Datenmanagement II 5.1 Datenbearbeitung Wir wissen nun wie Daten eingelesen und zusammengefügt werden Bevor wir mit unserem Datensatz Statistik betreiben können, müssen aber meist noch weitere Schritte der Datenbearbeitung durchgeführt werden Beispieldatensatz: PersonenDaten &lt;- read.table(&quot;data/PersonenDaten.txt&quot;, header = TRUE) head(PersonenDaten, 5) Person item1 item2 item3 item4 item5 item6 item7 item8 1 1 3 2 3 2 2 1 4 1 2 2 4 1 4 3 1 1 4 1 3 3 3 1 3 3 1 1 3 1 4 4 2 2 2 2 2 2 2 2 5 5 2 2 2 2 1 1 3 2 item9 item10 item11 age sex 1 1 3 4 25 2 2 1 4 2 27 1 3 1 3 4 38 1 4 2 2 4 31 2 5 2 2 2 22 1 5.1.1 Umkodierung Situation: Im PersonDaten- Datensatz ist die Variable mit 1 und 2 Kodiert. Die Kodierung soll aber 0 und 1 sein. Der $-Operator schafft hier Abhilfe wenn: eine neue Variable aus einer alten erstellt werden soll PersonenDaten$sexUmkodiert &lt;- PersonenDaten$sex-1 PersonenDaten$sexUmkodiert [1] 1 0 0 1 0 1 1 1 0 0 1 1 1 1 1 0 0 1 0 1 eine existierende Variable überschrieben werden soll PersonenDaten$sex &lt;- PersonenDaten$sex-1 PersonenDaten$sex [1] 1 0 0 1 0 1 1 1 0 0 1 1 1 1 1 0 0 1 0 1 Situation: Einige Items im Datensatz sind umgepolt worden. Für die weitere Verarbeitung sollen sie zurückgepolt werden. Anwendung der Formel: Item_Umkodiert = max(Skala_Item)+ 1 - Item_Original Beispiel: Umpolung von Item3 mit 6-stufiger Skala PersonenDaten$item3_umkodiert &lt;- 6 + 1 - PersonenDaten$item3 Original Umkodiert 1 3 4 2 4 3 3 3 4 4 2 5 5 2 5 5.1.2 Dichotomisierung Situation: Das Alter der Personen soll in eine Dummy Variable überführt werden, bei der alle maximal 30 jährigen den Wert 0 und alle älter als 30 den Wert 1 haben sollen. ifelse(test, yes, no) liefert die Möglichkeit, R-Befehle abhängig von einer Bedingung auszuführen: - test: Bedingung, die überprüft werden soll - yes: Welcher Wert soll zugewiesen werden, wenn die Bedingung erfüllt ist? - no: Welcher Wert soll zugewiesen werden, wenn die Bedingung nicht erfüllt ist? PersonenDaten$AlterDichotom &lt;- ifelse(PersonenDaten$age &lt;= 30, 0, 1) Überprüfung: Überprüfung &lt;- data.frame(Original=PersonenDaten$age, Umkodiert=PersonenDaten$AlterDichotom) head(Überprüfung,5) Original Umkodiert 1 25 0 2 27 0 3 38 1 4 31 1 5 22 0 5.1.3 Kategorisierung Situation: Das Alter der Personen soll in eine weitere Variable Altersgruppen überführt werden, bei der: - alle unter 25 den Wert 1 - alle von 25 bis 30 den Wert 2 - alle von 30 bis 35 den Wert 3 - alle über 35 den Wert 4 erhalten Dataset$neueVariable[test] &lt;- yes: Logisches Indizieren ist ein einfacher Weg, Variablenwerte abhängig von Bedingungen festzusetzen test: Bedingung, die überprüft werden soll yes: Welcher Wert soll zugewiesen werden, wenn die Bedingung erfüllt ist? PersonenDaten$Altersgruppen[PersonenDaten$age &lt;= 25] &lt;- 1 PersonenDaten$Altersgruppen[PersonenDaten$age &gt; 25 &amp; PersonenDaten$age &lt;= 30] &lt;- 2 PersonenDaten$Altersgruppen[PersonenDaten$age &gt; 30 &amp; PersonenDaten$age &lt;= 35] &lt;- 3 PersonenDaten$Altersgruppen[PersonenDaten$age &gt; 35] &lt;- 4 Überprüfung: Original Umkodiert 1 25 1 2 27 2 3 38 4 4 31 3 5 22 1 5.1.4 Skalenbildung Situation: aus den Werten der Items 1 bis 11 soll eine Summenscore/Mittelwertsscore gebildet werden. rowSums(x, na.rm = TRUE) bildet zeilenweise die Summe rowMeans(x, na.rm = TRUE) bildet zeilenweise den Mittelwert x: Legt den Bereich des Datensatzes fest für den die Funktion durchgeführt werden soll. Hier sollen alle Zeilen und die Spalten von 2 bis 12 einbezigen werden. na.rm: Bei TRUE werden fehlende Werte bei der Berechnung ausgelassen, bei FALSE wird die Berechnung nicht durchgeführt, sobald ein einzelner Wert fehlt PersonenDaten$Mittelwertsscore &lt;- rowMeans(PersonenDaten[,2:12], na.rm = T) PersonenDaten$Summenscore &lt;- rowSums(PersonenDaten[ ,2:12], na.rm = T) 5.1.5 Subsets erstellen Situation: Es sollen alle Personen heraus gefiltert werden, deren score von Item 1 über 4 oder von Item 2 über 3 liegt. Der neue Datensatz soll nur die Variablen Person, Item 1 und Item 2 enthalten. Die subset(x, subset, select) -Funktion bietet die Möglichkeit einen Datensatz nach bestimmten Voraussetzungen zu Filtern x: Datensatz subset: logische Aussage, wenn TRUE wird der Eintrag in das Subset übernommen, wenn FALSE aussortiert. select: Variablen die im Subset beibehalten werden sollen PersonenDatenSubset &lt;- subset(PersonenDaten, PersonenDaten$item1 &gt; 4| PersonenDaten$item2 &gt; 3, select = c(Person,item1,item2)) PersonenDatenSubset Person item1 item2 11 11 5 3 12 12 3 5 13 13 4 4 15 15 3 5 16 16 5 1 19 19 5 3 20 20 5 1 5.2 Long- und Wide-Format 5.2.1 Wide Format Alle Datensätz mit denen wir bisher gearbeitet haben, hatten folgende Eigenschaften: - Eine Zeile pro Person - Messwiederholungen (Items) als mehrere Variablen (Spalten) dargestellt - Häufige Variante für Messwiederholungsdaten bei SPSS - Praktisch bei der Dateneingabe Dataset &lt;- read.table(&quot;data/Daten.txt&quot;,header = TRUE) head(Dataset, 5) Person item1 item2 item3 item4 item5 item6 item7 item8 1 1 3 2 3 2 2 1 4 1 2 2 4 1 4 3 1 1 4 1 3 3 3 1 3 3 1 1 3 1 4 4 2 2 2 2 2 2 2 2 5 5 2 2 2 2 1 1 3 2 item9 item10 item11 1 1 3 4 2 1 4 2 3 1 3 4 4 2 2 4 5 2 2 2 5.2.2 Long-Format DatasetLong &lt;- read.table(&quot;data/Daten_Lang.txt&quot;, header = TRUE) head(DatasetLong,14) Person Item Score 1 1 item1 3 2 2 item1 4 3 3 item1 3 4 4 item1 2 5 5 item1 2 6 6 item1 4 7 7 item1 2 8 8 item1 3 9 9 item1 3 10 10 item1 4 11 1 item2 2 12 2 item2 1 13 3 item2 1 14 4 item2 2 Eigenschaften: - Eine Zeile pro Bedingung (Item) - Mehrere Zeilen pro Person - Messwiederholung wird in mehreren Zeilen dargestellt Wozu? Viele Modelle in R benötigen das Long Format 5.2.3 Long- und Wide-Format Wide –&gt; Long Mit dem Paket reshape2 können die Formate ineinander überführt werden. Für die Umformung wird der Befehl melt(data, id.vars = “…”, variable.name = “…”, value.name = “…”) verwendet: - data: Datensatz, dessen Format geändert werden soll - id.vars: Variable(n), die als einzelne Variable beibehalten werden soll(en) und nicht gesplittet werden soll(en). Bennenung muss gleich wie im Ursprungsdatensatz sein! - variable.name: Bezeichnung der Spalte, in der Variablennamen aufgeführt werden sollen Achtung: nicht Variablennamen selbst auflisten! - value.name: Bezeichnung der Spalte, in der die Werte der zugehörigen Variablen aufgeführt werden sollen library(reshape2) Dataset &lt;- read.table(&quot;data/Daten.txt&quot;, header = TRUE) DatenLong &lt;- melt(Dataset, id.vars = &quot;Person&quot;, variable.name = &quot;item&quot;, value.name = &quot;score&quot;) head(DatenLong) Person item score 1 1 item1 3 2 2 item1 4 3 3 item1 3 4 4 item1 2 5 5 item1 2 6 6 item1 4 Mit dem Paket reshape2 können die Formate ineinander übergeführt werden Der Befehl dafür dcast(data, formula, value.var = “…”) data: Datensatz, dessen Format geändert werden soll formula: Welche Variablen sollen nach welcher Bedingung getrennt werden? Format: A ~ B} A: ID-Variablen (vgl. id.vars) B: Variable(n), die die Messungen (Zeitpunkte, Items, …) enthält –&gt; Bildet neue Spalten (vgl. variable.name) Mit + werden mehrere Variablen in formula kombiniert value.var: Welche Variable im Long-Format enthält die Werte, die auf gesplittet werden sollen? (vgl. value.name) head(DatenLong,3) Person item score 1 1 item1 3 2 2 item1 4 3 3 item1 3 library(reshape2) DatenWide &lt;- dcast(DatenLong, Person ~ item, value.var = &quot;score&quot;) head(DatenWide,5) Person item1 item2 item3 item4 item5 item6 item7 item8 1 1 3 2 3 2 2 1 4 1 2 2 4 1 4 3 1 1 4 1 3 3 3 1 3 3 1 1 3 1 4 4 2 2 2 2 2 2 2 2 5 5 2 2 2 2 1 1 3 2 item9 item10 item11 1 1 3 4 2 1 4 2 3 1 3 4 4 2 2 4 5 2 2 2 "],["deskriptive-statistik.html", "6 Deskriptive Statistik 6.1 Absolute und Relative Häufigkeiten 6.2 Grundbefehle 6.3 Gruppengetrennte Analyse 6.4 Ergänzungen", " 6 Deskriptive Statistik 6.1 Absolute und Relative Häufigkeiten 6.1.1 Absolute Häufigkeiten Absolute Häufigkeiten können durch table() in einer Tabelle ausgegeben werden # Datensatz einlesen df_yoga &lt;- read.table(&quot;data/YogaPilates.txt&quot;, header = TRUE) # Beispiel für Tabelle tab &lt;- table(df_yoga$gruppe) tab kontroll pilates yoga 45 46 29 Die Gesamtzahl der Beobachtungen kann mit sum() ausgegeben werden: # Anzahl der Beobachtungen in einer Kreuztabelle mit sum() sum(tab) [1] 120 6.1.2 Relative Häufigkeiten Berechnung der relativen Häufigkeiten → Welchen Anteil macht eine Kategorie an der Gesamthäufigkeit aus? Genauso gut können wir die Funktion prop.table() verwenden # Kreuztabelle tab_rel mit relativen Häufigkeiten (Anteil/Gesamthäuf.) tab_rel &lt;- tab/sum(tab) # Alternative Berechnung mit prop.table() tab_rel &lt;- prop.table(tab) kontroll pilates yoga 0.38 0.38 0.24 6.1.3 Kreuztabelle/Kontingenztafel für mehrere Variablen Wie ist die (absolute und relative) Häufigkeit der Frauen und Männer (geschl) in den einzelnen Gruppen (gruppe)? Bei Angabe von mehreren Vektoren werden Kreuztabellen erzeugt # Kreuztabelle für Geschlecht und Gruppe tab2 &lt;- table(df_yoga$geschl, df_yoga$gruppe) # prop.table() für relative Häufigkeiten (auf 2 Stellen gerundet) round(prop.table(tab2), 2) kontroll pilates yoga m 0.12 0.10 0.05 w 0.26 0.28 0.19 6.2 Grundbefehle R-Befehl Bedeutung sum() Summe mean() Mittelwert var() Varianz sd() Standardabweichung min() Minimum max() Maximum quantile() Quartile range() Range median() Median Beispiele range(df_yoga$alter) [1] 21 40 mean(df_yoga$alter) [1] 30.73333 var(df_yoga$zufri) [1] NA Fehlende Werte Enthalten unsere Daten fehlende Werte (NA), dann ergeben die deskriptiven Berechnungen auch NA. Durch das Argument na.rm = TRUE werden NA-Werte ignoriert. mean(df_yoga$zufri) [1] NA mean(df_yoga$zufri, na.rm = TRUE) [1] 3.543103 Funktion summary() Mit summary() werden verschiedene deskriptive Statistiken ausgegeben: summary(df_yoga$alter) Min. 1st Qu. Median Mean 3rd Qu. Max. 21.00 26.00 30.50 30.73 35.25 40.00 summary() gibt zudem die Anzahl fehlender Werte an: summary(df_yoga$zufri) Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s 1.000 3.000 4.000 3.543 4.000 5.000 4 Paket psych Mit der Funktion describe() aus dem Paket psych lassen sich eine Vielzahl von Verteilungsparametern gleichzeitig ausgeben: library(psych) describe(df_yoga$alter, skew = FALSE) vars n mean sd min max range se X1 1 120 30.73 5.76 21 40 19 0.53 Optionale Argumente: - skew = FALSE, um Schiefe und Kurtosis nicht auszugeben - ranges = FALSE, um u.a. Median, Minimum und Maximum nicht auszugeben - IQR = TRUE, um Interquartilbereich auszugeben 6.3 Gruppengetrennte Analyse Wie können die Variablen alter, zufri und angst deskriptiv in den einzelnen Gruppen berechnet werden? # Ersten 5 Zeilen Beispieldatasatz head(df_yoga, 5) vp geschl alter gruppe zufri angst 1 AA21 w 37 yoga 5 1 2 AW14 m 31 pilates 4 4 3 BA55 w 38 yoga 4 2 4 BA76 m 35 yoga 5 2 5 BP45 w 23 pilates 4 1 6.3.1 Logisches Indizieren Jeweils Teile der Daten durch logisches Indizieren auswählen „Berechne Mittelwert der Spalte $alter, aber wähle hierfür nur Werte der Personen aus Gruppe Yoga“ # Durchschnittsalter, Gruppe = yoga mean(df_yoga$alter[df_yoga$gruppe == &quot;yoga&quot;]) [1] 31.13793 # Durchschnittsalter, Gruppe = pilates mean(df_yoga$alter[df_yoga$gruppe == &quot;pilates&quot;]) [1] 30.86957 # Durchschnittsalt,er, Gruppe = kontroll mean(df_yoga$alter[df_yoga$gruppe == &quot;kontroll&quot;]) [1] 30.33333 6.3.2 Funktion aggregate() Mit aggregate() können Funktionen für verschiedene Faktorenstufen (und deren Kombination) getrennt berechnet werden: aggregate(AV ~ UV, FUN = …, data = …) AV: Variable, deren Werte analysiert werden sollen UV: Faktor(en), mehrere Faktoren werden mit + verbunden FUN: Welche Funktion (deskriptive Statistik) soll berechnet werden? data: datasatz Beispiel # Durchschnitt Zufriedenheit, Gruppen = geschl, gruppe aggregate(zufri ~ geschl + gruppe, FUN = mean, data = df_yoga) geschl gruppe zufri 1 m kontroll 3.785714 2 w kontroll 3.258065 3 m pilates 3.181818 4 w pilates 3.454545 5 m yoga 4.166667 6 w yoga 3.952381 6.3.3 Funktion describeBy() aus dem Paket psych Anwendung der describe-Funktion getrennt nach Faktor(en) describeBy(x = …, group = list(…), …) x: Variable, deren Werte analysiert werden sollen (wie AV bei aggregate()) group = list(): Faktor(en), mehrere Faktoren werden mit “,” verbunden (wie UV bei aggregate()) Beispiel describeBy() # Deskriptive Statistiken nach Gruppe(=geschl) describeBy(x = df_yoga$zufri, group = list(df_yoga$geschl), skew = FALSE) Descriptive statistics by group : m vars n mean sd min max range se X1 1 31 3.65 0.95 2 5 3 0.17 --------------------------------------------- : w vars n mean sd min max range se X1 1 85 3.51 1.06 1 5 4 0.12 6.4 Ergänzungen 6.4.1 Bedingte Wahrscheinlichkeiten Mit prop.table(…, margin = …) werden bedingte Wahrscheinlichkeiten für ein Merkmal ausgegeben. (margin 1 = zeilenweise, margin 2 = spaltenweise) # Bedingte Wahrscheinlichkeit, zeilenweise, gerundet auf 2 Stellen round(prop.table(tab2, margin = 1), 2) kontroll pilates yoga m 0.44 0.38 0.19 w 0.35 0.39 0.26 # Bedingte Wahrscheinlichkeit, spaltenweise, gerundet auf 2 Stellen round(prop.table(tab2, margin = 2), 2) kontroll pilates yoga m 0.31 0.26 0.21 w 0.69 0.74 0.79 6.4.2 Skalieren Mit dem Befehl scale(…, center = …, scale = …) können Variablen zentriert und z-standardisiert werden. center: Soll von jedem Wert in der Variable der Variablenmittelwert abgezogen werden? (TRUE = ja) scale: Soll jeder Wert in der Variable durch die Variablenstandardabweichung dividiert werden? (TRUE = ja) Zentrierung, wenn nur center = TRUE, z-Standardisierung, wenn beide Argumente mit TRUE definiert wurden. 6.4.3 Extremwerte Mit dem Argument trim = … im Befehl mean() kann der Anteil ausgeschlossener Extremwerte definiert werden Das Paket moments gibt mit dem Befehl all.moments() alle statistischen Momente aus, mit skewness() die Schiefe und mit kurtosis() den Exzess 6.4.4 Gruppengetrennte Analyse - Funktion tapply() tapply(X = …, INDEX = list(…), FUN = …) X: Variable, deren Werte analysiert werden sollen (wie AV bei aggregate()) INDEX = list(): Faktor(en), mehrere Faktoren werden mit “,” verbunden (wie UV bei aggregate()) Bei einem Faktor muss list() nicht angegeben werden FUN: Welche Funktion (deskriptive Statistik) soll berechnet werden? (wie bei aggregate()) "],["korrelation-und-regression.html", "7 Korrelation und Regression 7.1 Kovarianz und Korrelation 7.2 Matrizen 7.3 (Multiple) lineare Regression 7.4 Ergänzungen", " 7 Korrelation und Regression 7.1 Kovarianz und Korrelation Beispieldatasatz Wie hängen die Itemantworten zusammen? # Beispieldatasatz einlesen df_exam &lt;- read.table(&quot;data/Daten.txt&quot;, header = TRUE) # Ersten 5 Zeilen und 8 Spalten anzeigen head(df_exam, c(5,8)) Person item1 item2 item3 item4 item5 item6 item7 1 1 3 2 3 2 2 1 4 2 2 4 1 4 3 1 1 4 3 3 3 1 3 3 1 1 3 4 4 2 2 2 2 2 2 2 5 5 2 2 2 2 1 1 3 Zwei Variablen Für die Berechnung der Korrelation wird cor() verwendet, für die Berechnung der Kovarianz cov() Argumente bei cor(x, y, method = “…”, use = “…”) x und y: datavektoren method: Welche Korrelation soll berechnet werden? Produkt-Moment-Korrelation (“pearson”), Rangkorrelation ρ (“spearman”) oder Rangkorrelation τ (“kendall”) use: Umgang mit fehlenden Werten (siehe nächste Folie) Fehlende Werte Use = “…” regelt den Umgang der Korrelationsfunktionen mit fehlenden Werten “everything”: Kein Umgang mit fehlenden Werten → bei einzelnen fehlenden Werten wird keine (einzige) Korrelation berechnet (Ergebnis: NA) “pairwise”: Korrelationen werden jeweils mit den vollständigen Fällen pro Variablenpaar berechnet (paarweiser Ausschluss) “complete”: Korrelationen werden nur aus in allen Variablen vollständigen Fällen berechnet (fallweiser Ausschluss) everything pairwise complete Funktion cor.test() cor.test(x, y, method = “…”, use = “…”, alternative = “…”, conf.level = …, …) für Inferenzstatistik bei Korrelationen. x, y, method, use: Siehe cor() alternative: Ist die Testrichtung… Ungerichtet (default): “two.sided” einseitig: Positive Korrelationshypothese “greater” einseitig: Negative Korrelationshypothese “less” conf.level: Konfidenzniveau (1 - α) default: 0.95 Beispiel # Korrelation item1, item2 cor(df_exam$item1, df_exam$item2, method = &quot;pearson&quot;) [1] -0.7698004 # Produkt-Moment-Korrelation, Hypothese: negativ ,alpha = 1% cor.test(df_exam$item1, df_exam$item2, method = &quot;pearson&quot;, alternative = &quot;less&quot;, conf.level = .99) Pearson&#39;s product-moment correlation data: df_exam$item1 and df_exam$item2 t = -3.4112, df = 8, p-value = 0.004603 alternative hypothesis: true correlation is less than 0 99 percent confidence interval: -1.0000000 -0.1396423 sample estimates: cor -0.7698004 7.2 Matrizen Werden Dataframes (oder Teile dessen) in cov() oder cor() eingefügt, werden Kovarianz- und Korrelationsmatrizen ausgegeben: cov(df_exam[, 2:4]) item1 item2 item3 item1 0.6666667 -0.4444444 0.3333333 item2 -0.4444444 0.5000000 -0.2222222 item3 0.3333333 -0.2222222 0.4444444 cor(df_exam[, c(&quot;item1&quot;, &quot;item2&quot;, &quot;item3&quot;)], method = &quot;pearson&quot;) item1 item2 item3 item1 1.0000000 -0.7698004 0.6123724 item2 -0.7698004 1.0000000 -0.4714045 item3 0.6123724 -0.4714045 1.0000000 corr.test() (Paket psych) liefert inferenzstatistische Berechnungen auch für Korrelationsmatrizen corr.test(df, method = “…”, adjust = “…”, alpha = “…”) df, method: Siehe cor(), Kendall τ kann aber nicht verwendet werden adjust: Methode zur α-Fehler-Adjustierung “none”, “bonferroni”, … Für weitere siehe ?p.adjust und ?corr.test alpha: Signifikanzniveau α library(psych) corr.test(df_exam[ ,2:4], method = &quot;pearson&quot;, adjust = &quot;bonferroni&quot;) Call:corr.test(x = df_exam[, 2:4], method = &quot;pearson&quot;, adjust = &quot;bonferroni&quot;) Correlation matrix item1 item2 item3 item1 1.00 -0.77 0.61 item2 -0.77 1.00 -0.47 item3 0.61 -0.47 1.00 Sample Size [1] 10 Probability values (Entries above the diagonal are adjusted for multiple tests.) item1 item2 item3 item1 0.00 0.03 0.18 item2 0.01 0.00 0.51 item3 0.06 0.17 0.00 To see confidence intervals of the correlations, print with the short=FALSE option Extraktion von Werten Mit $ können Werte aus der Berechnung extrahiert werden Dafür Korrelationsmatrix als Objekt speichern $se für Extraktion der Standardfehler $ci für Extraktion der Konfidenzintervalle $p für Extraktion der p-Werte $t für Extraktion der t-Werte Beispiel # Definition eines Objekts mit Inferenzstatistik für df_exam KorMat &lt;- corr.test(df_exam[ ,2:4], method = &quot;pearson&quot;, adjust = &quot;bonferroni&quot;) # Ausgabe Standardfehler KorMat$se item1 item2 item3 item1 0.0000000 0.2256677 0.2795085 item2 0.2256677 0.0000000 0.3118048 item3 0.2795085 0.3118048 0.0000000 # Ausgabe Konfidenzintervalle KorMat$ci lower r upper p item1-item2 -0.9425738 -0.7698004 -0.2720171 0.00920665 item1-item3 -0.0280809 0.6123724 0.8963838 0.05983788 item2-item3 -0.8490310 -0.4714045 0.2250053 0.16902020 # Ausgabe p-Werte KorMat$p item1 item2 item3 item1 0.00000000 0.02761995 0.1795136 item2 0.00920665 0.00000000 0.5070606 item3 0.05983788 0.16902020 0.0000000 7.3 (Multiple) lineare Regression Wiederholung: Ziel: Ein Kriterium (abhängige Variable) durch die Kombination von Prädiktoren (unabhängige Variablen) vorhersagen Signifikanztests für einzelne Prädiktoren das Gesamtmodell Modellvergleiche 7.3.1 Modelldefinition Um eine Regression in R zu rechnen, muss man anhand der vorliegenden Variablen ein Regressionsmodell spezifizieren Regressionsmodelle werden in sogenannten Formel-Objekten definiert Struktur: Abhängige Variable(n) ~ Unabhängige Variable + Unabhängige Variable + … Mehrere Variablen werden mit + verbunden Variablenbezeichnungen entsprechend anpassen item1 soll durch item2 und item3 vorhergesagt werden # Beispiel Modelldefinition model1 &lt;- item1 ~ item2 + item3 7.3.2 Modellschätzung Anhand des definierten Regressionsmodells und der vorhandenen Daten kann R nun die Modellparameter (inkl. Modellpassung) schätzen Lineare Regressionsmodelle werden mit lm(formula = …, data = …) geschätzt: formula: Definiert das Modell data: dataframe, der die Variablen enthält lm() sollte für die weiteren Berechnungen als Objekt gesichert werden Beispiel Modellschätzung # Modelldefinition model1 &lt;- item1 ~ item2 + item3 # Modellschätzung und Sicherung in fit1 fit1 &lt;- lm(formula = model1, data = df_exam) # Alternativ fit1 &lt;- lm(formula = item1 ~ item2 + item3, data = df_exam) # Modell anzeigen fit1 Call: lm(formula = item1 ~ item2 + item3, data = df_exam) Coefficients: (Intercept) item2 item3 2.8929 -0.7143 0.3929 Mit summary() lassen sich detaillierte Informationen zum Regressionsmodell ausgeben: # Ausgabe Informationen Regressionsmodell, fit1 summary(fit1) Call: lm(formula = item1 ~ item2 + item3, data = df_exam) Residuals: Min 1Q Median 3Q Max -0.75000 -0.33036 -0.08929 0.33036 0.64286 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 2.8929 1.1752 2.462 0.0434 * item2 -0.7143 0.2832 -2.523 0.0397 * item3 0.3929 0.3003 1.308 0.2322 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.5297 on 7 degrees of freedom Multiple R-squared: 0.6726, Adjusted R-squared: 0.5791 F-statistic: 7.191 on 2 and 7 DF, p-value: 0.02008 7.3.3 Modellvergleich Zwei genestete Modelle können mit anova(fit1, fit2) miteinander verglichen werden fit1: Ausgangsmodell (restringiertes Modell) fit2: Erweitertes Modell (mit zusätzlichen Prädiktoren) # Modelldefinition, model1 + model2 model1 &lt;- item1 ~ item2 + item3 model2 &lt;- item1 ~ item2 + item3 + item4 # Modellschätzung, fit1 + fit2 fit1 &lt;- lm(model1, df_exam) fit2 &lt;- lm(model2, df_exam) Beispiel Modellvergleich # Modellvergleich, fit1 + fit2 anova(fit1, fit2) Analysis of Variance Table Model 1: item1 ~ item2 + item3 Model 2: item1 ~ item2 + item3 + item4 Res.Df RSS Df Sum of Sq F Pr(&gt;F) 1 7 1.96429 2 6 0.80569 1 1.1586 8.6282 0.02604 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 7.4 Ergänzungen 7.4.1 Matrizen Kovarianzmatrizen werden mit cov2cor() in Korrelationsmatrizen umgewandelt: # Zuweisung Kova &lt;- cov(df_exam[ , 2:4]) # Berechnung cov2cor(Kova) item1 item2 item3 item1 1.0000000 -0.7698004 0.6123724 item2 -0.7698004 1.0000000 -0.4714045 item3 0.6123724 -0.4714045 1.0000000 # Alternativ cor(df_exam[ , 2:4]) item1 item2 item3 item1 1.0000000 -0.7698004 0.6123724 item2 -0.7698004 1.0000000 -0.4714045 item3 0.6123724 -0.4714045 1.0000000 7.4.2 Weitere Korrelationen Mit dem Paket psych können zudem Partialkorrelationen (partial.r()), tetrachorische (tetrachoric()), polychorische (polychoric()), biserale (biserial()) und polyseriale (polyserial()) Korrelationen berechnet werden Mit dem Paket ppcor können Semipartialkorrelationen berechnet werden 7.4.3 Standartisierte und unstandartisierte Regressionkoeffizienten Mit coefficients() oder $coefficients lassen sich die nicht-standardisierten Regressionskoeffizienten ausgeben Mit lm.beta() aus dem Paket QuantPsyc lassen sich standardisierte Regressionskoeffizienten ausgeben # Unstandartisierte Regressionskoeffizienten coefficients(fit1) (Intercept) item2 item3 2.8928571 -0.7142857 0.3928571 # Ausgabe fit1$coefficients (Intercept) item2 item3 2.8928571 -0.7142857 0.3928571 # Standartisierte Regressionskoeffizienten QuantPsyc::lm.beta(fit1) item2 item3 -0.6185896 0.3207665 7.4.4 Vorhergesagte Werte predict(), fitted.values() oder $fitted.values gibt die vorhergesagten Werte für jeden Datenpunkt als Vektor aus: # Vorhersage Werte, fit1 predict(fit1) 1 2 3 4 5 6 2.642857 3.750000 3.357143 2.250000 2.250000 3.357143 7 8 9 10 1.928571 3.750000 3.357143 3.357143 # Alternative 1 fitted.values(fit1) 1 2 3 4 5 6 2.642857 3.750000 3.357143 2.250000 2.250000 3.357143 7 8 9 10 1.928571 3.750000 3.357143 3.357143 # Alternative 2 fit1$fitted.values 1 2 3 4 5 6 2.642857 3.750000 3.357143 2.250000 2.250000 3.357143 7 8 9 10 1.928571 3.750000 3.357143 3.357143 7.4.5 Residuen resid(), residuals() oder $residuals gibt die unstandardisierten Residuen pro datapunkt aus rstandard() gibt die standardisierten Residuen aus # Residuen, fit1 fit1$residuals 1 2 3 4 5 0.35714286 0.25000000 -0.35714286 -0.25000000 -0.25000000 6 7 8 9 10 0.64285714 0.07142857 -0.75000000 -0.35714286 0.64285714 7.4.6 Modelldefinition Spezielle Modelle: Modell ohne Prädiktoren (Nullmodell): AV ~ 1 Modell ohne Intercept (Konstante): AV ~ 0 + UV1 + … Interaktionsterme (Moderation): AV ~ UV1 + UV2 + UV1*UV2 Statt “\\(*\\)” kann auch “\\(:\\)” verwendet werden 7.4.7 Hinweise zu weiteren Regressionsmodellen in R Literatur: Fox, J., &amp; Weisberg, S. (2011). An R companion to applied regression. London: SAGE. James, G., Witten, D., Hastie, T., &amp; Tibshirani, R. (2013). An introduction to statistical learning: With applications in R. Springer texts in statistics. New York: Springer. In R sind viele weitere Verfahren zur Modellselektion, -schätzung und -bewertung umgesetzt wie etwa: Bootstrapping Kreuzvalidierung Shrinkage (Lasse/Ridge) Regression Robust Regression Power Analyse 7.4.8 General Linear Model (GLM) glm(forumla, family, data) Generalisierte lineare Modelle als Verallgemeinerung des linearen Modells \\(\\rightarrow\\) Breite Einsatzmöglichkeiten formula: Wie bei lm() data: Wie bei lm() family: Definiert die Zufallsverteilung von Y und die Linkfunktion Beispiele: GLM mit Normalverteilung und ohne besondere Linkfunktion (familiy = gaussian(link = “identity”)) \\(\\rightarrow\\) Entspricht lm() GLM mit Binomialverteilung und logistischer Linkfunktion (family = binomial(link = “logit”)) \\(\\rightarrow\\) Binär-logistische Regression 7.4.9 Mediation und SEM Paket lavaan Ständig weiterentwickeltes Paket zur Schätzung von Pfad- und Strukturgleichungsmodellen in R Ermöglicht auch Bootstrapping-Konfidenzintervalle für indirekte Effekte Ausführliche Dokumentation mit Beispielcodes: http://lavaan.ugent.be/tutorial/est.html 7.4.10 Linear Mixed Model (LMM) Paket lme4 Liefert eine breite und gut erprobte Palette an Werkzeugen für linear mixed models (Hierarchische lineare Modelle; Mehrebenenmodelle) Siehe: Bates, D., Mächler, M., Bolker, B., &amp; Walker, S. (2015). Fitting Linear Mixed-Effects Models Using lme4. Journal of Statistical Software, 67(1). Paket lmerTest Reicht Signifikanztests nach, die in lme4 standardmäßig nicht implementiert sind https://cran.r-project.org/web/packages/lmerTest/index.html "],["grafiken.html", "8 Grafiken 8.1 Grafiken für univariate Verteilungen 8.2 Grafiken für bivariate Verteilungen 8.3 Tabellen", " 8 Grafiken 8.1 Grafiken für univariate Verteilungen 8.1.1 Allgemeines R bietet flexible Gestaltungsmöglichkeiten für eine Reihe verschiedener Graphiken Komplexere Grafiken können mit dem Paket ggplot2 erstellt werden Generell gilt: Auf „Nutzerfreundlichkeit“ und Qualität der Grafiken achten (z.B. Legenden) Fast alle Grafiken können mit folgenden Argumenten näher definiert werden: main = “…”: Überschrift col = “…”: Farbe(n); Mehrere Farben mit c(“…”, “…”, …) eingeben xlab = “…” und ylab = “…”: Titel der x- bzw. y-Achse xlim = c(…, …) und ylim = c(…, …): Achsenlimits … 8.1.1.1 Farben Farben können als: Farbname (“red”), siehe die Datei Farben in R (im moodle-Kurs) HEX-Code (“#FF8000FF”), oder RGB-Code (“229, 245, 249”) eingetragen werden 8.1.2 Darstellung von Häufigkeiten Kreisdiagramme mit pie() erstellen Datenformat: Variable in Häufigkeitstabelle (table()) Balkendiagramme mit barplot() erstellen Datenformat: Variable in Häufigkeitstabelle (table()) tab &lt;- table(Data$gruppe) barplot(tab) 8.1.3 Darstellung von Häufigkeiten Mit einigen zusätzlichen Optionen: barplot(tab, main = &quot;Absolute Häufigkeiten der Gruppen&quot;, col = c(&quot;coral2&quot;, &quot;darkcyan&quot;, &quot;goldenrod3&quot;), names.arg = c(&quot;Kontrollgruppe&quot;, &quot;Pilates&quot;, &quot;Yoga&quot;), ylab = &quot;Absolute Häufigkeit&quot;, ylim = c(0, 50)) 8.1.4 Darstellung von Verteilung hist(x, breaks, freq, …) zeigt die Verteilung einer Variablen in einem Histogramm x: Variable, für die das Histrogramm erzeugt werden soll (Wird mit dem $-Operator ausgewählt) breaks: Wie viele Balken sollen gebildet werden? Keine Angabe: Automatische Auswahl Einzelne Zahl n: Daten werden in n Balken geteilt Vektor: Balken werden zwischen den angegebenen Stellen gebildet z.B. breaks = c(0,1,2,3) → 3 Balken: 0 bis 1, 1 bis 2 und 2 bis 3 z.B. breaks = seq(0,100,20) → erstellt Werte im Schritt von 20 im Bereich zwischen 0 und 100 (5 Balken) freq: Soll statt der absoluten die relative Häufigkeit geplottet werden? (TRUE = ja) hist(Data$alter) Mit einigen zusätzlichen Optionen: hist(Data$alter, main = &quot;Histogramm von Alter&quot;, col = &quot;darkolivegreen3&quot;, xlab = &quot;Alter&quot;, ylab = &quot;Häufigkeit&quot;, ylim = c(0, 20)) boxplot(x, range =, …) erstellt einen bzw. mehrere Boxplots x: kann Vektor, Dataframe oder Formel sein Bei Eingabe eines Dataframes werden Boxplots für alle enthaltenen Variablen erzeugt Formeln ermöglichen Trennung nach Gruppen (s.u.) range: Definiert die maximale Länge des Whiskers Werte außerhalb werden als Ausreißer dargestellt z.B.: range = 2 (2 * Interquatilabstand bzw. Boxlänge) default: range = 1.5 boxplot(Data$zufri, col = &quot;darksalmon&quot;, main = &quot;Boxplot der Zufriedenheitswerte&quot;, ylab = &quot;Zufriedenheitswert&quot;, ylim = c(0, 6)) 8.2 Grafiken für bivariate Verteilungen 8.2.1 Verteilung einer metrischen auf eine kategoriale Variable Mithilfe von Boxplots lässt sich auch die Verteilung einer kategorialen Variable auf eine metrische Variable boxplot(A ~ B + C + …) erzeugt Boxplots für alle möglichen Kombinationen von Faktorstufen getrennt A: metrische abhängige Variable B,C: kategoriale Variablen nach deren Ausprägungen aufgeschlüsselt wird boxplot(Data$angst ~ Data$gruppe, col = &quot;deeppink&quot;, main = &quot;Boxplots der Angstwerte nach Gruppen getrennt&quot;, ylab = &quot;Angstwerte&quot;, ylim = c(0, 6), names = c(&quot;Kontrollgruppe&quot;, &quot;Pilates&quot;, &quot;Yoga&quot;)) 8.2.2 Verteilung zweier metrischer Variablen Für zwei metrische Variablen wird meistens die Darstellungsform des Streudiagramms gewählt, was mit dem Befehl plot(x,y,…) erstellt werden kann x: Erste Variable () y: Zweite Variable …: Weitere Graphikparameter (z.B. pch: Punkttyp \\(\\rightarrow\\) ?pch) plot(Dataset$item1, Dataset$item2, ylab = &quot;item1&quot;, xlab = &quot;item2&quot;, main = &quot;Streudiagramm von Item 1 und Item 2&quot;, xlim = c(0, 6), ylim = c(0, 6), pch = 18, col = &quot;firebrick4&quot;) Der Befehl abline() wird (separat) im Anschluss von plot() ausgeführt Mit abline(h = …, v = … , lwd = …, lty = …) können horizontale (h) bzw. vertikale (v) Linien an beliebiger Position eingefügt werden h/v:Wert durch welchen die Linie verlaufen soll lwd: legt die Stärke (Dicke) der Geraden fest (default = 1) lty: legt den Linientyp fest \\(\\rightarrow\\) ?par plot(Dataset$item1, Dataset$item2, ylab = &quot;Item 2&quot;, xlab = &quot;Item 1&quot;, main = &quot;Streudiagramm von Item 1 und Item 2&quot;, xlim = c(0, 6), ylim = c(0, 6), pch = 18, col = &quot;firebrick4&quot;) abline(h = mean(Dataset$item2), v = mean(Dataset$item1), col = &quot;black&quot;, lty = 2) Mit abline(fit) wird die Regressionsgerade in das bestehende Streudiagramm eingezeichnet fit entspricht dem definierten Modell, z.B. fit = lm(item2 ~ item1) model4 &lt;- item2 ~ item1 fit4 &lt;- lm(model4, Dataset) plot(Dataset$item1, Dataset$item2, ylab = &quot;Item 2&quot;, xlab = &quot;Item 1&quot;, main = &quot;Streudiagramm von Item 1 und Item 2&quot;, sub = &quot;Item 2 = 3.5000 - 0.6667 * Item 1&quot;, xlim = c(0, 6), ylim = c(0, 6), pch = 18, col = &quot;firebrick4&quot;) abline(fit4, lwd = 2, col = &quot;darkslategrey&quot;, lty = 2) 8.3 Tabellen 8.3.1 Mehrfelder Tabelle kontroll pilates yoga m 14 12 6 w 31 34 23 Korrelations Matrizen item1 item2 item3 item1 1.0000000 -0.7698004 0.6123724 item2 -0.7698004 1.0000000 -0.4714045 item3 0.6123724 -0.4714045 1.0000000 8.3.2 Formatierung Das Package stargazer bietet den Befehl stagazer(…,type = “html”, title = “title”, out = “…”,) …: einen Vektor, Matrix, Datensatz oder Modell type: Welches Dateiformat soll der Befehl ausgeben. Beispielsweise “html”, “latex” oder “text” title: Überschrift der Tabelle out: Name der Datei. Abhängig von type entweder mit .html, .tex oder .txt am Ende. Wenn davor kein Dateipfad angegeben wird, wird die Datei im Projektordner gespeichert. …noch viele weitere Möglichkeiten für Anpassungen. Siehe ?stargazer() Anwendung an Beispielen Korrelationstabelle library(psych) Dataset &lt;- read.table(&quot;data/Daten.txt&quot;, header = TRUE) Korrelationstabelle&lt;- cor(Dataset[,2:4], method = &quot;pearson&quot;) Korrelationstabelle item1 item2 item3 item1 1.0000000 -0.7698004 0.6123724 item2 -0.7698004 1.0000000 -0.4714045 item3 0.6123724 -0.4714045 1.0000000 library(stargazer) stargazer(Korrelationstabelle, type = &quot;html&quot;, title = &quot;Korellationstabelle&quot;, out = &quot;Korellationstabelle.html&quot;) Aussehen in der html Datei: Für Übertragung in Word einfach in der .html Dateien markieren, kopieren und in Word einfügen Regressionsmodelle: fit1 &lt;- lm(data = Dataset, formula = item1 ~ item2 + item3) summary(fit1) Call: lm(formula = item1 ~ item2 + item3, data = Dataset) Residuals: Min 1Q Median 3Q Max -0.75000 -0.33036 -0.08929 0.33036 0.64286 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 2.8929 1.1752 2.462 0.0434 * item2 -0.7143 0.2832 -2.523 0.0397 * item3 0.3929 0.3003 1.308 0.2322 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.5297 on 7 degrees of freedom Multiple R-squared: 0.6726, Adjusted R-squared: 0.5791 F-statistic: 7.191 on 2 and 7 DF, p-value: 0.02008 8.3.3 Tabellen und Modelle library(stargazer) stargazer(fit1, type = &quot;html&quot;, title = &quot;Beispiel Regression&quot;, out = &quot;Regression.html&quot;) library(stargazer) stargazer(fit1, type = &quot;html&quot;, title = &quot;Beispiel Regression&quot;, out = &quot;Regression.html&quot;, ci = TRUE, covariate.labels = c(&quot;sozioökonomischer status&quot;, &quot;Alter&quot;,&quot;Konstante&quot;), dep.var.labels = &quot;Einkommen&quot;, dep.var.caption = &quot;Abhängige Variable&quot;) ci: wenn TRUE ersetzt die angegebenen Standardfehler mit dem Konfifenzintervall für das Konfidenzniveau von 95 Prozent covariate.labels: ein Character-Vektor, der die Namen der unabhängigen Variablen ersetzt dep.var.labels: Name der abhängigen Variable dep.var.caption: Überschrift über die Abhängige Variable Resultat "],["beispielanalyse.html", "9 Beispielanalyse 9.1 Forschungsfrage 9.2 Datensatz 9.3 Variablen 9.4 Analyse 9.5 Einzelne Schritte", " 9 Beispielanalyse 9.1 Forschungsfrage Sind Menschen glücklicher, die sich gesund verhalten? 9.2 Datensatz ESS7 - integrated file, edition 2.2: - European Social Survey (2014) 9.3 Variablen AV (Abhängige Variablen) happiness: happy (num; 0-10) 0 = extremely unhappy, 10 = extremely happy “Taking all things together, how happy would you say you are?” UV (Unabhängige Variablen) Eat Fruit: etfruit (cat) Eat Veg: eatveg (cat) Sport: dosprt (num) cigarettes: cgtsday (num) how often alkohol: alcfreq (cat) Kontrollvariablen gender: gndr (cat) age: agea (num) income: hinctnta (cat) country: cntry (chr) 9.4 Analyse Vorgehen - Daten beschaffen - Daten einlesen - Daten säubern - Index bilden - Daten sichten - deskriptive Statistiken - Regressionsmodelle Daten beschaffen und einlesen Die Daten sind für alle über die Webseite der ESS zugänglich. Um umständliches Anmelden zu vermeiden könnt ihr sie über den Moodle-Kurs runterladen. Die genaue Beschreibung der Variablen könnt ihr ebenfalls über die Webseite einsehen. Datensatz säubern Für unsere Analyse möchten wir nur Variablen im Datensatz behalten, die wir tatsächlich für unsere Analyse brauchen (= AV, UV und KV), und alle anderen verwerfen. Diese Variablen haben teilweise Ausprägungen, die für unsere Analyse nicht zielführend sind. Fälle mit diesen Ausprägungen möchten wir ebenfalls ausschließen. (Antworten: “Refusal”,“Don’t Know”,“No answer”; bei cgtsday möchten wir “not applicable” beibehalten) Indexbildung Nach dem einlesen der Daten bauen wir einen Index, welcher sich aus negativen Werten für gesundheitsschädigendes Verhalten und positiven Werten für gesundheitsförderndes Verhalten aufwiegt. Gesundheitsschädigendes Verhalten ist dabei doppelt so stark gewichtet wie förderndes. Die Einstufung der einzelnen Variablen liegt dabei kein validiertes Wissen zugrunde, was der Demonstration im Weiteren aber nicht schaden sollte. Deskriptive Statistik Zunächst möchten wir einen Überblick über die Daten erhalten und lassen uns die Verteilungen für Zufriedenheit und unseren Index ausgeben. Zudem wollen wir mittels eines t-Tests überprüfen, ob geschlechtsspezifische Unterschiede bezüglich der Zufriedenheit statistisch signifikant sind. Korrelation Zu guter Letzt möchten wir die Korrelation zwischen Zufriedenheit und dem Index berechnen, um im Anschluss zwei lineare Regressionsmodelle zu definieren und schätzen zu lassen. Eines davon mit und eines davon ohne Kontrollvariablen. 9.5 Einzelne Schritte Daten einlesen Lese den Datensatz ein. Datensatz säubern 1. Erstelle einen neuen Datensatz mit den für die Analyse relevante Variablen (AV, UV und KV). 2. Lösche alle Fälle die NA-Werte enthalten. (Hinweis: complete.cases() prüft ob NA vorhanden sind) 3. Lösche alle Fälle die keine Informationen enthalten (“Refusal”,“Don’t Know”,“No answer”). Indexbildung Erstelle eine neue Variable (health_index) nach den folgenden Kriterien: - Isst eine Person weniger als 4 Mal, aber mindestens ein Mal die Woche Obst oder seltener werten wir das als ungesund (-1). Isst sie häufiger Obst werten wir das als gesund (0.5). - Isst eine Person weniger als 4 Mal, aber mindestens ein Mal die Woche Gemüse oder seltener werten wir das als ungesund (-1). Isst sie häufiger Gemüse werten wir das als gesund (0.5). - Macht eine Person weniger als 3 Mal die Woche Sport werten wir das als ungesund (-1). Macht sie häufiger Sport als gesund (0.5). - Raucht eine Person auch nur eine Zigarette am Tag werten wir das als ungesund (-1). Raucht sie gar nicht (“not applicable”) oder 0 Zigaretten am Tag werten wir das als einflusslos (0). - Trinkt eine Person ein Mal die Woche oder häufiger Alkohol werten wir das als ungesund (-1), trinkt sie seltener als einflusslos (0). Daten sichten Wie ist der Index (health_index) verteilt? Erstelle einen Plot um dir die Verteilung anzuzeigen. Wie ist die Zufriedenheit (happy) verteilt? Erstelle einen Plot. Wie ist die durchschnittliche Zufriedenheit nach Geschlecht verteilt? Ist der Unterschied in der Zufriedenheit nach Geschlecht statistisch signifikant? (Hinweis: Wie funktioniert die Funktion t.test()?) Korrelation Erstelle einen Boxplot mit den Variablen happy und health_index um einen ersten Überblick über deren Korrelation zu bekommen. Berechne den Korrelationskoeffizienten nach Pearson. Regression 1. Berechne folgende Regressionsmodelle und lass dir jeweils die Konfidenzintervalle der Regressionskoeffizienten ausgeben:  1.1 Univariat: Modell 1 = happy ~ health_index  1.2 Multivariat: Modell 2 = happy ~ health_index + gndr + age + inc      (Achtung: gndr: gndr faktorisiert (factor()), age: agea standardisiert      (scale()), inc: hinctnta gruppiert (ggplot2::cut_number()) 2. Erstelle eine .html-Datei mit einer gemeinsamen Regressionstabelle für Modell 1 und Modell 2. "],["weiterführende-inhalte.html", "10 Weiterführende Inhalte 10.1 R Markdown", " 10 Weiterführende Inhalte 10.1 R Markdown In Markdown können wir alles was wir bisher gelernt haben und noch viel mehr (z.B. auch andere Programmiersprachen) in einem Textdokument, einer Präsentation oder Sonstigem ausgeben lassen. Insbesondere wenn wir Code, Tabellen und Grafiken in unserer Arbeit darstellen möchten, bietet es sich an R Markdown zu verwenden, da hier die Formatierung um ein Vielfaches einfacher ist und besser aussieht, als in herkömmlichen Texteditoren, wie z.B. Microsoft-Word oder LibreOffice. Hier kommt ihr zur RStudio-Seite von Markdown: (link)https://rmarkdown.rstudio.com/ Cheatsheet-PDF: (link)https://github.com/rstudio/cheatsheets/raw/main/rmarkdown-2.0.pdf 10.1.1 Getting Started Um R Markdown in eurer RStudio Umgebung freizuschalten müsst ihr es, falls noch nicht geschehen, zunächst installieren und laden: ## R Markdown installieren install.packages(&quot;rmarkdown&quot;, dependencies = TRUE) ## R Markdown laden library(rmarkdown) "],["statistik-grundlagen.html", "11 Statistik Grundlagen 11.1 Skalenniveaus 11.2 Häufigkeiten und Verteilungen 11.3 Deskriptive Statistik 11.4 Induktive Statistik 7 11.5 Hypothesentests 11.6 lineare Regression", " 11 Statistik Grundlagen Unser R-Intensivkurs behandelt einige Themenbereiche der Statistik. Damit ihr sie euch nochmal ins Gedächtnis rufen könnt, gibt es hier eine Übersicht für euch. 11.1 Skalenniveaus Es gibt verschiedene Skalenniveaus, die uns Auskunft darüber geben welchen Informationsgehalt unsere Variablen haben. Anhand der Skalenniveaus können wir entscheiden welche statistischen Operationen mit einer Variable möglich sind.1 Skala Subskala Eigenschaften Beispiele Nominal - Klassifikation, Kategorie Geschlecht, Automarke, Stadt Ordinal - Rangordnung Schulnote, Bildungsstand Metrisch Intervall Rangordnung und Abstände Temperatur in Celsisus, IQ Metrisch Ratio Rangordnung, Abstände, und natürlicher Nullpunkt Gehalt, Gewicht Metrisch Absolut Rangordnung, Abstände, natürlicher Nullpunkt, und natürliche Einheit Anzahl Fachsemester 11.2 Häufigkeiten und Verteilungen Absolute und Relative Häufigkeiten Die absolute Häufigkeit ist die Anzahl (= ganze Zahl) wie oft ein Merkmal in einer Stichprobe vorkommt. Die relative Häufigkeit hingegen ist der Anteil der Fälle, in denen ein Merkmal in einer Stichprobe auftritt. Verteilungen Verteilungen betrachten wir in der deskriptiven Statistik in Form von Häufigkeitsverteilungen. Die Schiefe und die Wölbung geben uns Auskunft darüber inwiefern die Verteilung von Variablen von der Normalverteilung abweichen. 2 Schiefe Die Schiefe (skewness) trifft Aussagen zur Verteilungskurve einer Variable. Interpretation der Schiefewerte: Wert Bedeutung Interpretation \\(&lt;\\) 0 linksschief Die Verteilung ist nach links gestreckt, und der Großteil der Ausprägungen konzentriert sich auf der rechten Seite \\(&gt;\\) 0 rechtsschief Die Verteilung ist nach rechts gestreckt, und der Großteil der Ausprägungen konzentriert sich auf der linken Seite 0 symmetrisch Die Daten sind symmetrisch und gleichmäßig um den Mittelwert verteilt Wölbung Die Wölbung (kurtosis) misst wie spitz bzw. flach die Verteilung einer Variable ist. Interpretation der Kurtosis-Werte: Wert Bedeutung Interpretation \\(&lt;\\) 3 flachgifplig Die Verteilung ist gestaucht und wirken deshalb flacher \\(&gt;\\) 3 spitzgipflig Die Verteilung hat eine ausgeprägerte Spitze im Vergleich zu einer Normalverteilung 3 normalgipflig Die Verteilung folgt einer Normalverteilung 11.3 Deskriptive Statistik Die deskriptive Statistik ist ein grundlegendes Werkzeug für die empirische Forschung. Wenn wir mit Datensätzen arbeiten, kann es schwierig sein, Muster, Trends oder wichtige Merkmale nur anhand der Rohdaten zu erkennen. Deskriptive Statistiken helfen uns die Daten auf leicht verständliche Weise zu bündeln und zu verstehen.3 Lageparameter Lageparameter geben die zentrale Tendenz einer Variable an. Beispiele sind arithmetische Mittel, Median oder Modus.4 Lageparameter Beschreibung Skalenniveau Arithmetisches Mittel Summe der Werte einer Variable, geteilt durch die Anzahl an Werten Metrisch Median Wert der genau in der Mitte liegt, wenn die Werte einer Variable aufsteigend sortiert sind Metrisch, Ordinal Quartile 1.Quartile = Wert unter den 25% der Daten Fallen; 2. Quartile = Median; 3.Quartile = Wert unter den 75% der Daten fallen Metrisch, Ordinal Modus Wert(e) einer Variable mit häufigstem Auftreten Metrisch, Ordinal, Nominal Streuungsmaße Streuungsmaße geben an wie sehr unsere Daten streuen. Die Streuung kann dabei auf ein Lageparameter bezogen sein, z.B. bei der Varianz oder Standardabweichung, oder auch auf die Breite der Streuung aller Daten, z.B Spannweite oder Interquartilabstand.5 Streungsmaße Beschreibung Lageparameter Maximum Maximum ist der größte Wert einer Variable - Minimum Minimum ist der kleinste Werte einer Variable - Spannweit Die Spannweite ist die Differenz zwischen dem Maximum und dem Minimum - Varianz Quadrierte Abweichung vom arithmetischen Mittel (schwer interpretierbar) arithmetisches Mittel Standardabweichung Wurzel aus der Varianz (leichter interpretierbar) arithmetisches Mittel Interquantilabstand Differenz aus 3. und 1. Quartile Median Zusammenhangsmaße Mit Zusammenhangsmaßen können wir untersuchen wie stark zwei Variablen sich gegenseitig beieinflussen (zusammenhägen). Je nach Skalenniveau der betrachteten Variablen wählen wir unterschiedliche Verfahren.6 Skalennivau (Dichotom) Nominal Ordinal Metrisch (Dichotom) Phi-Koeffizient Cramers V, Kontingenzkoeffizienten Biserialer Rangkorrelationskoeffizient Punktbiseriale Korrel. tetrachorischr o. biseriale Korrelation Nominal Cramers V, Kontingenzkoeffizienten Cramers V, Kontingenzkoeffizienten – Eta Ordinal Kontingenzkoeffizient, Cramers V Kontingenzkoeffizient, Cramers V Spearman-Rangkorrelation, Kendalls Tau, Gamma – Metrisch Kontingenzkoeffizient, Cramers V Kontingenzkoeffizient, Cramers V Spearman-Rangkorrelation, Kendalls Tau, Gamma Korrelationskoeffizient n. Pearson, Produkt-Moment-Korrelation 11.4 Induktive Statistik 7 11.4.1 Grundlagen der induktiven Statistik Induktive Statistik, wofür? Das Problem in der Statistik ist oft, dass Aussagen über Grundgesamtheiten, z.B (Gesamte Bevölkerung eines Landes, Alle Mitglieder einer Firma) getroffen werden sollen. Allerdings ist das aus Kosten- und Aufwandgründen nicht realisierbar. Daher werden Stichproben aus der Gesamtheit gezogen. Die Frage ist jetzt, ob sich die Merkmale die in der Stichprobe erhoben auch auf die Grundgesamtheit übertragen lassen. Daher auch der Name “induktiv”, da man hier vom Speziellen auf das Allgemeine schließt anders als bei der Deduktion bei der vom Allgemeinen auf das Spezielle geschlossen wird. Das Problem ist, dass Aussagen, welche aus induktiven Schlüsse herrühren, nicht immer zu 100 Prozent mit der Wirklichkeit übereinstimmen. Es gibt immer eine Restunsicherheit. Und um diese zu Beschreiben gibt es die Induktive Statistik Wichtige Begriffe Grundgesamtheit: Die zu untersuchende Zielgruppe, z.B. alle Studenten der Uni Leipzig Stichprobe: Eine meist zufällig gezogene Teilmenge aus der Grundgesamtheit z.B. alle Erstsemestler statistischer Kennwert: Wert der ein Merkmal in irgendeiner Form zusammenfasst Beispiele: Mittelwert Varianz Standardabweichung (Siehe dazu Teil deskriptive Statistik) 11.5 Hypothesentests Um mit unseren Daten allgemein gültige Aussagen über die Grungesamtheit treffen zu können brauchen wir Hypothesentests. Solche Hypothesentests können verschiedene Forschungsfragen zugrunde liegen, z.B. Unterscheiden sich Populationsmittel eines Merkmals zwischen zwei Populatione? Sind die Ausprägungen eines Merkmals auf eine bestimmte Weise verteilt? Sind zwei Merkmale voneinander unabhängig? Die allgemein am meisten genutze Form des statistischen Hypothesentests ist der Signifikanztest. Diese sind immer gleich aufgebaut. Zuerst gibt es die Alternativhypothese \\(H_a\\). Das ist diejenige, von der wir hoffen, sie mit den Daten nachweisen zu können. Ihr gegenüber steht die Null-Hypothese \\(H_0\\), von der wir hoffen, dass sie sich anhand der Daten als statistisch unplausibel herausstellt und wir sie ablehnen können. Die zu untersuchende Hypothese bestimmt dabei die Auswahl des statistischen Testverfahrens. Nachfolgend werden anhand von Beispielen verschiedene Testverfahren erläutert. 11.5.1 t-Tests T-Tests sind die am häufigsten verwendeten statistischen Tests, wenn es darum geht mit Stichproben zu arbeiten. Sie bieten sich gut an um metrische Variablen zu vergleichen. 11.5.1.1 Einstichproben t-Test Beispiel: In einer Umfrage wurden 200 zufällig ausgewählte Studenten aus der UNI Leipzig zu ihrer persönlichen Zufriedenheit befragt und aus den gegebenen Antworten wurden pro Person (zeilenweise) ein Summenscore gebildet. Der Mittelwert des Summenscores der Stichprobe beträgt \\(\\bar x = 208\\) mit einer Standardabweichung von \\(s = 30\\). Es ist bekannt, dass der durchschnittliche Wert für die Gesammtpopulation aller Studenten bei \\(\\mu = 150\\) liegt, die Frage ist jetzt, weicht unserer Stichprobenmittel signifikant vom Populationsmittel ab? Dazu formulieren wir folgende Hypothese: Studenten der Uni Leipzig weichen signifikant von Studenten anderer Universitäten in ihren Zufriedenheitswerten ab. Daraus resultieren zwei statistische Hypothesen, welche die zwei Ausgänge eines Hypothesentests sind. Zum einen die Nullhypothese: \\(H_0: \\mu = \\mu_0\\) Übersetzt bedeutet das, wir vermuten nach dieser Hypothese, dass unser Stichprobenmittelwert \\(\\mu (= \\bar x)\\) mit dem Mittelwert der Population \\(\\mu_0\\) übereinstimmt. Das klingt jetzt erstmal verwirrend, da das ja unserer Forschungsfrage entgegengesetzt ist. Aber genau das ist, was die Nullhypothese machen soll, da sie ja am Ende verworfen werden soll. Die Alternativhypothese dazu ist entsprechend: \\(H_a: \\mu \\neq \\mu_0\\) Für jedes ausgewählte Testverfahren müssen bestimmte Voraussetzungen geprüft werden. Bevor ein Einstichproben t-Test durchgeführt werden kann, muss die abhängige Variable also die Zufriedenheitswerte auf Normalverteilung geprüft werden. Dies kann durch einen Shapiro-Test geklärt werden. Dafür wird der Vektor mit den Zufriedenheitswerten der Studenten verwendet. Die statistischen Hypothesen für diesen Test lauten: \\(H_0:\\) Daten sind normalverteilt \\(H_a:\\) Daten sind nicht normalverteilt Hier ist also das Ziel ein nicht signifikantes Testergebnis zu erhalten set.seed(6) stichprobe &lt;- rnorm(200, 208, 30) #erstellung einer zufälligen Verteilung von 200 Werten mit dem Mittelwert 208 und einer standardabweichung von 30 shapiro.test(stichprobe) Shapiro-Wilk normality test data: stichprobe W = 0.99478, p-value = 0.7174 Hier gilt es auf den berechneten p-Wert zu achten. Ist dieser über 0.05 können wir annehmen, dass die zugrunde liegenden zufriedenheitswerte normalverteilt sind. Heißt also der t-Test kann angewendet werden. Dafür gibt es in R den Befehl \\(\\texttt{t.test()}\\) t.test(stichprobe, mu = 150) #mit dem mu-Argument wird der wahre Wert des Mittelwerts festgelegt One Sample t-test data: stichprobe t = 27.333, df = 199, p-value &lt; 2.2e-16 alternative hypothesis: true mean is not equal to 150 95 percent confidence interval: 202.4357 210.5902 sample estimates: mean of x 206.513 auch hier ist wieder der p-Wert wichtig. Liegt dieser unter 0.05 können wir zu einem Signifikanzniveau von 5 % feststellen, dass der Mittelwert der Stichprobe tatsächlich vom wahren Mittelwert 150 abweicht. Da der p-Wert hier auch noch deutlich geringer ausfällt (\\(2.2e-16 = 2.2 * 10^-16\\)), können wir dies sogar für ein noch niedrigeres Signifikanzniveau feststellen. Für die p Werte gilt im Allgemeinen Symbol p-Wert Bedeutung * &lt; 0.05 signifikant ** &lt; 0.01 sehr signifikant *** &lt; 0.001 hoch signifikant 11.5.1.2 Zweistichproben t-Test Beispiel Zusätzlich zu der Stichprobe an der Uni Leipzig, wurde noch eine weitere Erhebung mit 200 Personen an der Uni Halle durchgeführt. Der Durchschnitt lag hier bei 180 und die Standardabweichung bei 55. Es soll jetzt festgestellt werden, ob die´zwei Stichproben sich signifikant voneinander unterscheiden. Dazu wird ein Zweistichproben t-Test verwendet. Dazu müssen allerdings auch die Voraussetzungen erfüllt sein, dass beide Stichproben normalverteilt sind… shapiro.test(stichprobe2) Shapiro-Wilk normality test data: stichprobe2 W = 0.99666, p-value = 0.9456 …sowie dass beide Stichproben die gleiche Varianz haben. Dies kann mithilfe des \\(\\texttt{var.test()}\\)-Befehls überprüft werden. var.test(stichprobe,stichprobe2) F test to compare two variances data: stichprobe and stichprobe2 F = 0.29927, num df = 199, denom df = 199, p-value &lt; 2.2e-16 alternative hypothesis: true ratio of variances is not equal to 1 95 percent confidence interval: 0.2264854 0.3954516 sample estimates: ratio of variances 0.2992725 Bei diesem Test ist die Nullhypothese, dass die Varianzen gleich sind. Also \\(H_0: \\sigma^2_1 = \\sigma^2_2\\) \\(H_a: \\sigma^2_1 \\neq \\sigma^2_2\\) Auch hier ist ein nicht signifikantes Ergebnis erwünscht. Da also die Bedingung nicht erfüllt ist, muss bei der Berechnung des t-Tests noch eine Einstellung geändert werden. Der Befehl bleibt aber gleich, nur, dass jetzt beide Stichproben in den Befehl eingefügt werden. t.test(stichprobe,stichprobe2, var.equal = FALSE)#var.equal auf FALSE zu setzen, sorgt dafür, dass eine abgewandelte Form des t-Tests angewandt wird, der die unterschiedlichen Varianzen einkalkuliert Welch Two Sample t-test data: stichprobe and stichprobe2 t = 7.3541, df = 308.32, p-value = 1.746e-12 alternative hypothesis: true difference in means is not equal to 0 95 percent confidence interval: 23.20512 40.15904 sample estimates: mean of x mean of y 206.5130 174.8309 Aufgrund des sehr niedrigen p-Werts zeigt sich hier, dass die zwei Stichproben sich hochsignifikant voneinander unterscheiden. Zusätzlich gibt es auch noch die Möglichkeit einen t-Test mit gepaarten Stichproben durchzuführen. Das wäre z.B. der Fall, wenn die Umfrage an der Uni Leipzig nochmal mit dem gleichen Personenkreis zu einem späteren Zeitpunkt durchgeführt würde. Dazu muss in der \\(\\texttt{t.test()}\\)-Funktion das Argument \\(\\texttt{paired}\\) auf \\(\\texttt{TRUE}\\) gesetzt werden. 11.5.2 Chi-Quadrat Test Wenn es darum geht ob eine kategoriale Variable einen Einfluss auf eine andere kategoriale Variable hat, bietet es sich an, das ganze mit einem Chi-Quadrat Test zu überprüfen. Dieser Test kann für folgende Fragestellungen verwendet werden: (1) folgen die Häufigkeiten einer bestimmten Variablen einer bekannten Verteilung? (2) beeinflussen sich zwei kategoriale Variablen gegenseitig? Beispiel: In einer Umfrage wurden Personen befragt,ob sie sich in ihrem leben glücklich fühlen und zum zweiten, ob sie eine feste Freundesgruppe haben. Es wird vermutet, dass es einen Zusammenhang zwischen diesen beiden Variablen gibt. Die Ergebnisse sehen so aus:   glücklich nicht glücklich   keine feste Freundesgruppe 13 % 4.5 % 17 % feste Freundesgruppe 61.5 % 21 % 82.5 %   74.5 % 25.5 % 100 % mit diesen Datens kann dann die Hypothese mithilfe des \\(\\texttt{chisq.test()}\\) überprüft werden chisq.test(stichprobe3$glck,stichprobe3$frnd) #&gt; #&gt; Pearson&#39;s Chi-squared test with Yates&#39; continuity #&gt; correction #&gt; #&gt; data: stichprobe3$glck and stichprobe3$frnd #&gt; X-squared = 0.070178, df = 1, p-value = 0.7911 Die Nullhypothese ist bei diesem Test, dass die zwei Variablen unabhängig sind bzw. dass sie sich nicht gegenseitig beeinflussen. Die Alternavhypothese besagt das Gegenteil, also dass sie sich gegenseitig beeinflussen. Da der p-Wert hier deutlich über 0,05 liegt können wir die Nullhypothese nicht ablehnen und müssen davon ausgehen, dass in diesem Fall das Vorhandensein von Freunden keinen Einfluss auf die Glücklichkeit hat. 11.6 lineare Regression Hinweis: Für lineare Regressionen sind eine vielzahl an Voraussetzungen zu überprüfen: (1)Es existiert ein linearer Zusammenhang (2)Erwartungswert der Residuen ist Null (3)Varianzhomogenität der Residuen (Homoskedastizität) (4)Die Residuen sind nicht miteinander Korreliert (Autoregression) (5)Die Residuen sind nicht mit den unabhängigen Variablen korreliert und die unabhängige Variable ist deterministisch(z.B. Omitted-Variable-Bias) (6)Normalverteilung der Residuen 11.6.1 einfache lineare Regression Mithilfe der linearen Regression ist es möglich, einen gerichteten Einfluss einer Variablen X auf eine Variable Y nachzuweisen. Es wird dabei ein linearer Zusammenhang angenommen. Die Grundlegende Form dieses Zusammenhanges ist wie folgt: \\(Y=\\alpha+\\beta \\cdot X +\\varepsilon\\) Die Koeffizienten beschreiben dabei: \\(\\alpha\\) ist der y-Achsenabschnitt, der uns sagt welchen Wert Y hätte, wenn X null wäre \\(\\beta\\) ist der Anstieg der linearen Regression. Wenn also der X-Wert um eins ansteigt, steigt die Y-Variable um den Wert von \\(\\beta\\) \\(\\varepsilon\\) ist der Fehlerterm. Da die wenigsten Zusammenhänge im echten Leben perfekt linear sind, zeigen sich in dem Fehlerterm zufällige Einflüsse. Am besten lässt sich ein solcher Zusammenhang vorstellen wenn man das ganze visuell betrachtet Was der Algorithmus hinter der lineare Regression macht, ist eine lineare Funktion für eine Gerade zu finden, die einen möglichst geringen Abstand zu den einzelnen Punkten hat. Etwa so: Eine lineare Regression wird in R mit dem Befehl \\(\\texttt{lm()}\\) erstellt wobei das erste Argument eine \\(\\texttt{formula}\\) mit der Struktur \\(\\texttt{AbhängigeVariable} \\sim \\texttt{UnabhängigeVariable}\\) das erste Argument ist. Hier wird jetzt für den Allbus2018 (allgemeine Bevölkerungsumfrage der Sozialwissenschaften) untersucht, ob das Alter (in Jahre) einen Einfluss auf die allgemeine Zufriedenheit (10 stufige Skala) hat. Allbus &lt;- readRDS(&quot;data/Allbus2018.rds&quot;) output &lt;- lm(Zufriedenheit~Alter,data = Allbus) summary(output) #&gt; #&gt; Call: #&gt; lm(formula = Zufriedenheit ~ Alter, data = Allbus) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -7.8923 -0.8804 0.1164 1.1124 2.1403 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 7.9218361 0.1471104 53.850 &lt;2e-16 *** #&gt; Alter -0.0007972 0.0031956 -0.249 0.803 #&gt; --- #&gt; Signif. codes: #&gt; 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 1.532 on 1574 degrees of freedom #&gt; Multiple R-squared: 3.954e-05, Adjusted R-squared: -0.0005958 #&gt; F-statistic: 0.06224 on 1 and 1574 DF, p-value: 0.803 Dem Output können drei wichtige Ergebnisse entnommen werden: (1) Der F-Test besagt, ob das gesammte Modell signifikant ist (wenn p-Wert &gt;0.05, dann nicht signifikannt) (2) Bestimmtheitsmaß \\(R^2\\) beschreibt den Anteil der durch das Gesamtmodell aufgeklärten Varianz (liegt zwischen 0 und 1, wobei 0 bedeutet, dass das Modell keine Erklärungskraft hat und 1, dass das Modell die abhängige Variable perfekt erklärt); wird für die Interpretation in Prozent angegeben. (3) Koeffizienten (\\(\\texttt{Estimates}\\)) beschreiben den Einfluss der unabhängigen Variable (Alter) auf die abhängige Variable (Zufriedenheitswert). Interpretation Der p-Wert der F-Statistik liegt hier mit 0.803 deutlich über dem Signifikanzniveau von 0.05, was bedeutet, dass das gesammte Modell nicht Statistisch signifikant ist. Dementsprechend klein ist auch das \\(R^2\\), welches mit $ 3.954^{-5}$ sehr gering ausfällt. Für die Interpretion der Koeffizienten gilt es zuerst auf die Werte unter \\(\\texttt{Coefficients}\\) in der Spalte \\(\\texttt{Pr(&gt;|t|)}\\) zu schauen. Die Werte hier sind die p-Werte für die jeweiligen Koeffizienten. Um an diese Signifikanzniveaus zu kommen wird automatisch für jeden Koeffizienten ein t-test gerechnet, welcher prüft, ob die Koeffizienten signifikant von Null verschieden sind. Also, ob ausgeschlossen werden kann, dass der berechnente Koeffizient nicht nur zufällig einen Einfluss auf die abhängige Variable zeigt. Generell gilt, je niedriger diese p-Werte, desto besser. Um von einem Signifikanten Koeffizienten zu sprechen, muss der p-Wert zumindest einen Wert von 0.05 unterschreiten. In der Tabelle wird das zusätzlich durch Sterne hinter den p-Werten angezeigt. Die bedeutung ist hier: Erst wenn einer dieser Fälle eintritt, können die Koeffizienten sinvoll interpretiert werden. Für diesen Fall können wir sagen, dass der \\(\\alpha\\)-Koeffizient, der hier mit \\(\\texttt{(Intercept)}\\) beschrieben wird, hochsignifikant ist. Interpretiert wird der Wert, der in der \\(Estimate\\)-Spalte steht (7.9218361). Übersetzt heißt dieser Wert, das wir für eine Person mit dem Alter null Jahre im Durchschnitt einen Zufriedenswert von \\(\\approx\\) 7.9 haben. Die Interpretation des Intercepts ist aber meistens nicht sinvoll, da Personen, die damit beschrieben werden irrelevant sind. Interessanter sind die \\(\\beta\\)-Koeffizienten, die neben den jeweiligen Variablen stehen. Für den Koffizient für Alter haben wir hier zwar keine Signifikanz, was bedeutet, dass das Alter keinen Einfluss auf die Zufriedenheit hat. Zur Veranschaulichung würde der Wert hier so interpretiert werden, dass wenn das Alter um ein Jahr steigt (immer um eine Einheit) der Zufriedenheitswert im Durchschnitt um 0.0007972 Punkte sinkt (da negativ). 11.6.2 multiple lineare Regression Meistens ist es das Ziel nicht nur den Einfluss von einer unabhängigen Variablen auf eine abhängige Variable zu prüfen, sondern festzustellen, ob mehrere Variablen zusammen einen Einfluss auf die abhängige Variable haben. Dementsprechend sieht die neue Formel dann so aus: \\(Y=\\alpha+\\beta_1 \\cdot X_1 + \\beta_2 \\cdot X_2 +...+ \\beta_k \\cdot X_k +\\varepsilon\\) mit \\(k\\) ist hier gemeint, dass unendlich viele unabhänige Variablen hinzugezogen werden können. Die Vorgehensweise ist ähnlich zur einfachen linearen Regression, nur das hier jetzt zusätlich zum Alter auch noch Einkommen (in 100 EUR) und Geschlecht als erklärende Variablen einbezogen werden. Diese werden in der Formel mit einem \\(+\\) hinzugefügt. output &lt;- lm(Zufriedenheit~Alter+Einkommen+Geschlecht,data = Allbus) summary(output) #&gt; #&gt; Call: #&gt; lm(formula = Zufriedenheit ~ Alter + Einkommen + Geschlecht, #&gt; data = Allbus) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -7.7490 -0.7313 0.1577 1.0315 2.7668 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 7.522119 0.154954 48.544 &lt; 2e-16 #&gt; Alter -0.006036 0.003219 -1.875 0.0609 #&gt; Einkommen 0.022417 0.003086 7.264 5.90e-13 #&gt; Geschlechtweiblich 0.354772 0.081627 4.346 1.47e-05 #&gt; #&gt; (Intercept) *** #&gt; Alter . #&gt; Einkommen *** #&gt; Geschlechtweiblich *** #&gt; --- #&gt; Signif. codes: #&gt; 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 1.506 on 1572 degrees of freedom #&gt; Multiple R-squared: 0.03466, Adjusted R-squared: 0.03282 #&gt; F-statistic: 18.81 on 3 and 1572 DF, p-value: 5.463e-12 Zusätlich gilt es bei multiplen linearen Regressionen auch auf den Wert des \\(\\texttt{Adjusted R-squared}\\) zu achten, welches den \\(\\texttt{R-Squared}\\)-Wert korrigiert, um den Effekt von zusätzlichen unabhängigen Variablen auszugleichen. Man kann es zum Vergleich verschiedener Modelle nutzen, aber nur das \\(\\texttt{R-Squared}\\) kann interpretiert werden. Interpretation Der p-Wert der F-Statistik liegt für dieses Modell deutlich unter 0.05 und damit ist das gesammte Modell hochsignifikant. Der Wert von \\(\\texttt{R-Squared}\\) liegt bei 0.03466, was bedeutet, dass die unabhängigen Variablen insgesammt 3.5% Varianz in der abhängigen Variable erklären. Ob diese Varianzaufklärung einen hohen Wert darstellt muss im Vergleich mit der umliegenden Literatur geklärt werden. Für das Alter zeigt sich auch in diesem Modell keine Signifikanz. Das Einkommen hat dafür einen hochsignifikanten Einfluss auf die Zufriedenheit. Der Wert kann so interpretiert werden, dass wenn das Einkommen um 100 EUR steigt, der Zufriedenheitswert im Durchschnitt und unter Konstanthaltung aller anderer Variablen um \\(\\approx\\) 0.022 Punkte steigt. Bei multiplen Regressionen ist es wichtig, immer dazu zu sagen, dass die restlichen Variablen Konstant gehalten werden (auch genannt ceteris paribus), da dies Vorraussetzung für die Interpretation ist. Das Geschlecht ist ebenfalls hochsignifikant. Diese Variable ist aber ein Sonderfall, da es sich hier um eine kategoriale Variable handelt. Diese können nicht um einen gewissen Betrag steigen. Daher werden für die Interpretaion eine Ausprägung der Variable (hier: männlich) als Referenzkategorie genommen, auf die sich die restlichen Ausprägungen beziehen. Das erklärt auch warum in der Tabelle \\(\\texttt{Geschlechtweiblich}\\) steht. Es geht hier um den Vergleich von weiblich zur Referenzkategorie männlich. Konkret bedeutet das hier, dass eine weibliche Person im Durchschnitt und unter Konstanthaltung aller anderer Variablen einen 0.35 Punkte höheren Zufriedenheitswert hat als eine männliche Person. Benning, V. (2022, 02. Dezember). Zusammenhangsmaße verstehen und anwenden + Beispiele. Scribbr. Abgerufen am 27. Juni 2023, von https://www.scribbr.de/statistik/zusammenhangsmasse/↩︎ https://de.statista.com/statistik/lexikon/definition/139/verteilung/↩︎ Benning, V. (2023, 09. Januar). Deskriptive Statistik verstehen und anwenden. Scribbr. Abgerufen am 16. Juni 2023, von https://www.scribbr.de/statistik/deskriptive-statistik/↩︎ Benning, V. (2023, 23. Mai). Die Lageparameter erklärt mit Beispielen. Scribbr. Abgerufen am 14. Juni 2023, von https://www.scribbr.de/statistik/lageparameter/↩︎ Benning, V. (2022, 01. Juni). Die Streuungsmaße einfach erklärt mit Beispielen. Scribbr. Abgerufen am 14. Juni 2023, von https://www.scribbr.de/statistik/streuungsmasse/↩︎ Benning, V. (2022, 02. Dezember). Zusammenhangsmaße verstehen und anwenden + Beispiele. Scribbr. Abgerufen am 27. Juni 2023, von https://www.scribbr.de/statistik/zusammenhangsmasse/, https://wwwhomes.uni-bielefeld.de/fvan_veen/StatistikII%20SS06/Sonstiges/Interpretationshilfe.pdf↩︎ Teile des Kapitels sind den Vorlesungsmaterialien zu “Einführung in die Statistik” von Dr. Stephan Poppe entnommen↩︎ "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
